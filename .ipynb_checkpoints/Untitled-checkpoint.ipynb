{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupance = pd.read_table('occupancy_data/datatraining.txt',delimiter=',')\n",
    "Stu_Eva = pd.read_csv('turkiye-student-evaluation_generic.csv',delimiter=',',header=0)\n",
    "Activity = pd.read_csv('Activity_Recognition/1.csv',delimiter=',',header=None)\n",
    "Parking = pd.read_csv('Parking_Birminghan_Data.csv',delimiter=',',header=0)\n",
    "Activity.columns=['sequential','x','y','z','activity_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sc = StandardScaler()\n",
    "# different test size\n",
    "test_size=[0.2,0.5,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Activity = pd.concat([Activity,pd.get_dummies(Activity['activity_type'])],axis=1)\n",
    "Activity_type = Activity.iloc[:,3]\n",
    "Activity.drop(columns=['activity_type','sequential'],inplace=True)\n",
    "\n",
    "Act_x = Activity.iloc[:,:3]\n",
    "Act_y = Activity.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162471</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162472</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162474</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162475</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162483</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162484</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162492</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162494</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162500</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162501 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  1  2  3  4  5  6  7\n",
       "0       0  1  0  0  0  0  0  0\n",
       "1       0  1  0  0  0  0  0  0\n",
       "2       0  1  0  0  0  0  0  0\n",
       "3       0  1  0  0  0  0  0  0\n",
       "4       0  1  0  0  0  0  0  0\n",
       "5       0  1  0  0  0  0  0  0\n",
       "6       0  1  0  0  0  0  0  0\n",
       "7       0  1  0  0  0  0  0  0\n",
       "8       0  1  0  0  0  0  0  0\n",
       "9       0  1  0  0  0  0  0  0\n",
       "10      0  1  0  0  0  0  0  0\n",
       "11      0  1  0  0  0  0  0  0\n",
       "12      0  1  0  0  0  0  0  0\n",
       "13      0  1  0  0  0  0  0  0\n",
       "14      0  1  0  0  0  0  0  0\n",
       "15      0  1  0  0  0  0  0  0\n",
       "16      0  1  0  0  0  0  0  0\n",
       "17      0  1  0  0  0  0  0  0\n",
       "18      0  1  0  0  0  0  0  0\n",
       "19      0  1  0  0  0  0  0  0\n",
       "20      0  1  0  0  0  0  0  0\n",
       "21      0  1  0  0  0  0  0  0\n",
       "22      0  1  0  0  0  0  0  0\n",
       "23      0  1  0  0  0  0  0  0\n",
       "24      0  1  0  0  0  0  0  0\n",
       "25      0  1  0  0  0  0  0  0\n",
       "26      0  1  0  0  0  0  0  0\n",
       "27      0  1  0  0  0  0  0  0\n",
       "28      0  1  0  0  0  0  0  0\n",
       "29      0  1  0  0  0  0  0  0\n",
       "...    .. .. .. .. .. .. .. ..\n",
       "162471  0  0  0  0  0  0  0  1\n",
       "162472  0  0  0  0  0  0  0  1\n",
       "162473  0  0  0  0  0  0  0  1\n",
       "162474  0  0  0  0  0  0  0  1\n",
       "162475  0  0  0  0  0  0  0  1\n",
       "162476  0  0  0  0  0  0  0  1\n",
       "162477  0  0  0  0  0  0  0  1\n",
       "162478  0  0  0  0  0  0  0  1\n",
       "162479  0  0  0  0  0  0  0  1\n",
       "162480  0  0  0  0  0  0  0  1\n",
       "162481  0  0  0  0  0  0  0  1\n",
       "162482  0  0  0  0  0  0  0  1\n",
       "162483  0  0  0  0  0  0  0  1\n",
       "162484  0  0  0  0  0  0  0  1\n",
       "162485  0  0  0  0  0  0  0  1\n",
       "162486  0  0  0  0  0  0  0  1\n",
       "162487  0  0  0  0  0  0  0  1\n",
       "162488  0  0  0  0  0  0  0  1\n",
       "162489  0  0  0  0  0  0  0  1\n",
       "162490  0  0  0  0  0  0  0  1\n",
       "162491  0  0  0  0  0  0  0  1\n",
       "162492  0  0  0  0  0  0  0  1\n",
       "162493  0  0  0  0  0  0  0  1\n",
       "162494  0  0  0  0  0  0  0  1\n",
       "162495  0  0  0  0  0  0  0  1\n",
       "162496  0  0  0  0  0  0  0  1\n",
       "162497  0  0  0  0  0  0  0  1\n",
       "162498  0  0  0  0  0  0  0  1\n",
       "162499  0  0  0  0  0  0  0  1\n",
       "162500  1  0  0  0  0  0  0  0\n",
       "\n",
       "[162501 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Act_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11f760210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(Activity.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupance.drop(columns=['date'],inplace=True)\n",
    "occupance['Occupancy']=occupance['Occupancy'].astype(str)\n",
    "occ_x = occupance.iloc[:,:-1]\n",
    "occ_y = occupance.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occ_y.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x124ff0790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(occupance.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Stu_Eva = pd.concat([Stu_Eva,pd.get_dummies(Stu_Eva['instr'])],axis=1)\n",
    "Stu_Eva.drop(columns=['instr'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on student evaluation dataset\n",
    "\n",
    "eva_x = Stu_Eva.iloc[:,0:-3]\n",
    "eva_y = Stu_Eva.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SystemCodeNumber</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>LastUpdated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>61</td>\n",
       "      <td>10/4/16 7:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>64</td>\n",
       "      <td>10/4/16 8:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>80</td>\n",
       "      <td>10/4/16 8:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>107</td>\n",
       "      <td>10/4/16 9:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>150</td>\n",
       "      <td>10/4/16 9:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>177</td>\n",
       "      <td>10/4/16 10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>219</td>\n",
       "      <td>10/4/16 10:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>247</td>\n",
       "      <td>10/4/16 11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>259</td>\n",
       "      <td>10/4/16 11:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>266</td>\n",
       "      <td>10/4/16 12:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>269</td>\n",
       "      <td>10/4/16 13:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>263</td>\n",
       "      <td>10/4/16 13:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>238</td>\n",
       "      <td>10/4/16 14:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>215</td>\n",
       "      <td>10/4/16 14:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>192</td>\n",
       "      <td>10/4/16 14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>165</td>\n",
       "      <td>10/4/16 15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>162</td>\n",
       "      <td>10/4/16 16:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>143</td>\n",
       "      <td>10/4/16 16:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>54</td>\n",
       "      <td>10/5/16 7:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>59</td>\n",
       "      <td>10/5/16 8:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>71</td>\n",
       "      <td>10/5/16 9:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>83</td>\n",
       "      <td>10/5/16 9:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>114</td>\n",
       "      <td>10/5/16 10:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>128</td>\n",
       "      <td>10/5/16 10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>148</td>\n",
       "      <td>10/5/16 11:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>162</td>\n",
       "      <td>10/5/16 11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>178</td>\n",
       "      <td>10/5/16 12:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>183</td>\n",
       "      <td>10/5/16 12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>175</td>\n",
       "      <td>10/5/16 12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BHMBCCMKT01</td>\n",
       "      <td>577</td>\n",
       "      <td>179</td>\n",
       "      <td>10/5/16 13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35687</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>505</td>\n",
       "      <td>12/18/16 11:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35688</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>595</td>\n",
       "      <td>12/18/16 11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35689</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>690</td>\n",
       "      <td>12/18/16 12:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35690</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>800</td>\n",
       "      <td>12/18/16 12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35691</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>852</td>\n",
       "      <td>12/18/16 12:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35692</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>921</td>\n",
       "      <td>12/18/16 13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35693</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>966</td>\n",
       "      <td>12/18/16 14:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35694</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>975</td>\n",
       "      <td>12/18/16 14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35695</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>978</td>\n",
       "      <td>12/18/16 14:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35696</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>907</td>\n",
       "      <td>12/18/16 15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35697</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>817</td>\n",
       "      <td>12/18/16 16:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35698</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>726</td>\n",
       "      <td>12/18/16 16:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35699</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>674</td>\n",
       "      <td>12/19/16 8:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35700</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>825</td>\n",
       "      <td>12/19/16 8:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35701</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1011</td>\n",
       "      <td>12/19/16 9:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35702</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1100</td>\n",
       "      <td>12/19/16 9:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35703</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1192</td>\n",
       "      <td>12/19/16 9:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35704</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1257</td>\n",
       "      <td>12/19/16 10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35705</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1317</td>\n",
       "      <td>12/19/16 11:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35706</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1371</td>\n",
       "      <td>12/19/16 11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35707</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1424</td>\n",
       "      <td>12/19/16 12:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35708</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1475</td>\n",
       "      <td>12/19/16 12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35709</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1510</td>\n",
       "      <td>12/19/16 13:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35710</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1510</td>\n",
       "      <td>12/19/16 13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35711</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1521</td>\n",
       "      <td>12/19/16 14:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35712</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1517</td>\n",
       "      <td>12/19/16 14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35713</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1487</td>\n",
       "      <td>12/19/16 15:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35714</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1432</td>\n",
       "      <td>12/19/16 15:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35715</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1321</td>\n",
       "      <td>12/19/16 16:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35716</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>1920</td>\n",
       "      <td>1180</td>\n",
       "      <td>12/19/16 16:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35717 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SystemCodeNumber  Capacity  Occupancy     LastUpdated\n",
       "0          BHMBCCMKT01       577         61    10/4/16 7:59\n",
       "1          BHMBCCMKT01       577         64    10/4/16 8:25\n",
       "2          BHMBCCMKT01       577         80    10/4/16 8:59\n",
       "3          BHMBCCMKT01       577        107    10/4/16 9:32\n",
       "4          BHMBCCMKT01       577        150    10/4/16 9:59\n",
       "5          BHMBCCMKT01       577        177   10/4/16 10:26\n",
       "6          BHMBCCMKT01       577        219   10/4/16 10:59\n",
       "7          BHMBCCMKT01       577        247   10/4/16 11:25\n",
       "8          BHMBCCMKT01       577        259   10/4/16 11:59\n",
       "9          BHMBCCMKT01       577        266   10/4/16 12:29\n",
       "10         BHMBCCMKT01       577        269   10/4/16 13:02\n",
       "11         BHMBCCMKT01       577        263   10/4/16 13:29\n",
       "12         BHMBCCMKT01       577        238   10/4/16 14:02\n",
       "13         BHMBCCMKT01       577        215   10/4/16 14:29\n",
       "14         BHMBCCMKT01       577        192   10/4/16 14:57\n",
       "15         BHMBCCMKT01       577        165   10/4/16 15:30\n",
       "16         BHMBCCMKT01       577        162   10/4/16 16:04\n",
       "17         BHMBCCMKT01       577        143   10/4/16 16:31\n",
       "18         BHMBCCMKT01       577         54    10/5/16 7:57\n",
       "19         BHMBCCMKT01       577         59    10/5/16 8:30\n",
       "20         BHMBCCMKT01       577         71    10/5/16 9:04\n",
       "21         BHMBCCMKT01       577         83    10/5/16 9:30\n",
       "22         BHMBCCMKT01       577        114   10/5/16 10:04\n",
       "23         BHMBCCMKT01       577        128   10/5/16 10:30\n",
       "24         BHMBCCMKT01       577        148   10/5/16 11:04\n",
       "25         BHMBCCMKT01       577        162   10/5/16 11:30\n",
       "26         BHMBCCMKT01       577        178   10/5/16 12:04\n",
       "27         BHMBCCMKT01       577        183   10/5/16 12:30\n",
       "28         BHMBCCMKT01       577        175   10/5/16 12:57\n",
       "29         BHMBCCMKT01       577        179   10/5/16 13:30\n",
       "...                ...       ...        ...             ...\n",
       "35687         Shopping      1920        505  12/18/16 11:03\n",
       "35688         Shopping      1920        595  12/18/16 11:30\n",
       "35689         Shopping      1920        690  12/18/16 12:03\n",
       "35690         Shopping      1920        800  12/18/16 12:30\n",
       "35691         Shopping      1920        852  12/18/16 12:56\n",
       "35692         Shopping      1920        921  12/18/16 13:30\n",
       "35693         Shopping      1920        966  12/18/16 14:03\n",
       "35694         Shopping      1920        975  12/18/16 14:30\n",
       "35695         Shopping      1920        978  12/18/16 14:56\n",
       "35696         Shopping      1920        907  12/18/16 15:30\n",
       "35697         Shopping      1920        817  12/18/16 16:03\n",
       "35698         Shopping      1920        726  12/18/16 16:30\n",
       "35699         Shopping      1920        674   12/19/16 8:03\n",
       "35700         Shopping      1920        825   12/19/16 8:30\n",
       "35701         Shopping      1920       1011   12/19/16 9:03\n",
       "35702         Shopping      1920       1100   12/19/16 9:30\n",
       "35703         Shopping      1920       1192   12/19/16 9:56\n",
       "35704         Shopping      1920       1257  12/19/16 10:30\n",
       "35705         Shopping      1920       1317  12/19/16 11:03\n",
       "35706         Shopping      1920       1371  12/19/16 11:30\n",
       "35707         Shopping      1920       1424  12/19/16 12:03\n",
       "35708         Shopping      1920       1475  12/19/16 12:30\n",
       "35709         Shopping      1920       1510  12/19/16 13:03\n",
       "35710         Shopping      1920       1510  12/19/16 13:30\n",
       "35711         Shopping      1920       1521  12/19/16 14:03\n",
       "35712         Shopping      1920       1517  12/19/16 14:30\n",
       "35713         Shopping      1920       1487  12/19/16 15:03\n",
       "35714         Shopping      1920       1432  12/19/16 15:29\n",
       "35715         Shopping      1920       1321  12/19/16 16:03\n",
       "35716         Shopping      1920       1180  12/19/16 16:30\n",
       "\n",
       "[35717 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(Stu_Eva.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process parking dataset\n",
    "Parking['SystemCodeNumber']=Parking['SystemCodeNumber'].astype(str)\n",
    "Parking.drop(columns=['LastUpdated'],inplace=True)\n",
    "Parking = pd.concat([Parking,pd.get_dummies(Parking['SystemCodeNumber'])],axis=1)\n",
    "Parking.drop(columns=['SystemCodeNumber'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_x = Parking.iloc[:,:2]\n",
    "parking_y = Parking.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x103c506d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(Parking.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35717, 2), (5820, 32), (8143, 5), (162501, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parking_x.shape,eva_x.shape,occ_x.shape,Act_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5820, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(in_num,out_num):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32,activation='relu',input_dim=in_num))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(units = out_num, activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4656 samples, validate on 4656 samples\n",
      "Epoch 1/200\n",
      "4656/4656 [==============================] - 2s 510us/step - loss: 0.9218 - acc: 0.6183 - val_loss: 0.8849 - val_acc: 0.6209\n",
      "Epoch 2/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.8791 - acc: 0.6207 - val_loss: 0.8654 - val_acc: 0.6231\n",
      "Epoch 3/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.8609 - acc: 0.6229 - val_loss: 0.8486 - val_acc: 0.6224\n",
      "Epoch 4/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.8490 - acc: 0.6216 - val_loss: 0.8418 - val_acc: 0.6248\n",
      "Epoch 5/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.8399 - acc: 0.6220 - val_loss: 0.8285 - val_acc: 0.6263\n",
      "Epoch 6/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.8308 - acc: 0.6259 - val_loss: 0.8196 - val_acc: 0.6370\n",
      "Epoch 7/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.8219 - acc: 0.6351 - val_loss: 0.8140 - val_acc: 0.6433\n",
      "Epoch 8/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.8150 - acc: 0.6366 - val_loss: 0.8041 - val_acc: 0.6366\n",
      "Epoch 9/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.8073 - acc: 0.6405 - val_loss: 0.7990 - val_acc: 0.6368\n",
      "Epoch 10/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.8004 - acc: 0.6435 - val_loss: 0.7912 - val_acc: 0.6458\n",
      "Epoch 11/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.7933 - acc: 0.6480 - val_loss: 0.7862 - val_acc: 0.6598\n",
      "Epoch 12/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.7883 - acc: 0.6482 - val_loss: 0.7788 - val_acc: 0.6611\n",
      "Epoch 13/200\n",
      "4656/4656 [==============================] - 0s 85us/step - loss: 0.7810 - acc: 0.6559 - val_loss: 0.7744 - val_acc: 0.6566\n",
      "Epoch 14/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.7757 - acc: 0.6555 - val_loss: 0.7658 - val_acc: 0.6615\n",
      "Epoch 15/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.7672 - acc: 0.6596 - val_loss: 0.7570 - val_acc: 0.6596\n",
      "Epoch 16/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.7615 - acc: 0.6667 - val_loss: 0.7524 - val_acc: 0.6699\n",
      "Epoch 17/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.7551 - acc: 0.6665 - val_loss: 0.7501 - val_acc: 0.6714\n",
      "Epoch 18/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.7490 - acc: 0.6701 - val_loss: 0.7394 - val_acc: 0.6789\n",
      "Epoch 19/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.7412 - acc: 0.6720 - val_loss: 0.7310 - val_acc: 0.6738\n",
      "Epoch 20/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.7330 - acc: 0.6770 - val_loss: 0.7343 - val_acc: 0.6710\n",
      "Epoch 21/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.7281 - acc: 0.6791 - val_loss: 0.7161 - val_acc: 0.6862\n",
      "Epoch 22/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.7218 - acc: 0.6845 - val_loss: 0.7194 - val_acc: 0.6851\n",
      "Epoch 23/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.7166 - acc: 0.6847 - val_loss: 0.7132 - val_acc: 0.6832\n",
      "Epoch 24/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.7082 - acc: 0.6886 - val_loss: 0.6985 - val_acc: 0.6959\n",
      "Epoch 25/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.7024 - acc: 0.6931 - val_loss: 0.6889 - val_acc: 0.7023\n",
      "Epoch 26/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.6956 - acc: 0.6980 - val_loss: 0.6843 - val_acc: 0.7047\n",
      "Epoch 27/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.6864 - acc: 0.7000 - val_loss: 0.6886 - val_acc: 0.7045\n",
      "Epoch 28/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.6841 - acc: 0.7032 - val_loss: 0.6728 - val_acc: 0.7051\n",
      "Epoch 29/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.6779 - acc: 0.7064 - val_loss: 0.6651 - val_acc: 0.7053\n",
      "Epoch 30/200\n",
      "4656/4656 [==============================] - 0s 88us/step - loss: 0.6698 - acc: 0.7068 - val_loss: 0.6615 - val_acc: 0.7066\n",
      "Epoch 31/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.6635 - acc: 0.7111 - val_loss: 0.6552 - val_acc: 0.7152\n",
      "Epoch 32/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.6586 - acc: 0.7174 - val_loss: 0.6462 - val_acc: 0.7135\n",
      "Epoch 33/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.6512 - acc: 0.7199 - val_loss: 0.6438 - val_acc: 0.7225\n",
      "Epoch 34/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.6457 - acc: 0.7219 - val_loss: 0.6449 - val_acc: 0.7270\n",
      "Epoch 35/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.6409 - acc: 0.7277 - val_loss: 0.6294 - val_acc: 0.7339\n",
      "Epoch 36/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.6332 - acc: 0.7240 - val_loss: 0.6263 - val_acc: 0.7388\n",
      "Epoch 37/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.6276 - acc: 0.7343 - val_loss: 0.6220 - val_acc: 0.7416\n",
      "Epoch 38/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.6214 - acc: 0.7380 - val_loss: 0.6168 - val_acc: 0.7315\n",
      "Epoch 39/200\n",
      "4656/4656 [==============================] - 0s 82us/step - loss: 0.6177 - acc: 0.7369 - val_loss: 0.6079 - val_acc: 0.7348\n",
      "Epoch 40/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.6116 - acc: 0.7380 - val_loss: 0.6023 - val_acc: 0.7485\n",
      "Epoch 41/200\n",
      "4656/4656 [==============================] - 0s 81us/step - loss: 0.6047 - acc: 0.7399 - val_loss: 0.5986 - val_acc: 0.7511\n",
      "Epoch 42/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.6011 - acc: 0.7453 - val_loss: 0.5899 - val_acc: 0.7401\n",
      "Epoch 43/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.5951 - acc: 0.7489 - val_loss: 0.5880 - val_acc: 0.7401\n",
      "Epoch 44/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.5881 - acc: 0.7461 - val_loss: 0.5821 - val_acc: 0.7556\n",
      "Epoch 45/200\n",
      "4656/4656 [==============================] - 0s 70us/step - loss: 0.5819 - acc: 0.7502 - val_loss: 0.5817 - val_acc: 0.7511\n",
      "Epoch 46/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.5788 - acc: 0.7519 - val_loss: 0.5695 - val_acc: 0.7652\n",
      "Epoch 47/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.5716 - acc: 0.7640 - val_loss: 0.5596 - val_acc: 0.7698\n",
      "Epoch 48/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.5654 - acc: 0.7603 - val_loss: 0.5688 - val_acc: 0.7745\n",
      "Epoch 49/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.5611 - acc: 0.7646 - val_loss: 0.5623 - val_acc: 0.7775\n",
      "Epoch 50/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.5560 - acc: 0.7648 - val_loss: 0.5476 - val_acc: 0.7792\n",
      "Epoch 51/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.5493 - acc: 0.7693 - val_loss: 0.5433 - val_acc: 0.7784\n",
      "Epoch 52/200\n",
      "4656/4656 [==============================] - 0s 89us/step - loss: 0.5445 - acc: 0.7715 - val_loss: 0.5427 - val_acc: 0.7801\n",
      "Epoch 53/200\n",
      "4656/4656 [==============================] - 0s 79us/step - loss: 0.5400 - acc: 0.7766 - val_loss: 0.5303 - val_acc: 0.7809\n",
      "Epoch 54/200\n",
      "4656/4656 [==============================] - 0s 92us/step - loss: 0.5363 - acc: 0.7809 - val_loss: 0.5441 - val_acc: 0.7784\n",
      "Epoch 55/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.5302 - acc: 0.7820 - val_loss: 0.5193 - val_acc: 0.7801\n",
      "Epoch 56/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.5225 - acc: 0.7910 - val_loss: 0.5144 - val_acc: 0.7874\n",
      "Epoch 57/200\n",
      "4656/4656 [==============================] - 0s 70us/step - loss: 0.5180 - acc: 0.7923 - val_loss: 0.5197 - val_acc: 0.7803\n",
      "Epoch 58/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.5131 - acc: 0.7942 - val_loss: 0.5074 - val_acc: 0.7841\n",
      "Epoch 59/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.5053 - acc: 0.7936 - val_loss: 0.4992 - val_acc: 0.8028\n",
      "Epoch 60/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.4997 - acc: 0.7988 - val_loss: 0.4989 - val_acc: 0.8138\n",
      "Epoch 61/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.4951 - acc: 0.8050 - val_loss: 0.4865 - val_acc: 0.8226\n",
      "Epoch 62/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.4889 - acc: 0.8065 - val_loss: 0.4805 - val_acc: 0.8155\n",
      "Epoch 63/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.4823 - acc: 0.8097 - val_loss: 0.4804 - val_acc: 0.8093\n",
      "Epoch 64/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.4769 - acc: 0.8134 - val_loss: 0.4714 - val_acc: 0.8239\n",
      "Epoch 65/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.4705 - acc: 0.8174 - val_loss: 0.4634 - val_acc: 0.8067\n",
      "Epoch 66/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.4657 - acc: 0.8202 - val_loss: 0.4570 - val_acc: 0.8217\n",
      "Epoch 67/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.4597 - acc: 0.8273 - val_loss: 0.4488 - val_acc: 0.8245\n",
      "Epoch 68/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.4552 - acc: 0.8275 - val_loss: 0.4569 - val_acc: 0.8200\n",
      "Epoch 69/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.4473 - acc: 0.8288 - val_loss: 0.4451 - val_acc: 0.8232\n",
      "Epoch 70/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.4402 - acc: 0.8353 - val_loss: 0.4322 - val_acc: 0.8387\n",
      "Epoch 71/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.4338 - acc: 0.8400 - val_loss: 0.4259 - val_acc: 0.8374\n",
      "Epoch 72/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.4277 - acc: 0.8415 - val_loss: 0.4282 - val_acc: 0.8404\n",
      "Epoch 73/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.4211 - acc: 0.8479 - val_loss: 0.4124 - val_acc: 0.8527\n",
      "Epoch 74/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.4138 - acc: 0.8482 - val_loss: 0.3995 - val_acc: 0.8518\n",
      "Epoch 75/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.4063 - acc: 0.8546 - val_loss: 0.4029 - val_acc: 0.8546\n",
      "Epoch 76/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.3981 - acc: 0.8589 - val_loss: 0.3850 - val_acc: 0.8703\n",
      "Epoch 77/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.3896 - acc: 0.8606 - val_loss: 0.3978 - val_acc: 0.8630\n",
      "Epoch 78/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.3844 - acc: 0.8651 - val_loss: 0.3748 - val_acc: 0.8724\n",
      "Epoch 79/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.3755 - acc: 0.8705 - val_loss: 0.3748 - val_acc: 0.8655\n",
      "Epoch 80/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.3657 - acc: 0.8756 - val_loss: 0.3483 - val_acc: 0.8900\n",
      "Epoch 81/200\n",
      "4656/4656 [==============================] - 0s 75us/step - loss: 0.3544 - acc: 0.8799 - val_loss: 0.3419 - val_acc: 0.8845\n",
      "Epoch 82/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.3478 - acc: 0.8853 - val_loss: 0.3447 - val_acc: 0.8817\n",
      "Epoch 83/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.3391 - acc: 0.8881 - val_loss: 0.3249 - val_acc: 0.8963\n",
      "Epoch 84/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.3254 - acc: 0.8978 - val_loss: 0.3165 - val_acc: 0.9003\n",
      "Epoch 85/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.3158 - acc: 0.9018 - val_loss: 0.3021 - val_acc: 0.9076\n",
      "Epoch 86/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.3071 - acc: 0.9053 - val_loss: 0.3044 - val_acc: 0.8952\n",
      "Epoch 87/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.2975 - acc: 0.9049 - val_loss: 0.2971 - val_acc: 0.9134\n",
      "Epoch 88/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.2868 - acc: 0.9107 - val_loss: 0.2762 - val_acc: 0.9165\n",
      "Epoch 89/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.2749 - acc: 0.9158 - val_loss: 0.2720 - val_acc: 0.9145\n",
      "Epoch 90/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.2681 - acc: 0.9201 - val_loss: 0.2536 - val_acc: 0.9283\n",
      "Epoch 91/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.2543 - acc: 0.9231 - val_loss: 0.2545 - val_acc: 0.9190\n",
      "Epoch 92/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.2458 - acc: 0.9270 - val_loss: 0.2390 - val_acc: 0.9311\n",
      "Epoch 93/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.2373 - acc: 0.9276 - val_loss: 0.2229 - val_acc: 0.9364\n",
      "Epoch 94/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.2268 - acc: 0.9323 - val_loss: 0.2355 - val_acc: 0.9317\n",
      "Epoch 95/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.2177 - acc: 0.9354 - val_loss: 0.2230 - val_acc: 0.9373\n",
      "Epoch 96/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.2104 - acc: 0.9360 - val_loss: 0.2014 - val_acc: 0.9474\n",
      "Epoch 97/200\n",
      "4656/4656 [==============================] - 0s 94us/step - loss: 0.2032 - acc: 0.9433 - val_loss: 0.2045 - val_acc: 0.9405\n",
      "Epoch 98/200\n",
      "4656/4656 [==============================] - 0s 95us/step - loss: 0.1928 - acc: 0.9442 - val_loss: 0.1933 - val_acc: 0.9474\n",
      "Epoch 99/200\n",
      "4656/4656 [==============================] - 0s 87us/step - loss: 0.1882 - acc: 0.9495 - val_loss: 0.1747 - val_acc: 0.9540\n",
      "Epoch 100/200\n",
      "4656/4656 [==============================] - 1s 128us/step - loss: 0.1818 - acc: 0.9497 - val_loss: 0.1853 - val_acc: 0.9435\n",
      "Epoch 101/200\n",
      "4656/4656 [==============================] - 0s 88us/step - loss: 0.1745 - acc: 0.9527 - val_loss: 0.1667 - val_acc: 0.9543\n",
      "Epoch 102/200\n",
      "4656/4656 [==============================] - 0s 104us/step - loss: 0.1669 - acc: 0.9562 - val_loss: 0.1598 - val_acc: 0.9560\n",
      "Epoch 103/200\n",
      "4656/4656 [==============================] - 0s 90us/step - loss: 0.1608 - acc: 0.9577 - val_loss: 0.1546 - val_acc: 0.9607\n",
      "Epoch 104/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.1552 - acc: 0.9573 - val_loss: 0.1459 - val_acc: 0.9626\n",
      "Epoch 105/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.1487 - acc: 0.9628 - val_loss: 0.1525 - val_acc: 0.9562\n",
      "Epoch 106/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.1456 - acc: 0.9631 - val_loss: 0.1475 - val_acc: 0.9626\n",
      "Epoch 107/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.1409 - acc: 0.9648 - val_loss: 0.1435 - val_acc: 0.9652\n",
      "Epoch 108/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.1382 - acc: 0.9680 - val_loss: 0.1300 - val_acc: 0.9712\n",
      "Epoch 109/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.1314 - acc: 0.9686 - val_loss: 0.1216 - val_acc: 0.9740\n",
      "Epoch 110/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.1282 - acc: 0.9684 - val_loss: 0.1357 - val_acc: 0.9590\n",
      "Epoch 111/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.1225 - acc: 0.9706 - val_loss: 0.1361 - val_acc: 0.9601\n",
      "Epoch 112/200\n",
      "4656/4656 [==============================] - 0s 93us/step - loss: 0.1196 - acc: 0.9732 - val_loss: 0.1102 - val_acc: 0.9742\n",
      "Epoch 113/200\n",
      "4656/4656 [==============================] - 0s 96us/step - loss: 0.1149 - acc: 0.9751 - val_loss: 0.1123 - val_acc: 0.9744\n",
      "Epoch 114/200\n",
      "4656/4656 [==============================] - 0s 83us/step - loss: 0.1137 - acc: 0.9740 - val_loss: 0.1073 - val_acc: 0.9787\n",
      "Epoch 115/200\n",
      "4656/4656 [==============================] - ETA: 0s - loss: 0.1096 - acc: 0.974 - 0s 90us/step - loss: 0.1093 - acc: 0.9747 - val_loss: 0.1079 - val_acc: 0.9738\n",
      "Epoch 116/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.1048 - acc: 0.9777 - val_loss: 0.1111 - val_acc: 0.9706\n",
      "Epoch 117/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.1016 - acc: 0.9783 - val_loss: 0.1003 - val_acc: 0.9759\n",
      "Epoch 118/200\n",
      "4656/4656 [==============================] - 0s 73us/step - loss: 0.1008 - acc: 0.9751 - val_loss: 0.1040 - val_acc: 0.9686\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.0984 - acc: 0.9753 - val_loss: 0.0943 - val_acc: 0.9772\n",
      "Epoch 120/200\n",
      "4656/4656 [==============================] - 0s 92us/step - loss: 0.0943 - acc: 0.9785 - val_loss: 0.0870 - val_acc: 0.9815\n",
      "Epoch 121/200\n",
      "4656/4656 [==============================] - 0s 82us/step - loss: 0.0918 - acc: 0.9794 - val_loss: 0.0972 - val_acc: 0.9777\n",
      "Epoch 122/200\n",
      "4656/4656 [==============================] - 0s 83us/step - loss: 0.0905 - acc: 0.9794 - val_loss: 0.0924 - val_acc: 0.9792\n",
      "Epoch 123/200\n",
      "4656/4656 [==============================] - 1s 115us/step - loss: 0.0871 - acc: 0.9813 - val_loss: 0.0827 - val_acc: 0.9826\n",
      "Epoch 124/200\n",
      "4656/4656 [==============================] - 0s 80us/step - loss: 0.0861 - acc: 0.9796 - val_loss: 0.0780 - val_acc: 0.9813\n",
      "Epoch 125/200\n",
      "4656/4656 [==============================] - 1s 128us/step - loss: 0.0838 - acc: 0.9822 - val_loss: 0.0816 - val_acc: 0.9811\n",
      "Epoch 126/200\n",
      "4656/4656 [==============================] - 0s 95us/step - loss: 0.0811 - acc: 0.9815 - val_loss: 0.0937 - val_acc: 0.9749\n",
      "Epoch 127/200\n",
      "4656/4656 [==============================] - 0s 107us/step - loss: 0.0802 - acc: 0.9824 - val_loss: 0.0819 - val_acc: 0.9794\n",
      "Epoch 128/200\n",
      "4656/4656 [==============================] - 0s 82us/step - loss: 0.0786 - acc: 0.9830 - val_loss: 0.0783 - val_acc: 0.9843\n",
      "Epoch 129/200\n",
      "4656/4656 [==============================] - 0s 78us/step - loss: 0.0756 - acc: 0.9828 - val_loss: 0.0717 - val_acc: 0.9835\n",
      "Epoch 130/200\n",
      "4656/4656 [==============================] - 0s 81us/step - loss: 0.0758 - acc: 0.9826 - val_loss: 0.0689 - val_acc: 0.9837\n",
      "Epoch 131/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.0726 - acc: 0.9839 - val_loss: 0.0692 - val_acc: 0.9841\n",
      "Epoch 132/200\n",
      "4656/4656 [==============================] - 0s 98us/step - loss: 0.0709 - acc: 0.9843 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 133/200\n",
      "4656/4656 [==============================] - 0s 79us/step - loss: 0.0688 - acc: 0.9839 - val_loss: 0.0683 - val_acc: 0.9850\n",
      "Epoch 134/200\n",
      "4656/4656 [==============================] - 0s 81us/step - loss: 0.0717 - acc: 0.9813 - val_loss: 0.0664 - val_acc: 0.9843\n",
      "Epoch 135/200\n",
      "4656/4656 [==============================] - 0s 95us/step - loss: 0.0683 - acc: 0.9843 - val_loss: 0.0753 - val_acc: 0.9811\n",
      "Epoch 136/200\n",
      "4656/4656 [==============================] - 0s 88us/step - loss: 0.0636 - acc: 0.9852 - val_loss: 0.0700 - val_acc: 0.9798\n",
      "Epoch 137/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.0657 - acc: 0.9832 - val_loss: 0.0647 - val_acc: 0.9845\n",
      "Epoch 138/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0645 - acc: 0.9850 - val_loss: 0.0599 - val_acc: 0.9852\n",
      "Epoch 139/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0626 - acc: 0.9856 - val_loss: 0.0646 - val_acc: 0.9841\n",
      "Epoch 140/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0616 - acc: 0.9850 - val_loss: 0.0585 - val_acc: 0.9869\n",
      "Epoch 141/200\n",
      "4656/4656 [==============================] - 0s 83us/step - loss: 0.0626 - acc: 0.9852 - val_loss: 0.0915 - val_acc: 0.9676\n",
      "Epoch 142/200\n",
      "4656/4656 [==============================] - 0s 106us/step - loss: 0.0604 - acc: 0.9867 - val_loss: 0.0601 - val_acc: 0.9858\n",
      "Epoch 143/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0594 - acc: 0.9856 - val_loss: 0.0636 - val_acc: 0.9817\n",
      "Epoch 144/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0579 - acc: 0.9856 - val_loss: 0.0611 - val_acc: 0.9832\n",
      "Epoch 145/200\n",
      "4656/4656 [==============================] - 1s 111us/step - loss: 0.0608 - acc: 0.9854 - val_loss: 0.0549 - val_acc: 0.9873\n",
      "Epoch 146/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.0555 - acc: 0.9856 - val_loss: 0.0524 - val_acc: 0.9873\n",
      "Epoch 147/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.0571 - acc: 0.9871 - val_loss: 0.0507 - val_acc: 0.9867\n",
      "Epoch 148/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.0559 - acc: 0.9869 - val_loss: 0.0499 - val_acc: 0.9873\n",
      "Epoch 149/200\n",
      "4656/4656 [==============================] - 0s 78us/step - loss: 0.0550 - acc: 0.9860 - val_loss: 0.0508 - val_acc: 0.9871\n",
      "Epoch 150/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0557 - acc: 0.9848 - val_loss: 0.0591 - val_acc: 0.9850\n",
      "Epoch 151/200\n",
      "4656/4656 [==============================] - 0s 79us/step - loss: 0.0540 - acc: 0.9863 - val_loss: 0.0489 - val_acc: 0.9873\n",
      "Epoch 152/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0531 - acc: 0.9860 - val_loss: 0.0541 - val_acc: 0.9850\n",
      "Epoch 153/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.0523 - acc: 0.9863 - val_loss: 0.0476 - val_acc: 0.9878\n",
      "Epoch 154/200\n",
      "4656/4656 [==============================] - 1s 113us/step - loss: 0.0529 - acc: 0.9865 - val_loss: 0.0484 - val_acc: 0.9878\n",
      "Epoch 155/200\n",
      "4656/4656 [==============================] - 0s 98us/step - loss: 0.0514 - acc: 0.9867 - val_loss: 0.0509 - val_acc: 0.9880\n",
      "Epoch 156/200\n",
      "4656/4656 [==============================] - 0s 81us/step - loss: 0.0509 - acc: 0.9875 - val_loss: 0.0652 - val_acc: 0.9832\n",
      "Epoch 157/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.0504 - acc: 0.9863 - val_loss: 0.0478 - val_acc: 0.9884\n",
      "Epoch 158/200\n",
      "4656/4656 [==============================] - 0s 89us/step - loss: 0.0493 - acc: 0.9863 - val_loss: 0.0627 - val_acc: 0.9822\n",
      "Epoch 159/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0482 - acc: 0.9867 - val_loss: 0.0467 - val_acc: 0.9888\n",
      "Epoch 160/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.0485 - acc: 0.9882 - val_loss: 0.0466 - val_acc: 0.9871\n",
      "Epoch 161/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0476 - acc: 0.9882 - val_loss: 0.0559 - val_acc: 0.9837\n",
      "Epoch 162/200\n",
      "4656/4656 [==============================] - 0s 75us/step - loss: 0.0478 - acc: 0.9878 - val_loss: 0.0438 - val_acc: 0.9882\n",
      "Epoch 163/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.0482 - acc: 0.9873 - val_loss: 0.0437 - val_acc: 0.9882\n",
      "Epoch 164/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0480 - acc: 0.9871 - val_loss: 0.0471 - val_acc: 0.9886\n",
      "Epoch 165/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0465 - acc: 0.9869 - val_loss: 0.0447 - val_acc: 0.9882\n",
      "Epoch 166/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.0459 - acc: 0.9873 - val_loss: 0.0429 - val_acc: 0.9884\n",
      "Epoch 167/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.0452 - acc: 0.9882 - val_loss: 0.0453 - val_acc: 0.9893\n",
      "Epoch 168/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0456 - acc: 0.9884 - val_loss: 0.0466 - val_acc: 0.9869\n",
      "Epoch 169/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.0458 - acc: 0.9873 - val_loss: 0.0412 - val_acc: 0.9888\n",
      "Epoch 170/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.0445 - acc: 0.9865 - val_loss: 0.0441 - val_acc: 0.9886\n",
      "Epoch 171/200\n",
      "4656/4656 [==============================] - 0s 73us/step - loss: 0.0442 - acc: 0.9886 - val_loss: 0.0405 - val_acc: 0.9886\n",
      "Epoch 172/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0442 - acc: 0.9882 - val_loss: 0.0424 - val_acc: 0.9886\n",
      "Epoch 173/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0449 - acc: 0.9871 - val_loss: 0.0415 - val_acc: 0.9886\n",
      "Epoch 174/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.0447 - acc: 0.9873 - val_loss: 0.0502 - val_acc: 0.9854\n",
      "Epoch 175/200\n",
      "4656/4656 [==============================] - 0s 75us/step - loss: 0.0443 - acc: 0.9886 - val_loss: 0.0399 - val_acc: 0.9886\n",
      "Epoch 176/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.0408 - acc: 0.9886 - val_loss: 0.0508 - val_acc: 0.9850\n",
      "Epoch 177/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.0430 - acc: 0.9869 - val_loss: 0.0407 - val_acc: 0.9893\n",
      "Epoch 178/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.0396 - val_acc: 0.9888\n",
      "Epoch 179/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.0427 - acc: 0.9886 - val_loss: 0.0383 - val_acc: 0.9886\n",
      "Epoch 180/200\n",
      "4656/4656 [==============================] - 0s 82us/step - loss: 0.0423 - acc: 0.9882 - val_loss: 0.0413 - val_acc: 0.9884\n",
      "Epoch 181/200\n",
      "4656/4656 [==============================] - 0s 78us/step - loss: 0.0414 - acc: 0.9878 - val_loss: 0.0442 - val_acc: 0.9880\n",
      "Epoch 182/200\n",
      "4656/4656 [==============================] - 0s 93us/step - loss: 0.0405 - acc: 0.9878 - val_loss: 0.0488 - val_acc: 0.9835\n",
      "Epoch 183/200\n",
      "4656/4656 [==============================] - 0s 91us/step - loss: 0.0421 - acc: 0.9880 - val_loss: 0.0383 - val_acc: 0.9890\n",
      "Epoch 184/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.0433 - acc: 0.9875 - val_loss: 0.0390 - val_acc: 0.9886\n",
      "Epoch 185/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.0421 - acc: 0.9882 - val_loss: 0.0400 - val_acc: 0.9893\n",
      "Epoch 186/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.0400 - acc: 0.9888 - val_loss: 0.0383 - val_acc: 0.9895\n",
      "Epoch 187/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.0405 - acc: 0.9880 - val_loss: 0.0417 - val_acc: 0.9890\n",
      "Epoch 188/200\n",
      "4656/4656 [==============================] - 0s 83us/step - loss: 0.0414 - acc: 0.9884 - val_loss: 0.0400 - val_acc: 0.9875\n",
      "Epoch 189/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.0399 - acc: 0.9886 - val_loss: 0.0386 - val_acc: 0.9899\n",
      "Epoch 190/200\n",
      "4656/4656 [==============================] - 0s 78us/step - loss: 0.0396 - acc: 0.9888 - val_loss: 0.0501 - val_acc: 0.9873\n",
      "Epoch 191/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0392 - acc: 0.9875 - val_loss: 0.0381 - val_acc: 0.9903\n",
      "Epoch 192/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0394 - acc: 0.9886 - val_loss: 0.0362 - val_acc: 0.9895\n",
      "Epoch 193/200\n",
      "4656/4656 [==============================] - 0s 80us/step - loss: 0.0396 - acc: 0.9886 - val_loss: 0.0394 - val_acc: 0.9897\n",
      "Epoch 194/200\n",
      "4656/4656 [==============================] - 0s 75us/step - loss: 0.0402 - acc: 0.9888 - val_loss: 0.0359 - val_acc: 0.9888\n",
      "Epoch 195/200\n",
      "4656/4656 [==============================] - ETA: 0s - loss: 0.0399 - acc: 0.989 - 0s 68us/step - loss: 0.0402 - acc: 0.9888 - val_loss: 0.0382 - val_acc: 0.9888\n",
      "Epoch 196/200\n",
      "4656/4656 [==============================] - 0s 73us/step - loss: 0.0379 - acc: 0.9878 - val_loss: 0.0352 - val_acc: 0.9890\n",
      "Epoch 197/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.0399 - acc: 0.9890 - val_loss: 0.0415 - val_acc: 0.9897\n",
      "Epoch 198/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.0394 - acc: 0.9873 - val_loss: 0.0349 - val_acc: 0.9888\n",
      "Epoch 199/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0390 - acc: 0.9890 - val_loss: 0.0380 - val_acc: 0.9893\n",
      "Epoch 200/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.0398 - acc: 0.9893 - val_loss: 0.0354 - val_acc: 0.9903\n",
      "1164/1164 [==============================] - 0s 80us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4656 samples, validate on 4656 samples\n",
      "Epoch 1/200\n",
      "4656/4656 [==============================] - 3s 622us/step - loss: 0.9519 - acc: 0.5810 - val_loss: 0.8977 - val_acc: 0.6164\n",
      "Epoch 2/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.8852 - acc: 0.6186 - val_loss: 0.8673 - val_acc: 0.6226\n",
      "Epoch 3/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.8625 - acc: 0.6216 - val_loss: 0.8522 - val_acc: 0.6211\n",
      "Epoch 4/200\n",
      "4656/4656 [==============================] - 0s 73us/step - loss: 0.8495 - acc: 0.6211 - val_loss: 0.8383 - val_acc: 0.6261\n",
      "Epoch 5/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.8398 - acc: 0.6216 - val_loss: 0.8306 - val_acc: 0.6207\n",
      "Epoch 6/200\n",
      "4656/4656 [==============================] - 0s 73us/step - loss: 0.8301 - acc: 0.6229 - val_loss: 0.8199 - val_acc: 0.6248\n",
      "Epoch 7/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.8204 - acc: 0.6244 - val_loss: 0.8120 - val_acc: 0.6357\n",
      "Epoch 8/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.8133 - acc: 0.6293 - val_loss: 0.8054 - val_acc: 0.6293\n",
      "Epoch 9/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.8046 - acc: 0.6317 - val_loss: 0.7963 - val_acc: 0.6360\n",
      "Epoch 10/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.7971 - acc: 0.6366 - val_loss: 0.7939 - val_acc: 0.6398\n",
      "Epoch 11/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.7889 - acc: 0.6394 - val_loss: 0.7822 - val_acc: 0.6473\n",
      "Epoch 12/200\n",
      "4656/4656 [==============================] - 0s 75us/step - loss: 0.7802 - acc: 0.6463 - val_loss: 0.7762 - val_acc: 0.6407\n",
      "Epoch 13/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.7738 - acc: 0.6426 - val_loss: 0.7638 - val_acc: 0.6482\n",
      "Epoch 14/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.7666 - acc: 0.6514 - val_loss: 0.7610 - val_acc: 0.6594\n",
      "Epoch 15/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.7591 - acc: 0.6546 - val_loss: 0.7501 - val_acc: 0.6619\n",
      "Epoch 16/200\n",
      "4656/4656 [==============================] - 0s 78us/step - loss: 0.7519 - acc: 0.6613 - val_loss: 0.7495 - val_acc: 0.6516\n",
      "Epoch 17/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.7443 - acc: 0.6639 - val_loss: 0.7352 - val_acc: 0.6688\n",
      "Epoch 18/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.7382 - acc: 0.6703 - val_loss: 0.7370 - val_acc: 0.6723\n",
      "Epoch 19/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.7305 - acc: 0.6742 - val_loss: 0.7209 - val_acc: 0.6851\n",
      "Epoch 20/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.7256 - acc: 0.6759 - val_loss: 0.7150 - val_acc: 0.6918\n",
      "Epoch 21/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.7191 - acc: 0.6836 - val_loss: 0.7077 - val_acc: 0.6948\n",
      "Epoch 22/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.7122 - acc: 0.6884 - val_loss: 0.7055 - val_acc: 0.7019\n",
      "Epoch 23/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.7049 - acc: 0.6957 - val_loss: 0.6998 - val_acc: 0.6892\n",
      "Epoch 24/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.7003 - acc: 0.6978 - val_loss: 0.6884 - val_acc: 0.7066\n",
      "Epoch 25/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.6931 - acc: 0.7060 - val_loss: 0.6832 - val_acc: 0.7107\n",
      "Epoch 26/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.6858 - acc: 0.7085 - val_loss: 0.6778 - val_acc: 0.7244\n",
      "Epoch 27/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.6806 - acc: 0.7152 - val_loss: 0.6726 - val_acc: 0.7156\n",
      "Epoch 28/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.6740 - acc: 0.7154 - val_loss: 0.6613 - val_acc: 0.7302\n",
      "Epoch 29/200\n",
      "4656/4656 [==============================] - 0s 78us/step - loss: 0.6671 - acc: 0.7240 - val_loss: 0.6628 - val_acc: 0.7232\n",
      "Epoch 30/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.6596 - acc: 0.7294 - val_loss: 0.6658 - val_acc: 0.7249\n",
      "Epoch 31/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.6527 - acc: 0.7382 - val_loss: 0.6531 - val_acc: 0.7285\n",
      "Epoch 32/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.6453 - acc: 0.7348 - val_loss: 0.6533 - val_acc: 0.7363\n",
      "Epoch 33/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.6396 - acc: 0.7427 - val_loss: 0.6291 - val_acc: 0.7517\n",
      "Epoch 34/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.6330 - acc: 0.7474 - val_loss: 0.6320 - val_acc: 0.7494\n",
      "Epoch 35/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.6245 - acc: 0.7504 - val_loss: 0.6341 - val_acc: 0.7408\n",
      "Epoch 36/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.6196 - acc: 0.7511 - val_loss: 0.6119 - val_acc: 0.7595\n",
      "Epoch 37/200\n",
      "4656/4656 [==============================] - ETA: 0s - loss: 0.6121 - acc: 0.757 - 0s 55us/step - loss: 0.6148 - acc: 0.7584 - val_loss: 0.6031 - val_acc: 0.7543\n",
      "Epoch 38/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.6060 - acc: 0.7616 - val_loss: 0.5995 - val_acc: 0.7588\n",
      "Epoch 39/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.6004 - acc: 0.7590 - val_loss: 0.5906 - val_acc: 0.7637\n",
      "Epoch 40/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.5925 - acc: 0.7652 - val_loss: 0.5805 - val_acc: 0.7665\n",
      "Epoch 41/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.5865 - acc: 0.7680 - val_loss: 0.5773 - val_acc: 0.7706\n",
      "Epoch 42/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.5785 - acc: 0.7732 - val_loss: 0.5717 - val_acc: 0.7824\n",
      "Epoch 43/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.5710 - acc: 0.7760 - val_loss: 0.5650 - val_acc: 0.7816\n",
      "Epoch 44/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.5656 - acc: 0.7777 - val_loss: 0.5561 - val_acc: 0.7846\n",
      "Epoch 45/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.5588 - acc: 0.7893 - val_loss: 0.5608 - val_acc: 0.7846\n",
      "Epoch 46/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.5540 - acc: 0.7887 - val_loss: 0.5487 - val_acc: 0.7927\n",
      "Epoch 47/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.5433 - acc: 0.7942 - val_loss: 0.5357 - val_acc: 0.7919\n",
      "Epoch 48/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.5389 - acc: 0.7960 - val_loss: 0.5369 - val_acc: 0.8035\n",
      "Epoch 49/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.5339 - acc: 0.7970 - val_loss: 0.5247 - val_acc: 0.8050\n",
      "Epoch 50/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.5257 - acc: 0.8048 - val_loss: 0.5277 - val_acc: 0.7979\n",
      "Epoch 51/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.5180 - acc: 0.8067 - val_loss: 0.5145 - val_acc: 0.8172\n",
      "Epoch 52/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.5142 - acc: 0.8056 - val_loss: 0.5076 - val_acc: 0.8097\n",
      "Epoch 53/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.5044 - acc: 0.8123 - val_loss: 0.4920 - val_acc: 0.8252\n",
      "Epoch 54/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.4988 - acc: 0.8166 - val_loss: 0.4949 - val_acc: 0.8226\n",
      "Epoch 55/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.4920 - acc: 0.8196 - val_loss: 0.4860 - val_acc: 0.8058\n",
      "Epoch 56/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.4855 - acc: 0.8187 - val_loss: 0.4795 - val_acc: 0.8301\n",
      "Epoch 57/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.4791 - acc: 0.8243 - val_loss: 0.4644 - val_acc: 0.8301\n",
      "Epoch 58/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.4719 - acc: 0.8247 - val_loss: 0.4742 - val_acc: 0.8329\n",
      "Epoch 59/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.4689 - acc: 0.8288 - val_loss: 0.4587 - val_acc: 0.8338\n",
      "Epoch 60/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.4595 - acc: 0.8288 - val_loss: 0.4603 - val_acc: 0.8185\n",
      "Epoch 61/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.4566 - acc: 0.8340 - val_loss: 0.4456 - val_acc: 0.8471\n",
      "Epoch 62/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.4469 - acc: 0.8368 - val_loss: 0.4361 - val_acc: 0.8370\n",
      "Epoch 63/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.4428 - acc: 0.8351 - val_loss: 0.4380 - val_acc: 0.8338\n",
      "Epoch 64/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.4374 - acc: 0.8385 - val_loss: 0.4265 - val_acc: 0.8475\n",
      "Epoch 65/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.4313 - acc: 0.8443 - val_loss: 0.4285 - val_acc: 0.8462\n",
      "Epoch 66/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.4233 - acc: 0.8486 - val_loss: 0.4267 - val_acc: 0.8451\n",
      "Epoch 67/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.4168 - acc: 0.8507 - val_loss: 0.4065 - val_acc: 0.8555\n",
      "Epoch 68/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.4110 - acc: 0.8565 - val_loss: 0.4139 - val_acc: 0.8426\n",
      "Epoch 69/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.4049 - acc: 0.8606 - val_loss: 0.4075 - val_acc: 0.8565\n",
      "Epoch 70/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.3969 - acc: 0.8658 - val_loss: 0.3889 - val_acc: 0.8634\n",
      "Epoch 71/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.3920 - acc: 0.8632 - val_loss: 0.3803 - val_acc: 0.8731\n",
      "Epoch 72/200\n",
      "4656/4656 [==============================] - 0s 72us/step - loss: 0.3850 - acc: 0.8681 - val_loss: 0.3807 - val_acc: 0.8615\n",
      "Epoch 73/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.3765 - acc: 0.8722 - val_loss: 0.3649 - val_acc: 0.8735\n",
      "Epoch 74/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.3700 - acc: 0.8713 - val_loss: 0.3548 - val_acc: 0.8825\n",
      "Epoch 75/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.3622 - acc: 0.8754 - val_loss: 0.3531 - val_acc: 0.8780\n",
      "Epoch 76/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.3522 - acc: 0.8808 - val_loss: 0.3630 - val_acc: 0.8806\n",
      "Epoch 77/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.3487 - acc: 0.8795 - val_loss: 0.3483 - val_acc: 0.8763\n",
      "Epoch 78/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.3377 - acc: 0.8853 - val_loss: 0.3413 - val_acc: 0.8857\n",
      "Epoch 79/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.3302 - acc: 0.8872 - val_loss: 0.3183 - val_acc: 0.8954\n",
      "Epoch 80/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.3238 - acc: 0.8915 - val_loss: 0.3248 - val_acc: 0.8860\n",
      "Epoch 81/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.3163 - acc: 0.8963 - val_loss: 0.3069 - val_acc: 0.9042\n",
      "Epoch 82/200\n",
      "4656/4656 [==============================] - ETA: 0s - loss: 0.3131 - acc: 0.894 - 0s 67us/step - loss: 0.3074 - acc: 0.8948 - val_loss: 0.2958 - val_acc: 0.9066\n",
      "Epoch 83/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.3000 - acc: 0.9018 - val_loss: 0.2851 - val_acc: 0.9085\n",
      "Epoch 84/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.2923 - acc: 0.9036 - val_loss: 0.2837 - val_acc: 0.9018\n",
      "Epoch 85/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.2865 - acc: 0.9091 - val_loss: 0.2819 - val_acc: 0.9072\n",
      "Epoch 86/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.2780 - acc: 0.9111 - val_loss: 0.2769 - val_acc: 0.9091\n",
      "Epoch 87/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.2723 - acc: 0.9132 - val_loss: 0.2587 - val_acc: 0.9139\n",
      "Epoch 88/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.2638 - acc: 0.9134 - val_loss: 0.2626 - val_acc: 0.9244\n",
      "Epoch 89/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.2611 - acc: 0.9141 - val_loss: 0.2492 - val_acc: 0.9257\n",
      "Epoch 90/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.2521 - acc: 0.9214 - val_loss: 0.2397 - val_acc: 0.9253\n",
      "Epoch 91/200\n",
      "4656/4656 [==============================] - 0s 50us/step - loss: 0.2475 - acc: 0.9182 - val_loss: 0.2332 - val_acc: 0.9261\n",
      "Epoch 92/200\n",
      "4656/4656 [==============================] - 0s 50us/step - loss: 0.2395 - acc: 0.9257 - val_loss: 0.2380 - val_acc: 0.9298\n",
      "Epoch 93/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.2350 - acc: 0.9244 - val_loss: 0.2262 - val_acc: 0.9274\n",
      "Epoch 94/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.2295 - acc: 0.9261 - val_loss: 0.2184 - val_acc: 0.9338\n",
      "Epoch 95/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.2234 - acc: 0.9326 - val_loss: 0.2188 - val_acc: 0.9255\n",
      "Epoch 96/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.2183 - acc: 0.9304 - val_loss: 0.2146 - val_acc: 0.9351\n",
      "Epoch 97/200\n",
      "4656/4656 [==============================] - 0s 50us/step - loss: 0.2119 - acc: 0.9366 - val_loss: 0.2136 - val_acc: 0.9345\n",
      "Epoch 98/200\n",
      "4656/4656 [==============================] - 0s 50us/step - loss: 0.2070 - acc: 0.9371 - val_loss: 0.1992 - val_acc: 0.9433\n",
      "Epoch 99/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.2015 - acc: 0.9386 - val_loss: 0.2094 - val_acc: 0.9351\n",
      "Epoch 100/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.1983 - acc: 0.9418 - val_loss: 0.1911 - val_acc: 0.9392\n",
      "Epoch 101/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.1930 - acc: 0.9429 - val_loss: 0.1850 - val_acc: 0.9422\n",
      "Epoch 102/200\n",
      "4656/4656 [==============================] - 0s 49us/step - loss: 0.1880 - acc: 0.9431 - val_loss: 0.1824 - val_acc: 0.9538\n",
      "Epoch 103/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.1816 - acc: 0.9467 - val_loss: 0.1838 - val_acc: 0.9478\n",
      "Epoch 104/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.1790 - acc: 0.9487 - val_loss: 0.1708 - val_acc: 0.9512\n",
      "Epoch 105/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.1744 - acc: 0.9502 - val_loss: 0.1737 - val_acc: 0.9549\n",
      "Epoch 106/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1707 - acc: 0.9519 - val_loss: 0.1665 - val_acc: 0.9540\n",
      "Epoch 107/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.1655 - acc: 0.9555 - val_loss: 0.1578 - val_acc: 0.9545\n",
      "Epoch 108/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.1633 - acc: 0.9551 - val_loss: 0.1558 - val_acc: 0.9570\n",
      "Epoch 109/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.1586 - acc: 0.9553 - val_loss: 0.1499 - val_acc: 0.9601\n",
      "Epoch 110/200\n",
      "4656/4656 [==============================] - 0s 86us/step - loss: 0.1544 - acc: 0.9547 - val_loss: 0.1593 - val_acc: 0.9540\n",
      "Epoch 111/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.1531 - acc: 0.9549 - val_loss: 0.1436 - val_acc: 0.9592\n",
      "Epoch 112/200\n",
      "4656/4656 [==============================] - 0s 81us/step - loss: 0.1429 - acc: 0.9588 - val_loss: 0.1411 - val_acc: 0.9579\n",
      "Epoch 113/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.1421 - acc: 0.9592 - val_loss: 0.1395 - val_acc: 0.9609\n",
      "Epoch 114/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.1389 - acc: 0.9618 - val_loss: 0.1430 - val_acc: 0.9530\n",
      "Epoch 115/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.1388 - acc: 0.9590 - val_loss: 0.1376 - val_acc: 0.9603\n",
      "Epoch 116/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.1317 - acc: 0.9633 - val_loss: 0.1283 - val_acc: 0.9637\n",
      "Epoch 117/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.1288 - acc: 0.9643 - val_loss: 0.1208 - val_acc: 0.9667\n",
      "Epoch 118/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.1275 - acc: 0.9635 - val_loss: 0.1223 - val_acc: 0.9682\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.1235 - acc: 0.9635 - val_loss: 0.1203 - val_acc: 0.9695\n",
      "Epoch 120/200\n",
      "4656/4656 [==============================] - 0s 49us/step - loss: 0.1210 - acc: 0.9671 - val_loss: 0.1225 - val_acc: 0.9646\n",
      "Epoch 121/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.1176 - acc: 0.9665 - val_loss: 0.1124 - val_acc: 0.9689\n",
      "Epoch 122/200\n",
      "4656/4656 [==============================] - 0s 71us/step - loss: 0.1150 - acc: 0.9682 - val_loss: 0.1093 - val_acc: 0.9716\n",
      "Epoch 123/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.1131 - acc: 0.9682 - val_loss: 0.1138 - val_acc: 0.9691\n",
      "Epoch 124/200\n",
      "4656/4656 [==============================] - 0s 67us/step - loss: 0.1090 - acc: 0.9712 - val_loss: 0.1059 - val_acc: 0.9671\n",
      "Epoch 125/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.1067 - acc: 0.9706 - val_loss: 0.1000 - val_acc: 0.9727\n",
      "Epoch 126/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1050 - acc: 0.9723 - val_loss: 0.0976 - val_acc: 0.9764\n",
      "Epoch 127/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.1015 - acc: 0.9708 - val_loss: 0.1001 - val_acc: 0.9736\n",
      "Epoch 128/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.1009 - acc: 0.9736 - val_loss: 0.0956 - val_acc: 0.9727\n",
      "Epoch 129/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.0982 - acc: 0.9732 - val_loss: 0.0925 - val_acc: 0.9757\n",
      "Epoch 130/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0969 - acc: 0.9721 - val_loss: 0.0905 - val_acc: 0.9747\n",
      "Epoch 131/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0946 - acc: 0.9727 - val_loss: 0.0918 - val_acc: 0.9762\n",
      "Epoch 132/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0915 - acc: 0.9755 - val_loss: 0.0884 - val_acc: 0.9766\n",
      "Epoch 133/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0913 - acc: 0.9738 - val_loss: 0.0946 - val_acc: 0.9755\n",
      "Epoch 134/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0886 - acc: 0.9747 - val_loss: 0.0855 - val_acc: 0.9747\n",
      "Epoch 135/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0878 - acc: 0.9757 - val_loss: 0.0867 - val_acc: 0.9708\n",
      "Epoch 136/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0865 - acc: 0.9732 - val_loss: 0.0812 - val_acc: 0.9781\n",
      "Epoch 137/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0851 - acc: 0.9747 - val_loss: 0.1106 - val_acc: 0.9631\n",
      "Epoch 138/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0835 - acc: 0.9751 - val_loss: 0.0781 - val_acc: 0.9781\n",
      "Epoch 139/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.0801 - acc: 0.9757 - val_loss: 0.0749 - val_acc: 0.9794\n",
      "Epoch 140/200\n",
      "4656/4656 [==============================] - 0s 77us/step - loss: 0.0806 - acc: 0.9766 - val_loss: 0.0800 - val_acc: 0.9770\n",
      "Epoch 141/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0789 - acc: 0.9785 - val_loss: 0.0727 - val_acc: 0.9796\n",
      "Epoch 142/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0779 - acc: 0.9785 - val_loss: 0.0746 - val_acc: 0.9785\n",
      "Epoch 143/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0756 - acc: 0.9774 - val_loss: 0.0721 - val_acc: 0.9790\n",
      "Epoch 144/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0748 - acc: 0.9779 - val_loss: 0.0718 - val_acc: 0.9792\n",
      "Epoch 145/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0739 - acc: 0.9783 - val_loss: 0.0693 - val_acc: 0.9796\n",
      "Epoch 146/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0732 - acc: 0.9790 - val_loss: 0.0676 - val_acc: 0.9817\n",
      "Epoch 147/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0723 - acc: 0.9790 - val_loss: 0.0755 - val_acc: 0.9811\n",
      "Epoch 148/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0692 - acc: 0.9774 - val_loss: 0.0679 - val_acc: 0.9798\n",
      "Epoch 149/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0688 - acc: 0.9796 - val_loss: 0.0673 - val_acc: 0.9794\n",
      "Epoch 150/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0690 - acc: 0.9790 - val_loss: 0.0648 - val_acc: 0.9813\n",
      "Epoch 151/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0676 - acc: 0.9807 - val_loss: 0.0627 - val_acc: 0.9817\n",
      "Epoch 152/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0642 - acc: 0.9811 - val_loss: 0.0728 - val_acc: 0.9794\n",
      "Epoch 153/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0643 - acc: 0.9822 - val_loss: 0.0603 - val_acc: 0.9822\n",
      "Epoch 154/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0634 - acc: 0.9813 - val_loss: 0.0671 - val_acc: 0.9820\n",
      "Epoch 155/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0625 - acc: 0.9813 - val_loss: 0.0571 - val_acc: 0.9835\n",
      "Epoch 156/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0622 - acc: 0.9815 - val_loss: 0.0627 - val_acc: 0.9798\n",
      "Epoch 157/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0604 - acc: 0.9830 - val_loss: 0.0562 - val_acc: 0.9843\n",
      "Epoch 158/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0594 - acc: 0.9835 - val_loss: 0.0610 - val_acc: 0.9809\n",
      "Epoch 159/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0587 - acc: 0.9832 - val_loss: 0.0546 - val_acc: 0.9852\n",
      "Epoch 160/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0574 - acc: 0.9822 - val_loss: 0.0591 - val_acc: 0.9835\n",
      "Epoch 161/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0592 - acc: 0.9815 - val_loss: 0.0541 - val_acc: 0.9852\n",
      "Epoch 162/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0573 - acc: 0.9824 - val_loss: 0.0561 - val_acc: 0.9839\n",
      "Epoch 163/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0561 - acc: 0.9832 - val_loss: 0.0600 - val_acc: 0.9809\n",
      "Epoch 164/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.0565 - acc: 0.9839 - val_loss: 0.0543 - val_acc: 0.9858\n",
      "Epoch 165/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0548 - acc: 0.9845 - val_loss: 0.0518 - val_acc: 0.9878\n",
      "Epoch 166/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0537 - acc: 0.9843 - val_loss: 0.0512 - val_acc: 0.9858\n",
      "Epoch 167/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.0528 - acc: 0.9860 - val_loss: 0.0597 - val_acc: 0.9826\n",
      "Epoch 168/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0529 - acc: 0.9850 - val_loss: 0.0524 - val_acc: 0.9852\n",
      "Epoch 169/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0524 - acc: 0.9843 - val_loss: 0.0518 - val_acc: 0.9848\n",
      "Epoch 170/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0501 - acc: 0.9875 - val_loss: 0.0505 - val_acc: 0.9832\n",
      "Epoch 171/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0513 - acc: 0.9860 - val_loss: 0.0525 - val_acc: 0.9848\n",
      "Epoch 172/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0504 - acc: 0.9867 - val_loss: 0.0483 - val_acc: 0.9893\n",
      "Epoch 173/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0489 - acc: 0.9865 - val_loss: 0.0473 - val_acc: 0.9869\n",
      "Epoch 174/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.0481 - acc: 0.9886 - val_loss: 0.0455 - val_acc: 0.9873\n",
      "Epoch 175/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.0486 - acc: 0.9867 - val_loss: 0.0446 - val_acc: 0.9893\n",
      "Epoch 176/200\n",
      "4656/4656 [==============================] - 0s 65us/step - loss: 0.0482 - acc: 0.9871 - val_loss: 0.0497 - val_acc: 0.9837\n",
      "Epoch 177/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0467 - acc: 0.9878 - val_loss: 0.0722 - val_acc: 0.9744\n",
      "Epoch 178/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0470 - acc: 0.9865 - val_loss: 0.0461 - val_acc: 0.9882\n",
      "Epoch 179/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0472 - acc: 0.9882 - val_loss: 0.0470 - val_acc: 0.9865\n",
      "Epoch 180/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0457 - acc: 0.9882 - val_loss: 0.0432 - val_acc: 0.9888\n",
      "Epoch 181/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0452 - acc: 0.9880 - val_loss: 0.0422 - val_acc: 0.9880\n",
      "Epoch 182/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.0442 - acc: 0.9886 - val_loss: 0.0458 - val_acc: 0.9878\n",
      "Epoch 183/200\n",
      "4656/4656 [==============================] - 0s 49us/step - loss: 0.0450 - acc: 0.9884 - val_loss: 0.0501 - val_acc: 0.9888\n",
      "Epoch 184/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0450 - acc: 0.9880 - val_loss: 0.0433 - val_acc: 0.9901\n",
      "Epoch 185/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.0439 - acc: 0.9890 - val_loss: 0.0403 - val_acc: 0.9903\n",
      "Epoch 186/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0430 - acc: 0.9888 - val_loss: 0.0450 - val_acc: 0.9882\n",
      "Epoch 187/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0416 - acc: 0.9888 - val_loss: 0.0398 - val_acc: 0.9903\n",
      "Epoch 188/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0427 - acc: 0.9893 - val_loss: 0.0407 - val_acc: 0.9897\n",
      "Epoch 189/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0419 - acc: 0.9893 - val_loss: 0.0447 - val_acc: 0.9888\n",
      "Epoch 190/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0422 - acc: 0.9886 - val_loss: 0.0443 - val_acc: 0.9901\n",
      "Epoch 191/200\n",
      "4656/4656 [==============================] - 0s 49us/step - loss: 0.0432 - acc: 0.9890 - val_loss: 0.0393 - val_acc: 0.9903\n",
      "Epoch 192/200\n",
      "4656/4656 [==============================] - 0s 49us/step - loss: 0.0424 - acc: 0.9888 - val_loss: 0.0381 - val_acc: 0.9905\n",
      "Epoch 193/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0404 - acc: 0.9893 - val_loss: 0.0492 - val_acc: 0.9854\n",
      "Epoch 194/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0411 - acc: 0.9888 - val_loss: 0.0369 - val_acc: 0.9905\n",
      "Epoch 195/200\n",
      "4656/4656 [==============================] - ETA: 0s - loss: 0.0394 - acc: 0.9907- ETA: 0s - loss: 0.0372 - acc: 0.99 - 0s 55us/step - loss: 0.0395 - acc: 0.9905 - val_loss: 0.0387 - val_acc: 0.9905\n",
      "Epoch 196/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.0402 - acc: 0.9890 - val_loss: 0.0374 - val_acc: 0.9901\n",
      "Epoch 197/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.0385 - acc: 0.9901 - val_loss: 0.0381 - val_acc: 0.9901\n",
      "Epoch 198/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0393 - acc: 0.9897 - val_loss: 0.0356 - val_acc: 0.9908\n",
      "Epoch 199/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0402 - acc: 0.9895 - val_loss: 0.0362 - val_acc: 0.9912\n",
      "Epoch 200/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.0384 - acc: 0.9899 - val_loss: 0.0357 - val_acc: 0.9910\n",
      "1164/1164 [==============================] - 0s 70us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4656 samples, validate on 4656 samples\n",
      "Epoch 1/200\n",
      "4656/4656 [==============================] - 3s 628us/step - loss: 0.9632 - acc: 0.5754 - val_loss: 0.9015 - val_acc: 0.6164\n",
      "Epoch 2/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.8884 - acc: 0.6171 - val_loss: 0.8731 - val_acc: 0.6190\n",
      "Epoch 3/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.8688 - acc: 0.6192 - val_loss: 0.8590 - val_acc: 0.6171\n",
      "Epoch 4/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.8589 - acc: 0.6209 - val_loss: 0.8523 - val_acc: 0.6267\n",
      "Epoch 5/200\n",
      "4656/4656 [==============================] - 0s 66us/step - loss: 0.8481 - acc: 0.6261 - val_loss: 0.8410 - val_acc: 0.6302\n",
      "Epoch 6/200\n",
      "4656/4656 [==============================] - 0s 89us/step - loss: 0.8417 - acc: 0.6297 - val_loss: 0.8319 - val_acc: 0.6342\n",
      "Epoch 7/200\n",
      "4656/4656 [==============================] - 0s 89us/step - loss: 0.8343 - acc: 0.6293 - val_loss: 0.8260 - val_acc: 0.6347\n",
      "Epoch 8/200\n",
      "4656/4656 [==============================] - ETA: 0s - loss: 0.8262 - acc: 0.633 - 0s 66us/step - loss: 0.8254 - acc: 0.6342 - val_loss: 0.8217 - val_acc: 0.6229\n",
      "Epoch 9/200\n",
      "4656/4656 [==============================] - 0s 98us/step - loss: 0.8195 - acc: 0.6321 - val_loss: 0.8095 - val_acc: 0.6418\n",
      "Epoch 10/200\n",
      "4656/4656 [==============================] - 0s 101us/step - loss: 0.8109 - acc: 0.6394 - val_loss: 0.8090 - val_acc: 0.6366\n",
      "Epoch 11/200\n",
      "4656/4656 [==============================] - 0s 79us/step - loss: 0.8043 - acc: 0.6383 - val_loss: 0.7941 - val_acc: 0.6433\n",
      "Epoch 12/200\n",
      "4656/4656 [==============================] - 0s 106us/step - loss: 0.7960 - acc: 0.6452 - val_loss: 0.7862 - val_acc: 0.6411\n",
      "Epoch 13/200\n",
      "4656/4656 [==============================] - 0s 73us/step - loss: 0.7887 - acc: 0.6482 - val_loss: 0.7783 - val_acc: 0.6555\n",
      "Epoch 14/200\n",
      "4656/4656 [==============================] - 1s 150us/step - loss: 0.7812 - acc: 0.6495 - val_loss: 0.7750 - val_acc: 0.6523\n",
      "Epoch 15/200\n",
      "4656/4656 [==============================] - 0s 87us/step - loss: 0.7740 - acc: 0.6542 - val_loss: 0.7639 - val_acc: 0.6682\n",
      "Epoch 16/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.7678 - acc: 0.6613 - val_loss: 0.7592 - val_acc: 0.6669\n",
      "Epoch 17/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.7598 - acc: 0.6626 - val_loss: 0.7483 - val_acc: 0.6701\n",
      "Epoch 18/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.7533 - acc: 0.6628 - val_loss: 0.7452 - val_acc: 0.6757\n",
      "Epoch 19/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.7451 - acc: 0.6780 - val_loss: 0.7356 - val_acc: 0.6819\n",
      "Epoch 20/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.7385 - acc: 0.6791 - val_loss: 0.7276 - val_acc: 0.6854\n",
      "Epoch 21/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.7330 - acc: 0.6862 - val_loss: 0.7215 - val_acc: 0.6796\n",
      "Epoch 22/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.7239 - acc: 0.6866 - val_loss: 0.7172 - val_acc: 0.6976\n",
      "Epoch 23/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.7164 - acc: 0.6980 - val_loss: 0.7103 - val_acc: 0.7002\n",
      "Epoch 24/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.7131 - acc: 0.6985 - val_loss: 0.7039 - val_acc: 0.7008\n",
      "Epoch 25/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.7033 - acc: 0.7081 - val_loss: 0.7124 - val_acc: 0.6903\n",
      "Epoch 26/200\n",
      "4656/4656 [==============================] - 0s 70us/step - loss: 0.6980 - acc: 0.7092 - val_loss: 0.6878 - val_acc: 0.7126\n",
      "Epoch 27/200\n",
      "4656/4656 [==============================] - 0s 68us/step - loss: 0.6910 - acc: 0.7137 - val_loss: 0.6835 - val_acc: 0.7081\n",
      "Epoch 28/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.6864 - acc: 0.7126 - val_loss: 0.6712 - val_acc: 0.7232\n",
      "Epoch 29/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.6769 - acc: 0.7210 - val_loss: 0.6677 - val_acc: 0.7320\n",
      "Epoch 30/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.6719 - acc: 0.7255 - val_loss: 0.6643 - val_acc: 0.7229\n",
      "Epoch 31/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.6626 - acc: 0.7305 - val_loss: 0.6583 - val_acc: 0.7436\n",
      "Epoch 32/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.6572 - acc: 0.7408 - val_loss: 0.6533 - val_acc: 0.7311\n",
      "Epoch 33/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.6498 - acc: 0.7468 - val_loss: 0.6421 - val_acc: 0.7491\n",
      "Epoch 34/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.6445 - acc: 0.7504 - val_loss: 0.6373 - val_acc: 0.7547\n",
      "Epoch 35/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.6377 - acc: 0.7526 - val_loss: 0.6240 - val_acc: 0.7612\n",
      "Epoch 36/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.6267 - acc: 0.7614 - val_loss: 0.6151 - val_acc: 0.7640\n",
      "Epoch 37/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.6187 - acc: 0.7616 - val_loss: 0.6074 - val_acc: 0.7693\n",
      "Epoch 38/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.6115 - acc: 0.7693 - val_loss: 0.6053 - val_acc: 0.7719\n",
      "Epoch 39/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.6043 - acc: 0.7745 - val_loss: 0.5932 - val_acc: 0.7811\n",
      "Epoch 40/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.5969 - acc: 0.7751 - val_loss: 0.5905 - val_acc: 0.7831\n",
      "Epoch 41/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.5901 - acc: 0.7762 - val_loss: 0.5800 - val_acc: 0.7869\n",
      "Epoch 42/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.5798 - acc: 0.7841 - val_loss: 0.5733 - val_acc: 0.7846\n",
      "Epoch 43/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.5709 - acc: 0.7902 - val_loss: 0.5739 - val_acc: 0.7936\n",
      "Epoch 44/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.5650 - acc: 0.7910 - val_loss: 0.5579 - val_acc: 0.7938\n",
      "Epoch 45/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.5594 - acc: 0.7921 - val_loss: 0.5461 - val_acc: 0.8026\n",
      "Epoch 46/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.5492 - acc: 0.7973 - val_loss: 0.5512 - val_acc: 0.7917\n",
      "Epoch 47/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.5414 - acc: 0.8013 - val_loss: 0.5319 - val_acc: 0.8061\n",
      "Epoch 48/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.5335 - acc: 0.8054 - val_loss: 0.5220 - val_acc: 0.8162\n",
      "Epoch 49/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.5257 - acc: 0.8033 - val_loss: 0.5197 - val_acc: 0.8146\n",
      "Epoch 50/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.5180 - acc: 0.8095 - val_loss: 0.5111 - val_acc: 0.8123\n",
      "Epoch 51/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.5114 - acc: 0.8153 - val_loss: 0.5059 - val_acc: 0.8108\n",
      "Epoch 52/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.5027 - acc: 0.8155 - val_loss: 0.4978 - val_acc: 0.8224\n",
      "Epoch 53/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.4965 - acc: 0.8162 - val_loss: 0.4895 - val_acc: 0.8204\n",
      "Epoch 54/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.4887 - acc: 0.8170 - val_loss: 0.4898 - val_acc: 0.8269\n",
      "Epoch 55/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.4800 - acc: 0.8239 - val_loss: 0.4737 - val_acc: 0.8342\n",
      "Epoch 56/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.4730 - acc: 0.8293 - val_loss: 0.4740 - val_acc: 0.8295\n",
      "Epoch 57/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.4644 - acc: 0.8329 - val_loss: 0.4528 - val_acc: 0.8271\n",
      "Epoch 58/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.4569 - acc: 0.8353 - val_loss: 0.4618 - val_acc: 0.8194\n",
      "Epoch 59/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.4492 - acc: 0.8363 - val_loss: 0.4528 - val_acc: 0.8351\n",
      "Epoch 60/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.4396 - acc: 0.8411 - val_loss: 0.4261 - val_acc: 0.8497\n",
      "Epoch 61/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.4321 - acc: 0.8482 - val_loss: 0.4235 - val_acc: 0.8565\n",
      "Epoch 62/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.4218 - acc: 0.8486 - val_loss: 0.4383 - val_acc: 0.8527\n",
      "Epoch 63/200\n",
      "4656/4656 [==============================] - 0s 50us/step - loss: 0.4117 - acc: 0.8518 - val_loss: 0.4043 - val_acc: 0.8602\n",
      "Epoch 64/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.4019 - acc: 0.8572 - val_loss: 0.3877 - val_acc: 0.8565\n",
      "Epoch 65/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.3906 - acc: 0.8634 - val_loss: 0.3793 - val_acc: 0.8679\n",
      "Epoch 66/200\n",
      "4656/4656 [==============================] - 0s 70us/step - loss: 0.3812 - acc: 0.8625 - val_loss: 0.3681 - val_acc: 0.8799\n",
      "Epoch 67/200\n",
      "4656/4656 [==============================] - 0s 76us/step - loss: 0.3684 - acc: 0.8722 - val_loss: 0.3642 - val_acc: 0.8729\n",
      "Epoch 68/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.3592 - acc: 0.8729 - val_loss: 0.3568 - val_acc: 0.8857\n",
      "Epoch 69/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.3493 - acc: 0.8821 - val_loss: 0.3375 - val_acc: 0.8780\n",
      "Epoch 70/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.3395 - acc: 0.8836 - val_loss: 0.3271 - val_acc: 0.8991\n",
      "Epoch 71/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.3292 - acc: 0.8892 - val_loss: 0.3194 - val_acc: 0.9025\n",
      "Epoch 72/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.3156 - acc: 0.8967 - val_loss: 0.3102 - val_acc: 0.9021\n",
      "Epoch 73/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.3047 - acc: 0.9018 - val_loss: 0.2917 - val_acc: 0.9087\n",
      "Epoch 74/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.2929 - acc: 0.9098 - val_loss: 0.2879 - val_acc: 0.8991\n",
      "Epoch 75/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.2862 - acc: 0.9057 - val_loss: 0.2793 - val_acc: 0.9188\n",
      "Epoch 76/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.2749 - acc: 0.9145 - val_loss: 0.2673 - val_acc: 0.9171\n",
      "Epoch 77/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.2634 - acc: 0.9203 - val_loss: 0.2542 - val_acc: 0.9225\n",
      "Epoch 78/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.2532 - acc: 0.9225 - val_loss: 0.2514 - val_acc: 0.9244\n",
      "Epoch 79/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.2444 - acc: 0.9283 - val_loss: 0.2323 - val_acc: 0.9278\n",
      "Epoch 80/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.2363 - acc: 0.9302 - val_loss: 0.2258 - val_acc: 0.9349\n",
      "Epoch 81/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.2279 - acc: 0.9354 - val_loss: 0.2192 - val_acc: 0.9336\n",
      "Epoch 82/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.2195 - acc: 0.9349 - val_loss: 0.2098 - val_acc: 0.9392\n",
      "Epoch 83/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.2126 - acc: 0.9360 - val_loss: 0.2064 - val_acc: 0.9433\n",
      "Epoch 84/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.2053 - acc: 0.9399 - val_loss: 0.2055 - val_acc: 0.9433\n",
      "Epoch 85/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1974 - acc: 0.9416 - val_loss: 0.1904 - val_acc: 0.9371\n",
      "Epoch 86/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1884 - acc: 0.9435 - val_loss: 0.1832 - val_acc: 0.9461\n",
      "Epoch 87/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.1868 - acc: 0.9457 - val_loss: 0.1993 - val_acc: 0.9390\n",
      "Epoch 88/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1796 - acc: 0.9446 - val_loss: 0.1707 - val_acc: 0.9491\n",
      "Epoch 89/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1730 - acc: 0.9487 - val_loss: 0.1698 - val_acc: 0.9517\n",
      "Epoch 90/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.1677 - acc: 0.9495 - val_loss: 0.1597 - val_acc: 0.9579\n",
      "Epoch 91/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.1618 - acc: 0.9543 - val_loss: 0.1527 - val_acc: 0.9577\n",
      "Epoch 92/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.1562 - acc: 0.9538 - val_loss: 0.1615 - val_acc: 0.9474\n",
      "Epoch 93/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1530 - acc: 0.9547 - val_loss: 0.1496 - val_acc: 0.9562\n",
      "Epoch 94/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.1481 - acc: 0.9566 - val_loss: 0.1400 - val_acc: 0.9609\n",
      "Epoch 95/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.1421 - acc: 0.9607 - val_loss: 0.1371 - val_acc: 0.9635\n",
      "Epoch 96/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.1394 - acc: 0.9594 - val_loss: 0.1331 - val_acc: 0.9585\n",
      "Epoch 97/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.1373 - acc: 0.9601 - val_loss: 0.1313 - val_acc: 0.9641\n",
      "Epoch 98/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.1320 - acc: 0.9633 - val_loss: 0.1285 - val_acc: 0.9618\n",
      "Epoch 99/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.1286 - acc: 0.9613 - val_loss: 0.1238 - val_acc: 0.9684\n",
      "Epoch 100/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.1242 - acc: 0.9654 - val_loss: 0.1216 - val_acc: 0.9663\n",
      "Epoch 101/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.1211 - acc: 0.9650 - val_loss: 0.1130 - val_acc: 0.9674\n",
      "Epoch 102/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.1202 - acc: 0.9674 - val_loss: 0.1153 - val_acc: 0.9674\n",
      "Epoch 103/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.1145 - acc: 0.9661 - val_loss: 0.1177 - val_acc: 0.9682\n",
      "Epoch 104/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.1134 - acc: 0.9686 - val_loss: 0.1040 - val_acc: 0.9701\n",
      "Epoch 105/200\n",
      "4656/4656 [==============================] - 0s 74us/step - loss: 0.1105 - acc: 0.9693 - val_loss: 0.1125 - val_acc: 0.9661\n",
      "Epoch 106/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.1070 - acc: 0.9693 - val_loss: 0.1034 - val_acc: 0.9751\n",
      "Epoch 107/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.1035 - acc: 0.9721 - val_loss: 0.1117 - val_acc: 0.9701\n",
      "Epoch 108/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.1013 - acc: 0.9689 - val_loss: 0.1010 - val_acc: 0.9699\n",
      "Epoch 109/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.1000 - acc: 0.9710 - val_loss: 0.0999 - val_acc: 0.9736\n",
      "Epoch 110/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0974 - acc: 0.9725 - val_loss: 0.0963 - val_acc: 0.9732\n",
      "Epoch 111/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0965 - acc: 0.9725 - val_loss: 0.0895 - val_acc: 0.9738\n",
      "Epoch 112/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0908 - acc: 0.9749 - val_loss: 0.0939 - val_acc: 0.9744\n",
      "Epoch 113/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0923 - acc: 0.9774 - val_loss: 0.0943 - val_acc: 0.9680\n",
      "Epoch 114/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0907 - acc: 0.9744 - val_loss: 0.0870 - val_acc: 0.9774\n",
      "Epoch 115/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0882 - acc: 0.9757 - val_loss: 0.0813 - val_acc: 0.9762\n",
      "Epoch 116/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0851 - acc: 0.9759 - val_loss: 0.0808 - val_acc: 0.9774\n",
      "Epoch 117/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0831 - acc: 0.9768 - val_loss: 0.0907 - val_acc: 0.9734\n",
      "Epoch 118/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0820 - acc: 0.9766 - val_loss: 0.0840 - val_acc: 0.9783\n",
      "Epoch 119/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0798 - acc: 0.9777 - val_loss: 0.0793 - val_acc: 0.9796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0774 - acc: 0.9790 - val_loss: 0.0748 - val_acc: 0.9828\n",
      "Epoch 121/200\n",
      "4656/4656 [==============================] - 0s 50us/step - loss: 0.0747 - acc: 0.9805 - val_loss: 0.0853 - val_acc: 0.9783\n",
      "Epoch 122/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0770 - acc: 0.9790 - val_loss: 0.0679 - val_acc: 0.9835\n",
      "Epoch 123/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0729 - acc: 0.9807 - val_loss: 0.0752 - val_acc: 0.9802\n",
      "Epoch 124/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.0738 - acc: 0.9800 - val_loss: 0.0669 - val_acc: 0.9835\n",
      "Epoch 125/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0707 - acc: 0.9809 - val_loss: 0.0689 - val_acc: 0.9811\n",
      "Epoch 126/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0700 - acc: 0.9809 - val_loss: 0.0664 - val_acc: 0.9835\n",
      "Epoch 127/200\n",
      "4656/4656 [==============================] - 0s 52us/step - loss: 0.0668 - acc: 0.9837 - val_loss: 0.0682 - val_acc: 0.9811\n",
      "Epoch 128/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0680 - acc: 0.9835 - val_loss: 0.0608 - val_acc: 0.9837\n",
      "Epoch 129/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0659 - acc: 0.9822 - val_loss: 0.0611 - val_acc: 0.9845\n",
      "Epoch 130/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0630 - acc: 0.9839 - val_loss: 0.0748 - val_acc: 0.9781\n",
      "Epoch 131/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0632 - acc: 0.9848 - val_loss: 0.0617 - val_acc: 0.9856\n",
      "Epoch 132/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0611 - acc: 0.9863 - val_loss: 0.0590 - val_acc: 0.9860\n",
      "Epoch 133/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.0618 - acc: 0.9839 - val_loss: 0.0622 - val_acc: 0.9867\n",
      "Epoch 134/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0611 - acc: 0.9848 - val_loss: 0.0552 - val_acc: 0.9882\n",
      "Epoch 135/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.0589 - acc: 0.9854 - val_loss: 0.0610 - val_acc: 0.9852\n",
      "Epoch 136/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0580 - acc: 0.9863 - val_loss: 0.0543 - val_acc: 0.9865\n",
      "Epoch 137/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0570 - acc: 0.9854 - val_loss: 0.0563 - val_acc: 0.9869\n",
      "Epoch 138/200\n",
      "4656/4656 [==============================] - 0s 49us/step - loss: 0.0561 - acc: 0.9848 - val_loss: 0.0654 - val_acc: 0.9852\n",
      "Epoch 139/200\n",
      "4656/4656 [==============================] - 0s 53us/step - loss: 0.0551 - acc: 0.9865 - val_loss: 0.0495 - val_acc: 0.9884\n",
      "Epoch 140/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0529 - acc: 0.9882 - val_loss: 0.0516 - val_acc: 0.9875\n",
      "Epoch 141/200\n",
      "4656/4656 [==============================] - 0s 51us/step - loss: 0.0538 - acc: 0.9867 - val_loss: 0.0572 - val_acc: 0.9856\n",
      "Epoch 142/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0521 - acc: 0.9880 - val_loss: 0.0798 - val_acc: 0.9667\n",
      "Epoch 143/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.0537 - acc: 0.9860 - val_loss: 0.0530 - val_acc: 0.9878\n",
      "Epoch 144/200\n",
      "4656/4656 [==============================] - 0s 69us/step - loss: 0.0511 - acc: 0.9871 - val_loss: 0.0478 - val_acc: 0.9897\n",
      "Epoch 145/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.0500 - acc: 0.9873 - val_loss: 0.0454 - val_acc: 0.9882\n",
      "Epoch 146/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0481 - acc: 0.9878 - val_loss: 0.1011 - val_acc: 0.9527\n",
      "Epoch 147/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0464 - acc: 0.9869 - val_loss: 0.0452 - val_acc: 0.9875\n",
      "Epoch 148/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0481 - acc: 0.9863 - val_loss: 0.0476 - val_acc: 0.9888\n",
      "Epoch 149/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0486 - acc: 0.9878 - val_loss: 0.0443 - val_acc: 0.9886\n",
      "Epoch 150/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0468 - acc: 0.9880 - val_loss: 0.0446 - val_acc: 0.9888\n",
      "Epoch 151/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0463 - acc: 0.9869 - val_loss: 0.0480 - val_acc: 0.9895\n",
      "Epoch 152/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0461 - acc: 0.9893 - val_loss: 0.0438 - val_acc: 0.9882\n",
      "Epoch 153/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0482 - val_acc: 0.9869\n",
      "Epoch 154/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0476 - acc: 0.9882 - val_loss: 0.0429 - val_acc: 0.9897\n",
      "Epoch 155/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0439 - acc: 0.9871 - val_loss: 0.0399 - val_acc: 0.9897\n",
      "Epoch 156/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0432 - acc: 0.9882 - val_loss: 0.0436 - val_acc: 0.9886\n",
      "Epoch 157/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0437 - acc: 0.9882 - val_loss: 0.0395 - val_acc: 0.9895\n",
      "Epoch 158/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0435 - acc: 0.9882 - val_loss: 0.0394 - val_acc: 0.9893\n",
      "Epoch 159/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0424 - acc: 0.9875 - val_loss: 0.0387 - val_acc: 0.9884\n",
      "Epoch 160/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0432 - acc: 0.9886 - val_loss: 0.0427 - val_acc: 0.9882\n",
      "Epoch 161/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0433 - acc: 0.9878 - val_loss: 0.0441 - val_acc: 0.9888\n",
      "Epoch 162/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0418 - acc: 0.9875 - val_loss: 0.0369 - val_acc: 0.9901\n",
      "Epoch 163/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0394 - acc: 0.9899 - val_loss: 0.0375 - val_acc: 0.9893\n",
      "Epoch 164/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0402 - acc: 0.9897 - val_loss: 0.0396 - val_acc: 0.9901\n",
      "Epoch 165/200\n",
      "4656/4656 [==============================] - 0s 54us/step - loss: 0.0399 - acc: 0.9901 - val_loss: 0.0399 - val_acc: 0.9888\n",
      "Epoch 166/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0427 - acc: 0.9880 - val_loss: 0.0358 - val_acc: 0.9895\n",
      "Epoch 167/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0393 - acc: 0.9893 - val_loss: 0.0397 - val_acc: 0.9895\n",
      "Epoch 168/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0384 - acc: 0.9903 - val_loss: 0.0358 - val_acc: 0.9905\n",
      "Epoch 169/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0402 - acc: 0.9888 - val_loss: 0.0346 - val_acc: 0.9893\n",
      "Epoch 170/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0388 - acc: 0.9888 - val_loss: 0.0390 - val_acc: 0.9897\n",
      "Epoch 171/200\n",
      "4656/4656 [==============================] - 0s 56us/step - loss: 0.0375 - acc: 0.9890 - val_loss: 0.0366 - val_acc: 0.9901\n",
      "Epoch 172/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0373 - acc: 0.9897 - val_loss: 0.0359 - val_acc: 0.9895\n",
      "Epoch 173/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0385 - acc: 0.9895 - val_loss: 0.0394 - val_acc: 0.9895\n",
      "Epoch 174/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0367 - acc: 0.9888 - val_loss: 0.0356 - val_acc: 0.9905\n",
      "Epoch 175/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0377 - acc: 0.9895 - val_loss: 0.0357 - val_acc: 0.9908\n",
      "Epoch 176/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0384 - acc: 0.9886 - val_loss: 0.0340 - val_acc: 0.9897\n",
      "Epoch 177/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0368 - acc: 0.9888 - val_loss: 0.0348 - val_acc: 0.9895\n",
      "Epoch 178/200\n",
      "4656/4656 [==============================] - 0s 63us/step - loss: 0.0381 - acc: 0.9888 - val_loss: 0.0343 - val_acc: 0.9910\n",
      "Epoch 179/200\n",
      "4656/4656 [==============================] - 0s 64us/step - loss: 0.0367 - acc: 0.9899 - val_loss: 0.0331 - val_acc: 0.9905\n",
      "Epoch 180/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0352 - acc: 0.9897 - val_loss: 0.0348 - val_acc: 0.9897\n",
      "Epoch 181/200\n",
      "4656/4656 [==============================] - 0s 62us/step - loss: 0.0371 - acc: 0.9890 - val_loss: 0.0373 - val_acc: 0.9893\n",
      "Epoch 182/200\n",
      "4656/4656 [==============================] - 0s 60us/step - loss: 0.0353 - acc: 0.9897 - val_loss: 0.0360 - val_acc: 0.9903\n",
      "Epoch 183/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0377 - acc: 0.9895 - val_loss: 0.0363 - val_acc: 0.9901\n",
      "Epoch 184/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0360 - acc: 0.9903 - val_loss: 0.0793 - val_acc: 0.9616\n",
      "Epoch 185/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0372 - acc: 0.9884 - val_loss: 0.0368 - val_acc: 0.9897\n",
      "Epoch 186/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0350 - acc: 0.9895 - val_loss: 0.0350 - val_acc: 0.9905\n",
      "Epoch 187/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0364 - acc: 0.9908 - val_loss: 0.0316 - val_acc: 0.9910\n",
      "Epoch 188/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0359 - acc: 0.9901 - val_loss: 0.0323 - val_acc: 0.9912\n",
      "Epoch 189/200\n",
      "4656/4656 [==============================] - 0s 61us/step - loss: 0.0363 - acc: 0.9903 - val_loss: 0.0332 - val_acc: 0.9901\n",
      "Epoch 190/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0343 - acc: 0.9899 - val_loss: 0.0321 - val_acc: 0.9905\n",
      "Epoch 191/200\n",
      "4656/4656 [==============================] - 0s 55us/step - loss: 0.0352 - acc: 0.9901 - val_loss: 0.0348 - val_acc: 0.9901\n",
      "Epoch 192/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0346 - acc: 0.9901 - val_loss: 0.0305 - val_acc: 0.9908\n",
      "Epoch 193/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0347 - acc: 0.9903 - val_loss: 0.0349 - val_acc: 0.9910\n",
      "Epoch 194/200\n",
      "4656/4656 [==============================] - 0s 57us/step - loss: 0.0333 - acc: 0.9910 - val_loss: 0.0310 - val_acc: 0.9908\n",
      "Epoch 195/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0349 - val_acc: 0.9905\n",
      "Epoch 196/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.0419 - val_acc: 0.9910\n",
      "Epoch 197/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0333 - acc: 0.9908 - val_loss: 0.0307 - val_acc: 0.9903\n",
      "Epoch 198/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0329 - acc: 0.9905 - val_loss: 0.0314 - val_acc: 0.9910\n",
      "Epoch 199/200\n",
      "4656/4656 [==============================] - 0s 58us/step - loss: 0.0328 - acc: 0.9901 - val_loss: 0.0335 - val_acc: 0.9895\n",
      "Epoch 200/200\n",
      "4656/4656 [==============================] - 0s 59us/step - loss: 0.0321 - acc: 0.9908 - val_loss: 0.0304 - val_acc: 0.9908\n",
      "1164/1164 [==============================] - 0s 72us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2910 samples, validate on 2910 samples\n",
      "Epoch 1/200\n",
      "2910/2910 [==============================] - 2s 836us/step - loss: 1.0902 - acc: 0.4478 - val_loss: 0.9615 - val_acc: 0.5849\n",
      "Epoch 2/200\n",
      "2910/2910 [==============================] - ETA: 0s - loss: 0.9423 - acc: 0.592 - 0s 68us/step - loss: 0.9373 - acc: 0.5969 - val_loss: 0.9238 - val_acc: 0.6034\n",
      "Epoch 3/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.9137 - acc: 0.6038 - val_loss: 0.9258 - val_acc: 0.6093\n",
      "Epoch 4/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8971 - acc: 0.6089 - val_loss: 0.8908 - val_acc: 0.6079\n",
      "Epoch 5/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.8881 - acc: 0.6093 - val_loss: 0.8828 - val_acc: 0.6103\n",
      "Epoch 6/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8795 - acc: 0.6107 - val_loss: 0.8795 - val_acc: 0.6086\n",
      "Epoch 7/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.8728 - acc: 0.6107 - val_loss: 0.8758 - val_acc: 0.6134\n",
      "Epoch 8/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.8661 - acc: 0.6110 - val_loss: 0.8648 - val_acc: 0.6110\n",
      "Epoch 9/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.8592 - acc: 0.6113 - val_loss: 0.8788 - val_acc: 0.6000\n",
      "Epoch 10/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8565 - acc: 0.6148 - val_loss: 0.8514 - val_acc: 0.6227\n",
      "Epoch 11/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.8497 - acc: 0.6168 - val_loss: 0.8471 - val_acc: 0.6182\n",
      "Epoch 12/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.8454 - acc: 0.6210 - val_loss: 0.8392 - val_acc: 0.6216\n",
      "Epoch 13/200\n",
      "2910/2910 [==============================] - 0s 55us/step - loss: 0.8402 - acc: 0.6241 - val_loss: 0.8403 - val_acc: 0.6265\n",
      "Epoch 14/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8350 - acc: 0.6244 - val_loss: 0.8285 - val_acc: 0.6192\n",
      "Epoch 15/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.8308 - acc: 0.6223 - val_loss: 0.8259 - val_acc: 0.6265\n",
      "Epoch 16/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8259 - acc: 0.6282 - val_loss: 0.8239 - val_acc: 0.6271\n",
      "Epoch 17/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.8216 - acc: 0.6268 - val_loss: 0.8152 - val_acc: 0.6268\n",
      "Epoch 18/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.8199 - acc: 0.6282 - val_loss: 0.8126 - val_acc: 0.6309\n",
      "Epoch 19/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8147 - acc: 0.6313 - val_loss: 0.8152 - val_acc: 0.6230\n",
      "Epoch 20/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.8106 - acc: 0.6278 - val_loss: 0.8141 - val_acc: 0.6282\n",
      "Epoch 21/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.8059 - acc: 0.6344 - val_loss: 0.8095 - val_acc: 0.6313\n",
      "Epoch 22/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.8030 - acc: 0.6296 - val_loss: 0.8000 - val_acc: 0.6440\n",
      "Epoch 23/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.7959 - acc: 0.6337 - val_loss: 0.7933 - val_acc: 0.6330\n",
      "Epoch 24/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.7953 - acc: 0.6351 - val_loss: 0.8113 - val_acc: 0.6282\n",
      "Epoch 25/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.7920 - acc: 0.6354 - val_loss: 0.7842 - val_acc: 0.6409\n",
      "Epoch 26/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.7861 - acc: 0.6436 - val_loss: 0.7885 - val_acc: 0.6412\n",
      "Epoch 27/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.7821 - acc: 0.6433 - val_loss: 0.7818 - val_acc: 0.6388\n",
      "Epoch 28/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.7805 - acc: 0.6426 - val_loss: 0.7833 - val_acc: 0.6522\n",
      "Epoch 29/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.7760 - acc: 0.6488 - val_loss: 0.7770 - val_acc: 0.6505\n",
      "Epoch 30/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.7709 - acc: 0.6464 - val_loss: 0.7854 - val_acc: 0.6333\n",
      "Epoch 31/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.7700 - acc: 0.6430 - val_loss: 0.7734 - val_acc: 0.6512\n",
      "Epoch 32/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.7654 - acc: 0.6488 - val_loss: 0.7840 - val_acc: 0.6512\n",
      "Epoch 33/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.7613 - acc: 0.6553 - val_loss: 0.7682 - val_acc: 0.6577\n",
      "Epoch 34/200\n",
      "2910/2910 [==============================] - 0s 50us/step - loss: 0.7600 - acc: 0.6540 - val_loss: 0.7575 - val_acc: 0.6615\n",
      "Epoch 35/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.7558 - acc: 0.6560 - val_loss: 0.7588 - val_acc: 0.6581\n",
      "Epoch 36/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.7515 - acc: 0.6591 - val_loss: 0.7659 - val_acc: 0.6553\n",
      "Epoch 37/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.7497 - acc: 0.6636 - val_loss: 0.7476 - val_acc: 0.6615\n",
      "Epoch 38/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.7451 - acc: 0.6629 - val_loss: 0.7681 - val_acc: 0.6412\n",
      "Epoch 39/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.7448 - acc: 0.6667 - val_loss: 0.7769 - val_acc: 0.6481\n",
      "Epoch 40/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.7431 - acc: 0.6656 - val_loss: 0.7368 - val_acc: 0.6622\n",
      "Epoch 41/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.7362 - acc: 0.6667 - val_loss: 0.7333 - val_acc: 0.6766\n",
      "Epoch 42/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.7330 - acc: 0.6732 - val_loss: 0.7340 - val_acc: 0.6619\n",
      "Epoch 43/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.7308 - acc: 0.6753 - val_loss: 0.7559 - val_acc: 0.6694\n",
      "Epoch 44/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.7290 - acc: 0.6729 - val_loss: 0.7303 - val_acc: 0.6780\n",
      "Epoch 45/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.7256 - acc: 0.6780 - val_loss: 0.7206 - val_acc: 0.6842\n",
      "Epoch 46/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.7210 - acc: 0.6811 - val_loss: 0.7271 - val_acc: 0.6722\n",
      "Epoch 47/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.7184 - acc: 0.6869 - val_loss: 0.7318 - val_acc: 0.6598\n",
      "Epoch 48/200\n",
      "2910/2910 [==============================] - 0s 115us/step - loss: 0.7167 - acc: 0.6883 - val_loss: 0.7407 - val_acc: 0.6725\n",
      "Epoch 49/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.7130 - acc: 0.6890 - val_loss: 0.7124 - val_acc: 0.6948\n",
      "Epoch 50/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.7087 - acc: 0.6948 - val_loss: 0.7014 - val_acc: 0.6942\n",
      "Epoch 51/200\n",
      "2910/2910 [==============================] - 0s 105us/step - loss: 0.7045 - acc: 0.6921 - val_loss: 0.7045 - val_acc: 0.6794\n",
      "Epoch 52/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.7037 - acc: 0.6911 - val_loss: 0.7265 - val_acc: 0.6746\n",
      "Epoch 53/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.7028 - acc: 0.6979 - val_loss: 0.7055 - val_acc: 0.7007\n",
      "Epoch 54/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.6991 - acc: 0.7003 - val_loss: 0.6937 - val_acc: 0.7027\n",
      "Epoch 55/200\n",
      "2910/2910 [==============================] - 0s 95us/step - loss: 0.6947 - acc: 0.7021 - val_loss: 0.6946 - val_acc: 0.7058\n",
      "Epoch 56/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.6912 - acc: 0.6986 - val_loss: 0.6908 - val_acc: 0.7082\n",
      "Epoch 57/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.6884 - acc: 0.7052 - val_loss: 0.6884 - val_acc: 0.7021\n",
      "Epoch 58/200\n",
      "2910/2910 [==============================] - ETA: 0s - loss: 0.6879 - acc: 0.707 - 0s 68us/step - loss: 0.6847 - acc: 0.7107 - val_loss: 0.6942 - val_acc: 0.7082\n",
      "Epoch 59/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.6840 - acc: 0.7038 - val_loss: 0.6889 - val_acc: 0.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "2910/2910 [==============================] - 0s 107us/step - loss: 0.6790 - acc: 0.7151 - val_loss: 0.6830 - val_acc: 0.7021\n",
      "Epoch 61/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.6770 - acc: 0.7079 - val_loss: 0.6853 - val_acc: 0.7093\n",
      "Epoch 62/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.6748 - acc: 0.7103 - val_loss: 0.6683 - val_acc: 0.7168\n",
      "Epoch 63/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.6736 - acc: 0.7120 - val_loss: 0.6837 - val_acc: 0.7058\n",
      "Epoch 64/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.6697 - acc: 0.7155 - val_loss: 0.6627 - val_acc: 0.7210\n",
      "Epoch 65/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.6671 - acc: 0.7210 - val_loss: 0.6605 - val_acc: 0.7172\n",
      "Epoch 66/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.6630 - acc: 0.7223 - val_loss: 0.6680 - val_acc: 0.7192\n",
      "Epoch 67/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.6612 - acc: 0.7230 - val_loss: 0.6637 - val_acc: 0.7172\n",
      "Epoch 68/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.6597 - acc: 0.7210 - val_loss: 0.6794 - val_acc: 0.7058\n",
      "Epoch 69/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.6547 - acc: 0.7223 - val_loss: 0.6555 - val_acc: 0.7199\n",
      "Epoch 70/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.6550 - acc: 0.7251 - val_loss: 0.6630 - val_acc: 0.7230\n",
      "Epoch 71/200\n",
      "2910/2910 [==============================] - 0s 84us/step - loss: 0.6517 - acc: 0.7278 - val_loss: 0.6963 - val_acc: 0.6935\n",
      "Epoch 72/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.6493 - acc: 0.7282 - val_loss: 0.6412 - val_acc: 0.7378\n",
      "Epoch 73/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.6475 - acc: 0.7344 - val_loss: 0.6623 - val_acc: 0.7216\n",
      "Epoch 74/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.6428 - acc: 0.7313 - val_loss: 0.6598 - val_acc: 0.7213\n",
      "Epoch 75/200\n",
      "2910/2910 [==============================] - 0s 88us/step - loss: 0.6381 - acc: 0.7368 - val_loss: 0.7259 - val_acc: 0.6921\n",
      "Epoch 76/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.6421 - acc: 0.7375 - val_loss: 0.6424 - val_acc: 0.7309\n",
      "Epoch 77/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.6341 - acc: 0.7402 - val_loss: 0.6362 - val_acc: 0.7340\n",
      "Epoch 78/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.6308 - acc: 0.7436 - val_loss: 0.6530 - val_acc: 0.7237\n",
      "Epoch 79/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.6316 - acc: 0.7447 - val_loss: 0.6449 - val_acc: 0.7196\n",
      "Epoch 80/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.6289 - acc: 0.7340 - val_loss: 0.6288 - val_acc: 0.7471\n",
      "Epoch 81/200\n",
      "2910/2910 [==============================] - 0s 88us/step - loss: 0.6212 - acc: 0.7464 - val_loss: 0.6265 - val_acc: 0.7337\n",
      "Epoch 82/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.6220 - acc: 0.7447 - val_loss: 0.6344 - val_acc: 0.7412\n",
      "Epoch 83/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.6237 - acc: 0.7471 - val_loss: 0.6288 - val_acc: 0.7536\n",
      "Epoch 84/200\n",
      "2910/2910 [==============================] - 0s 99us/step - loss: 0.6158 - acc: 0.7581 - val_loss: 0.6075 - val_acc: 0.7584\n",
      "Epoch 85/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.6115 - acc: 0.7584 - val_loss: 0.6044 - val_acc: 0.7622\n",
      "Epoch 86/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.6089 - acc: 0.7584 - val_loss: 0.6111 - val_acc: 0.7515\n",
      "Epoch 87/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.6076 - acc: 0.7529 - val_loss: 0.6087 - val_acc: 0.7629\n",
      "Epoch 88/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.6046 - acc: 0.7557 - val_loss: 0.5953 - val_acc: 0.7674\n",
      "Epoch 89/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.6017 - acc: 0.7622 - val_loss: 0.6019 - val_acc: 0.7570\n",
      "Epoch 90/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.6007 - acc: 0.7646 - val_loss: 0.6460 - val_acc: 0.7244\n",
      "Epoch 91/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.6002 - acc: 0.7649 - val_loss: 0.6067 - val_acc: 0.7509\n",
      "Epoch 92/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.5946 - acc: 0.7680 - val_loss: 0.6041 - val_acc: 0.7522\n",
      "Epoch 93/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.5922 - acc: 0.7674 - val_loss: 0.6167 - val_acc: 0.7450\n",
      "Epoch 94/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5894 - acc: 0.7687 - val_loss: 0.5847 - val_acc: 0.7722\n",
      "Epoch 95/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.5860 - acc: 0.7698 - val_loss: 0.5913 - val_acc: 0.7670\n",
      "Epoch 96/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.5838 - acc: 0.7704 - val_loss: 0.6153 - val_acc: 0.7378\n",
      "Epoch 97/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.5873 - acc: 0.7643 - val_loss: 0.5851 - val_acc: 0.7732\n",
      "Epoch 98/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.5797 - acc: 0.7722 - val_loss: 0.5716 - val_acc: 0.7729\n",
      "Epoch 99/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.5797 - acc: 0.7756 - val_loss: 0.5870 - val_acc: 0.7560\n",
      "Epoch 100/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.5759 - acc: 0.7766 - val_loss: 0.5810 - val_acc: 0.7639\n",
      "Epoch 101/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.5744 - acc: 0.7718 - val_loss: 0.5754 - val_acc: 0.7732\n",
      "Epoch 102/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.5708 - acc: 0.7756 - val_loss: 0.5638 - val_acc: 0.7863\n",
      "Epoch 103/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.5685 - acc: 0.7784 - val_loss: 0.5855 - val_acc: 0.7715\n",
      "Epoch 104/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.5665 - acc: 0.7794 - val_loss: 0.5808 - val_acc: 0.7622\n",
      "Epoch 105/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.5661 - acc: 0.7852 - val_loss: 0.5976 - val_acc: 0.7694\n",
      "Epoch 106/200\n",
      "2910/2910 [==============================] - 0s 153us/step - loss: 0.5624 - acc: 0.7821 - val_loss: 0.5600 - val_acc: 0.7759\n",
      "Epoch 107/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.5586 - acc: 0.7849 - val_loss: 0.5597 - val_acc: 0.7842\n",
      "Epoch 108/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.5549 - acc: 0.7924 - val_loss: 0.5864 - val_acc: 0.7536\n",
      "Epoch 109/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.5581 - acc: 0.7766 - val_loss: 0.5592 - val_acc: 0.7859\n",
      "Epoch 110/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.5491 - acc: 0.7921 - val_loss: 0.5578 - val_acc: 0.7746\n",
      "Epoch 111/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.5509 - acc: 0.7828 - val_loss: 0.6098 - val_acc: 0.7567\n",
      "Epoch 112/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.5478 - acc: 0.7921 - val_loss: 0.5654 - val_acc: 0.7770\n",
      "Epoch 113/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.5448 - acc: 0.7914 - val_loss: 0.5946 - val_acc: 0.7546\n",
      "Epoch 114/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.5416 - acc: 0.7904 - val_loss: 0.5469 - val_acc: 0.7883\n",
      "Epoch 115/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.5362 - acc: 0.7969 - val_loss: 0.5397 - val_acc: 0.7918\n",
      "Epoch 116/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.5370 - acc: 0.7976 - val_loss: 0.5464 - val_acc: 0.7835\n",
      "Epoch 117/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.5341 - acc: 0.7952 - val_loss: 0.5442 - val_acc: 0.7887\n",
      "Epoch 118/200\n",
      "2910/2910 [==============================] - 0s 112us/step - loss: 0.5340 - acc: 0.7945 - val_loss: 0.5289 - val_acc: 0.8010\n",
      "Epoch 119/200\n",
      "2400/2910 [=======================>......] - ETA: 0s - loss: 0.5235 - acc: 0.8071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.141707). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910/2910 [==============================] - 0s 138us/step - loss: 0.5307 - acc: 0.8027 - val_loss: 0.5777 - val_acc: 0.7808\n",
      "Epoch 120/200\n",
      "2910/2910 [==============================] - 0s 80us/step - loss: 0.5304 - acc: 0.8052 - val_loss: 0.5351 - val_acc: 0.7907\n",
      "Epoch 121/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.5276 - acc: 0.8038 - val_loss: 0.5503 - val_acc: 0.7828\n",
      "Epoch 122/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.5266 - acc: 0.8007 - val_loss: 0.5496 - val_acc: 0.7856\n",
      "Epoch 123/200\n",
      "2910/2910 [==============================] - 1s 216us/step - loss: 0.5249 - acc: 0.8027 - val_loss: 0.5429 - val_acc: 0.7952\n",
      "Epoch 124/200\n",
      "2910/2910 [==============================] - 0s 101us/step - loss: 0.5243 - acc: 0.8079 - val_loss: 0.5342 - val_acc: 0.8041\n",
      "Epoch 125/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.5217 - acc: 0.8027 - val_loss: 0.5293 - val_acc: 0.7966\n",
      "Epoch 126/200\n",
      "2910/2910 [==============================] - 0s 96us/step - loss: 0.5179 - acc: 0.8069 - val_loss: 0.5248 - val_acc: 0.7986\n",
      "Epoch 127/200\n",
      "2910/2910 [==============================] - 0s 121us/step - loss: 0.5179 - acc: 0.8069 - val_loss: 0.5822 - val_acc: 0.7794\n",
      "Epoch 128/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.5198 - acc: 0.8086 - val_loss: 0.5146 - val_acc: 0.8021\n",
      "Epoch 129/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.5138 - acc: 0.8079 - val_loss: 0.5177 - val_acc: 0.8110\n",
      "Epoch 130/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.5138 - acc: 0.8089 - val_loss: 0.6045 - val_acc: 0.7533\n",
      "Epoch 131/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.5107 - acc: 0.8124 - val_loss: 0.5419 - val_acc: 0.7876\n",
      "Epoch 132/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.5090 - acc: 0.8124 - val_loss: 0.5043 - val_acc: 0.8048\n",
      "Epoch 133/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.5027 - acc: 0.8117 - val_loss: 0.5398 - val_acc: 0.7773\n",
      "Epoch 134/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.5107 - acc: 0.8113 - val_loss: 0.5080 - val_acc: 0.8045\n",
      "Epoch 135/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.5005 - acc: 0.8172 - val_loss: 0.5113 - val_acc: 0.8107\n",
      "Epoch 136/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.5011 - acc: 0.8110 - val_loss: 0.5531 - val_acc: 0.7818\n",
      "Epoch 137/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5047 - acc: 0.8113 - val_loss: 0.4955 - val_acc: 0.8210\n",
      "Epoch 138/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.4972 - acc: 0.8127 - val_loss: 0.5099 - val_acc: 0.7997\n",
      "Epoch 139/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4976 - acc: 0.8162 - val_loss: 0.5253 - val_acc: 0.8034\n",
      "Epoch 140/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4960 - acc: 0.8144 - val_loss: 0.5742 - val_acc: 0.7595\n",
      "Epoch 141/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.4951 - acc: 0.8165 - val_loss: 0.4948 - val_acc: 0.8110\n",
      "Epoch 142/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4928 - acc: 0.8172 - val_loss: 0.4993 - val_acc: 0.8182\n",
      "Epoch 143/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.4917 - acc: 0.8220 - val_loss: 0.5356 - val_acc: 0.7856\n",
      "Epoch 144/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.4890 - acc: 0.8199 - val_loss: 0.5352 - val_acc: 0.7811\n",
      "Epoch 145/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4888 - acc: 0.8155 - val_loss: 0.4943 - val_acc: 0.8113\n",
      "Epoch 146/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.4888 - acc: 0.8189 - val_loss: 0.5105 - val_acc: 0.7948\n",
      "Epoch 147/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.4878 - acc: 0.8165 - val_loss: 0.5178 - val_acc: 0.8031\n",
      "Epoch 148/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.4841 - acc: 0.8223 - val_loss: 0.5081 - val_acc: 0.7959\n",
      "Epoch 149/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4816 - acc: 0.8241 - val_loss: 0.4700 - val_acc: 0.8244\n",
      "Epoch 150/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.4799 - acc: 0.8199 - val_loss: 0.5142 - val_acc: 0.7935\n",
      "Epoch 151/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.4828 - acc: 0.8134 - val_loss: 0.4754 - val_acc: 0.8175\n",
      "Epoch 152/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.4802 - acc: 0.8192 - val_loss: 0.4726 - val_acc: 0.8227\n",
      "Epoch 153/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.4745 - acc: 0.8192 - val_loss: 0.4711 - val_acc: 0.8179\n",
      "Epoch 154/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.4804 - acc: 0.8230 - val_loss: 0.5211 - val_acc: 0.7979\n",
      "Epoch 155/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4738 - acc: 0.8220 - val_loss: 0.4794 - val_acc: 0.8247\n",
      "Epoch 156/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.4731 - acc: 0.8162 - val_loss: 0.4895 - val_acc: 0.8127\n",
      "Epoch 157/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.4740 - acc: 0.8268 - val_loss: 0.4961 - val_acc: 0.8113\n",
      "Epoch 158/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4733 - acc: 0.8196 - val_loss: 0.4819 - val_acc: 0.8151\n",
      "Epoch 159/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.4675 - acc: 0.8210 - val_loss: 0.5043 - val_acc: 0.8096\n",
      "Epoch 160/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4689 - acc: 0.8223 - val_loss: 0.4717 - val_acc: 0.8127\n",
      "Epoch 161/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.4664 - acc: 0.8258 - val_loss: 0.4610 - val_acc: 0.8302\n",
      "Epoch 162/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.4692 - acc: 0.8179 - val_loss: 0.4773 - val_acc: 0.8151\n",
      "Epoch 163/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.4661 - acc: 0.8196 - val_loss: 0.4532 - val_acc: 0.8278\n",
      "Epoch 164/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.4616 - acc: 0.8296 - val_loss: 0.4582 - val_acc: 0.8213\n",
      "Epoch 165/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4589 - acc: 0.8234 - val_loss: 0.4501 - val_acc: 0.8309\n",
      "Epoch 166/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4604 - acc: 0.8234 - val_loss: 0.5328 - val_acc: 0.7997\n",
      "Epoch 167/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.4610 - acc: 0.8275 - val_loss: 0.4596 - val_acc: 0.8285\n",
      "Epoch 168/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.4584 - acc: 0.8244 - val_loss: 0.4872 - val_acc: 0.8017\n",
      "Epoch 169/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.4560 - acc: 0.8227 - val_loss: 0.4656 - val_acc: 0.8045\n",
      "Epoch 170/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4522 - acc: 0.8296 - val_loss: 0.4949 - val_acc: 0.8024\n",
      "Epoch 171/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4517 - acc: 0.8285 - val_loss: 0.4861 - val_acc: 0.8072\n",
      "Epoch 172/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4526 - acc: 0.8230 - val_loss: 0.4450 - val_acc: 0.8320\n",
      "Epoch 173/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4510 - acc: 0.8299 - val_loss: 0.4987 - val_acc: 0.8162\n",
      "Epoch 174/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.4535 - acc: 0.8278 - val_loss: 0.4868 - val_acc: 0.8165\n",
      "Epoch 175/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.4480 - acc: 0.8323 - val_loss: 0.4673 - val_acc: 0.8237\n",
      "Epoch 176/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.4506 - acc: 0.8316 - val_loss: 0.4390 - val_acc: 0.8378\n",
      "Epoch 177/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4473 - acc: 0.8316 - val_loss: 0.4397 - val_acc: 0.8326\n",
      "Epoch 178/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4458 - acc: 0.8254 - val_loss: 0.4963 - val_acc: 0.7962\n",
      "Epoch 179/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.4455 - acc: 0.8265 - val_loss: 0.4951 - val_acc: 0.8144\n",
      "Epoch 180/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.4452 - acc: 0.8354 - val_loss: 0.4894 - val_acc: 0.8206\n",
      "Epoch 181/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.4423 - acc: 0.8344 - val_loss: 0.4991 - val_acc: 0.7918\n",
      "Epoch 182/200\n",
      "2910/2910 [==============================] - 0s 49us/step - loss: 0.4365 - acc: 0.8371 - val_loss: 0.4291 - val_acc: 0.8409\n",
      "Epoch 183/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.4324 - acc: 0.8375 - val_loss: 0.4352 - val_acc: 0.8333\n",
      "Epoch 184/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.4330 - acc: 0.8385 - val_loss: 0.4342 - val_acc: 0.8323\n",
      "Epoch 185/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.4364 - acc: 0.8375 - val_loss: 0.4556 - val_acc: 0.8223\n",
      "Epoch 186/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.4359 - acc: 0.8326 - val_loss: 0.4274 - val_acc: 0.8440\n",
      "Epoch 187/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.4284 - acc: 0.8405 - val_loss: 0.4353 - val_acc: 0.8412\n",
      "Epoch 188/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.4307 - acc: 0.8368 - val_loss: 0.4217 - val_acc: 0.8478\n",
      "Epoch 189/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.4273 - acc: 0.8364 - val_loss: 0.4267 - val_acc: 0.8426\n",
      "Epoch 190/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.4278 - acc: 0.8409 - val_loss: 0.4731 - val_acc: 0.8096\n",
      "Epoch 191/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.4287 - acc: 0.8399 - val_loss: 0.5814 - val_acc: 0.7766\n",
      "Epoch 192/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.4320 - acc: 0.8375 - val_loss: 0.4616 - val_acc: 0.8299\n",
      "Epoch 193/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.4252 - acc: 0.8412 - val_loss: 0.4938 - val_acc: 0.7873\n",
      "Epoch 194/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4267 - acc: 0.8402 - val_loss: 0.4329 - val_acc: 0.8371\n",
      "Epoch 195/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4249 - acc: 0.8399 - val_loss: 0.4177 - val_acc: 0.8399\n",
      "Epoch 196/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.4233 - acc: 0.8409 - val_loss: 0.4648 - val_acc: 0.8227\n",
      "Epoch 197/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.4209 - acc: 0.8423 - val_loss: 0.5191 - val_acc: 0.7904\n",
      "Epoch 198/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.4220 - acc: 0.8409 - val_loss: 0.4215 - val_acc: 0.8433\n",
      "Epoch 199/200\n",
      "2910/2910 [==============================] - 0s 55us/step - loss: 0.4167 - acc: 0.8385 - val_loss: 0.4447 - val_acc: 0.8285\n",
      "Epoch 200/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.4177 - acc: 0.8440 - val_loss: 0.5055 - val_acc: 0.8223\n",
      "2910/2910 [==============================] - 0s 67us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2910 samples, validate on 2910 samples\n",
      "Epoch 1/200\n",
      "2910/2910 [==============================] - 2s 800us/step - loss: 1.0024 - acc: 0.5739 - val_loss: 0.9372 - val_acc: 0.6175\n",
      "Epoch 2/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.9185 - acc: 0.6155 - val_loss: 0.8971 - val_acc: 0.6213\n",
      "Epoch 3/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8880 - acc: 0.6216 - val_loss: 0.8771 - val_acc: 0.6234\n",
      "Epoch 4/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.8740 - acc: 0.6247 - val_loss: 0.8750 - val_acc: 0.6096\n",
      "Epoch 5/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.8605 - acc: 0.6241 - val_loss: 0.8542 - val_acc: 0.6227\n",
      "Epoch 6/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.8494 - acc: 0.6199 - val_loss: 0.8451 - val_acc: 0.6275\n",
      "Epoch 7/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.8421 - acc: 0.6251 - val_loss: 0.8357 - val_acc: 0.6340\n",
      "Epoch 8/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.8347 - acc: 0.6302 - val_loss: 0.8356 - val_acc: 0.6268\n",
      "Epoch 9/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.8285 - acc: 0.6316 - val_loss: 0.8223 - val_acc: 0.6292\n",
      "Epoch 10/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.8224 - acc: 0.6309 - val_loss: 0.8173 - val_acc: 0.6323\n",
      "Epoch 11/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.8177 - acc: 0.6361 - val_loss: 0.8140 - val_acc: 0.6299\n",
      "Epoch 12/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.8121 - acc: 0.6344 - val_loss: 0.8221 - val_acc: 0.6326\n",
      "Epoch 13/200\n",
      "2910/2910 [==============================] - 0s 58us/step - loss: 0.8074 - acc: 0.6333 - val_loss: 0.8049 - val_acc: 0.6354\n",
      "Epoch 14/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.8030 - acc: 0.6395 - val_loss: 0.7970 - val_acc: 0.6368\n",
      "Epoch 15/200\n",
      "2910/2910 [==============================] - 0s 146us/step - loss: 0.7975 - acc: 0.6361 - val_loss: 0.8053 - val_acc: 0.6364\n",
      "Epoch 16/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.7930 - acc: 0.6467 - val_loss: 0.7880 - val_acc: 0.6388\n",
      "Epoch 17/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.7891 - acc: 0.6419 - val_loss: 0.7908 - val_acc: 0.6412\n",
      "Epoch 18/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.7847 - acc: 0.6471 - val_loss: 0.7860 - val_acc: 0.6440\n",
      "Epoch 19/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.7786 - acc: 0.6443 - val_loss: 0.7819 - val_acc: 0.6485\n",
      "Epoch 20/200\n",
      "2910/2910 [==============================] - 0s 111us/step - loss: 0.7758 - acc: 0.6485 - val_loss: 0.7688 - val_acc: 0.6457\n",
      "Epoch 21/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.7708 - acc: 0.6560 - val_loss: 0.7814 - val_acc: 0.6433\n",
      "Epoch 22/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.7681 - acc: 0.6526 - val_loss: 0.7618 - val_acc: 0.6515\n",
      "Epoch 23/200\n",
      "2910/2910 [==============================] - 0s 96us/step - loss: 0.7644 - acc: 0.6557 - val_loss: 0.7584 - val_acc: 0.6509\n",
      "Epoch 24/200\n",
      "2910/2910 [==============================] - 0s 126us/step - loss: 0.7613 - acc: 0.6567 - val_loss: 0.7533 - val_acc: 0.6595\n",
      "Epoch 25/200\n",
      "2910/2910 [==============================] - 0s 135us/step - loss: 0.7566 - acc: 0.6584 - val_loss: 0.7584 - val_acc: 0.6591\n",
      "Epoch 26/200\n",
      "2910/2910 [==============================] - 0s 88us/step - loss: 0.7523 - acc: 0.6591 - val_loss: 0.7447 - val_acc: 0.6550\n",
      "Epoch 27/200\n",
      "2910/2910 [==============================] - 0s 87us/step - loss: 0.7478 - acc: 0.6639 - val_loss: 0.7459 - val_acc: 0.6639\n",
      "Epoch 28/200\n",
      "2910/2910 [==============================] - 0s 84us/step - loss: 0.7446 - acc: 0.6643 - val_loss: 0.7463 - val_acc: 0.6522\n",
      "Epoch 29/200\n",
      "2910/2910 [==============================] - 0s 140us/step - loss: 0.7431 - acc: 0.6595 - val_loss: 0.7406 - val_acc: 0.6619\n",
      "Epoch 30/200\n",
      "2910/2910 [==============================] - 0s 164us/step - loss: 0.7380 - acc: 0.6625 - val_loss: 0.7360 - val_acc: 0.6656\n",
      "Epoch 31/200\n",
      "2910/2910 [==============================] - 0s 124us/step - loss: 0.7334 - acc: 0.6711 - val_loss: 0.7274 - val_acc: 0.6663\n",
      "Epoch 32/200\n",
      "2910/2910 [==============================] - 0s 123us/step - loss: 0.7311 - acc: 0.6646 - val_loss: 0.7321 - val_acc: 0.6646\n",
      "Epoch 33/200\n",
      "2910/2910 [==============================] - 0s 167us/step - loss: 0.7270 - acc: 0.6729 - val_loss: 0.7233 - val_acc: 0.6677\n",
      "Epoch 34/200\n",
      "2910/2910 [==============================] - 0s 93us/step - loss: 0.7250 - acc: 0.6753 - val_loss: 0.7205 - val_acc: 0.6680\n",
      "Epoch 35/200\n",
      "2910/2910 [==============================] - 0s 93us/step - loss: 0.7202 - acc: 0.6759 - val_loss: 0.7241 - val_acc: 0.6667\n",
      "Epoch 36/200\n",
      "2910/2910 [==============================] - 0s 100us/step - loss: 0.7182 - acc: 0.6742 - val_loss: 0.7285 - val_acc: 0.6698\n",
      "Epoch 37/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.7136 - acc: 0.6880 - val_loss: 0.7266 - val_acc: 0.6773\n",
      "Epoch 38/200\n",
      "2910/2910 [==============================] - 0s 80us/step - loss: 0.7119 - acc: 0.6838 - val_loss: 0.7182 - val_acc: 0.6804\n",
      "Epoch 39/200\n",
      "2910/2910 [==============================] - 0s 110us/step - loss: 0.7095 - acc: 0.6842 - val_loss: 0.7039 - val_acc: 0.6924\n",
      "Epoch 40/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.7047 - acc: 0.6890 - val_loss: 0.7062 - val_acc: 0.6869\n",
      "Epoch 41/200\n",
      "2910/2910 [==============================] - 0s 163us/step - loss: 0.7042 - acc: 0.6873 - val_loss: 0.7062 - val_acc: 0.6838\n",
      "Epoch 42/200\n",
      "2910/2910 [==============================] - 0s 153us/step - loss: 0.7023 - acc: 0.6911 - val_loss: 0.6975 - val_acc: 0.6794\n",
      "Epoch 43/200\n",
      "2910/2910 [==============================] - 0s 132us/step - loss: 0.6977 - acc: 0.6948 - val_loss: 0.6960 - val_acc: 0.7003\n",
      "Epoch 44/200\n",
      "2910/2910 [==============================] - 0s 129us/step - loss: 0.6954 - acc: 0.6928 - val_loss: 0.6992 - val_acc: 0.6952\n",
      "Epoch 45/200\n",
      "2910/2910 [==============================] - 0s 119us/step - loss: 0.6958 - acc: 0.6935 - val_loss: 0.7001 - val_acc: 0.6924\n",
      "Epoch 46/200\n",
      "2910/2910 [==============================] - 0s 115us/step - loss: 0.6876 - acc: 0.7024 - val_loss: 0.6911 - val_acc: 0.6873\n",
      "Epoch 47/200\n",
      "2910/2910 [==============================] - 0s 95us/step - loss: 0.6858 - acc: 0.6990 - val_loss: 0.6937 - val_acc: 0.7038\n",
      "Epoch 48/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.6860 - acc: 0.7003 - val_loss: 0.6845 - val_acc: 0.6973\n",
      "Epoch 49/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.6823 - acc: 0.7017 - val_loss: 0.6784 - val_acc: 0.7113\n",
      "Epoch 50/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.6791 - acc: 0.7065 - val_loss: 0.7051 - val_acc: 0.6849\n",
      "Epoch 51/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.6774 - acc: 0.7082 - val_loss: 0.6744 - val_acc: 0.7014\n",
      "Epoch 52/200\n",
      "2910/2910 [==============================] - 0s 101us/step - loss: 0.6745 - acc: 0.7041 - val_loss: 0.6695 - val_acc: 0.7031\n",
      "Epoch 53/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.6732 - acc: 0.7079 - val_loss: 0.6636 - val_acc: 0.7182\n",
      "Epoch 54/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.6721 - acc: 0.7089 - val_loss: 0.6806 - val_acc: 0.6969\n",
      "Epoch 55/200\n",
      "2910/2910 [==============================] - 0s 80us/step - loss: 0.6698 - acc: 0.7034 - val_loss: 0.6699 - val_acc: 0.7038\n",
      "Epoch 56/200\n",
      "2910/2910 [==============================] - 0s 77us/step - loss: 0.6669 - acc: 0.7117 - val_loss: 0.6803 - val_acc: 0.6931\n",
      "Epoch 57/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.6647 - acc: 0.7082 - val_loss: 0.6599 - val_acc: 0.7234\n",
      "Epoch 58/200\n",
      "2910/2910 [==============================] - 0s 114us/step - loss: 0.6622 - acc: 0.7069 - val_loss: 0.6572 - val_acc: 0.7192\n",
      "Epoch 59/200\n",
      "2910/2910 [==============================] - 0s 125us/step - loss: 0.6603 - acc: 0.7113 - val_loss: 0.6625 - val_acc: 0.6990\n",
      "Epoch 60/200\n",
      "2910/2910 [==============================] - 0s 99us/step - loss: 0.6595 - acc: 0.7100 - val_loss: 0.6497 - val_acc: 0.7137\n",
      "Epoch 61/200\n",
      "2910/2910 [==============================] - 0s 77us/step - loss: 0.6553 - acc: 0.7179 - val_loss: 0.6502 - val_acc: 0.7172\n",
      "Epoch 62/200\n",
      "2910/2910 [==============================] - 0s 127us/step - loss: 0.6514 - acc: 0.7189 - val_loss: 0.6519 - val_acc: 0.7196\n",
      "Epoch 63/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.6486 - acc: 0.7220 - val_loss: 0.6748 - val_acc: 0.7027\n",
      "Epoch 64/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.6507 - acc: 0.7216 - val_loss: 0.6518 - val_acc: 0.7131\n",
      "Epoch 65/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.6453 - acc: 0.7196 - val_loss: 0.6445 - val_acc: 0.7141\n",
      "Epoch 66/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.6412 - acc: 0.7213 - val_loss: 0.6737 - val_acc: 0.6993\n",
      "Epoch 67/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.6388 - acc: 0.7271 - val_loss: 0.6398 - val_acc: 0.7285\n",
      "Epoch 68/200\n",
      "2910/2910 [==============================] - 0s 77us/step - loss: 0.6364 - acc: 0.7309 - val_loss: 0.6330 - val_acc: 0.7275\n",
      "Epoch 69/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.6337 - acc: 0.7278 - val_loss: 0.6327 - val_acc: 0.7296\n",
      "Epoch 70/200\n",
      "2910/2910 [==============================] - 0s 87us/step - loss: 0.6321 - acc: 0.7337 - val_loss: 0.6288 - val_acc: 0.7241\n",
      "Epoch 71/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.6297 - acc: 0.7296 - val_loss: 0.6239 - val_acc: 0.7419\n",
      "Epoch 72/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.6295 - acc: 0.7326 - val_loss: 0.6417 - val_acc: 0.7199\n",
      "Epoch 73/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.6273 - acc: 0.7275 - val_loss: 0.6458 - val_acc: 0.7034\n",
      "Epoch 74/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.6252 - acc: 0.7271 - val_loss: 0.6218 - val_acc: 0.7409\n",
      "Epoch 75/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.6209 - acc: 0.7430 - val_loss: 0.6375 - val_acc: 0.7048\n",
      "Epoch 76/200\n",
      "2910/2910 [==============================] - 0s 88us/step - loss: 0.6214 - acc: 0.7333 - val_loss: 0.6303 - val_acc: 0.7216\n",
      "Epoch 77/200\n",
      "2910/2910 [==============================] - 0s 87us/step - loss: 0.6175 - acc: 0.7447 - val_loss: 0.6614 - val_acc: 0.7062\n",
      "Epoch 78/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.6142 - acc: 0.7402 - val_loss: 0.6136 - val_acc: 0.7371\n",
      "Epoch 79/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.6139 - acc: 0.7371 - val_loss: 0.6186 - val_acc: 0.7405\n",
      "Epoch 80/200\n",
      "2910/2910 [==============================] - 0s 124us/step - loss: 0.6087 - acc: 0.7412 - val_loss: 0.6572 - val_acc: 0.7069\n",
      "Epoch 81/200\n",
      "2910/2910 [==============================] - 0s 95us/step - loss: 0.6111 - acc: 0.7505 - val_loss: 0.6119 - val_acc: 0.7454\n",
      "Epoch 82/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.6044 - acc: 0.7433 - val_loss: 0.6214 - val_acc: 0.7213\n",
      "Epoch 83/200\n",
      "2910/2910 [==============================] - 0s 96us/step - loss: 0.6065 - acc: 0.7430 - val_loss: 0.6147 - val_acc: 0.7478\n",
      "Epoch 84/200\n",
      "2910/2910 [==============================] - 0s 84us/step - loss: 0.6018 - acc: 0.7491 - val_loss: 0.5909 - val_acc: 0.7595\n",
      "Epoch 85/200\n",
      "2910/2910 [==============================] - 0s 111us/step - loss: 0.5975 - acc: 0.7485 - val_loss: 0.6145 - val_acc: 0.7320\n",
      "Epoch 86/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.5954 - acc: 0.7454 - val_loss: 0.6071 - val_acc: 0.7316\n",
      "Epoch 87/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.5957 - acc: 0.7543 - val_loss: 0.5973 - val_acc: 0.7526\n",
      "Epoch 88/200\n",
      "2910/2910 [==============================] - 0s 105us/step - loss: 0.5912 - acc: 0.7502 - val_loss: 0.5861 - val_acc: 0.7467\n",
      "Epoch 89/200\n",
      "2910/2910 [==============================] - 0s 101us/step - loss: 0.5885 - acc: 0.7550 - val_loss: 0.6245 - val_acc: 0.7213\n",
      "Epoch 90/200\n",
      "2910/2910 [==============================] - 0s 172us/step - loss: 0.5861 - acc: 0.7605 - val_loss: 0.5819 - val_acc: 0.7656\n",
      "Epoch 91/200\n",
      "2910/2910 [==============================] - 0s 102us/step - loss: 0.5842 - acc: 0.7546 - val_loss: 0.6168 - val_acc: 0.7433\n",
      "Epoch 92/200\n",
      "2910/2910 [==============================] - 0s 139us/step - loss: 0.5847 - acc: 0.7588 - val_loss: 0.5823 - val_acc: 0.7639\n",
      "Epoch 93/200\n",
      "2910/2910 [==============================] - 0s 145us/step - loss: 0.5799 - acc: 0.7581 - val_loss: 0.5752 - val_acc: 0.7529\n",
      "Epoch 94/200\n",
      "2910/2910 [==============================] - 0s 142us/step - loss: 0.5773 - acc: 0.7608 - val_loss: 0.6339 - val_acc: 0.7206\n",
      "Epoch 95/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.5791 - acc: 0.7529 - val_loss: 0.5958 - val_acc: 0.7574\n",
      "Epoch 96/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.5746 - acc: 0.7646 - val_loss: 0.5784 - val_acc: 0.7533\n",
      "Epoch 97/200\n",
      "2910/2910 [==============================] - 0s 95us/step - loss: 0.5720 - acc: 0.7632 - val_loss: 0.5682 - val_acc: 0.7632\n",
      "Epoch 98/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.5680 - acc: 0.7663 - val_loss: 0.5780 - val_acc: 0.7632\n",
      "Epoch 99/200\n",
      "2910/2910 [==============================] - ETA: 0s - loss: 0.5694 - acc: 0.768 - 0s 93us/step - loss: 0.5676 - acc: 0.7670 - val_loss: 0.5751 - val_acc: 0.7612\n",
      "Epoch 100/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5699 - acc: 0.7584 - val_loss: 0.5569 - val_acc: 0.7729\n",
      "Epoch 101/200\n",
      "2910/2910 [==============================] - 0s 50us/step - loss: 0.5640 - acc: 0.7704 - val_loss: 0.5588 - val_acc: 0.7711\n",
      "Epoch 102/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.5612 - acc: 0.7715 - val_loss: 0.5780 - val_acc: 0.7512\n",
      "Epoch 103/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.5611 - acc: 0.7701 - val_loss: 0.5922 - val_acc: 0.7515\n",
      "Epoch 104/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.5602 - acc: 0.7739 - val_loss: 0.5558 - val_acc: 0.7677\n",
      "Epoch 105/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.5551 - acc: 0.7773 - val_loss: 0.5900 - val_acc: 0.7560\n",
      "Epoch 106/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.5563 - acc: 0.7725 - val_loss: 0.5541 - val_acc: 0.7828\n",
      "Epoch 107/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.5526 - acc: 0.7811 - val_loss: 0.5566 - val_acc: 0.7636\n",
      "Epoch 108/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.5513 - acc: 0.7814 - val_loss: 0.5533 - val_acc: 0.7794\n",
      "Epoch 109/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.5476 - acc: 0.7838 - val_loss: 0.5455 - val_acc: 0.7869\n",
      "Epoch 110/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.5466 - acc: 0.7801 - val_loss: 0.5864 - val_acc: 0.7440\n",
      "Epoch 111/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.5466 - acc: 0.7845 - val_loss: 0.5788 - val_acc: 0.7540\n",
      "Epoch 112/200\n",
      "2910/2910 [==============================] - 0s 111us/step - loss: 0.5450 - acc: 0.7780 - val_loss: 0.5726 - val_acc: 0.7502\n",
      "Epoch 113/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.5440 - acc: 0.7845 - val_loss: 0.5588 - val_acc: 0.7564\n",
      "Epoch 114/200\n",
      "2910/2910 [==============================] - 0s 107us/step - loss: 0.5375 - acc: 0.7780 - val_loss: 0.5512 - val_acc: 0.7732\n",
      "Epoch 115/200\n",
      "2910/2910 [==============================] - 0s 96us/step - loss: 0.5402 - acc: 0.7842 - val_loss: 0.5455 - val_acc: 0.7808\n",
      "Epoch 116/200\n",
      "2910/2910 [==============================] - 0s 99us/step - loss: 0.5354 - acc: 0.7832 - val_loss: 0.5417 - val_acc: 0.7900\n",
      "Epoch 117/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.5356 - acc: 0.7918 - val_loss: 0.5273 - val_acc: 0.7869\n",
      "Epoch 118/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.5318 - acc: 0.7897 - val_loss: 0.5577 - val_acc: 0.7560\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.5269 - acc: 0.7887 - val_loss: 0.5333 - val_acc: 0.7849\n",
      "Epoch 120/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.5277 - acc: 0.7883 - val_loss: 0.5283 - val_acc: 0.8017\n",
      "Epoch 121/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.5256 - acc: 0.7907 - val_loss: 0.5845 - val_acc: 0.7625\n",
      "Epoch 122/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.5262 - acc: 0.7914 - val_loss: 0.5309 - val_acc: 0.7801\n",
      "Epoch 123/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5220 - acc: 0.7962 - val_loss: 0.5409 - val_acc: 0.8041\n",
      "Epoch 124/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.5229 - acc: 0.7979 - val_loss: 0.5174 - val_acc: 0.8082\n",
      "Epoch 125/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.5200 - acc: 0.7955 - val_loss: 0.5558 - val_acc: 0.7756\n",
      "Epoch 126/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.5178 - acc: 0.7948 - val_loss: 0.5256 - val_acc: 0.7832\n",
      "Epoch 127/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.5137 - acc: 0.8014 - val_loss: 0.5312 - val_acc: 0.7691\n",
      "Epoch 128/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.5149 - acc: 0.7979 - val_loss: 0.5027 - val_acc: 0.8100\n",
      "Epoch 129/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.5127 - acc: 0.8007 - val_loss: 0.5224 - val_acc: 0.7838\n",
      "Epoch 130/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.5079 - acc: 0.8017 - val_loss: 0.5303 - val_acc: 0.8021\n",
      "Epoch 131/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5074 - acc: 0.8065 - val_loss: 0.5295 - val_acc: 0.7821\n",
      "Epoch 132/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.5095 - acc: 0.8079 - val_loss: 0.5012 - val_acc: 0.8062\n",
      "Epoch 133/200\n",
      "2910/2910 [==============================] - 0s 88us/step - loss: 0.5016 - acc: 0.8021 - val_loss: 0.5298 - val_acc: 0.7904\n",
      "Epoch 134/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.5051 - acc: 0.8024 - val_loss: 0.5134 - val_acc: 0.8131\n",
      "Epoch 135/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.5001 - acc: 0.8038 - val_loss: 0.5137 - val_acc: 0.7873\n",
      "Epoch 136/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4992 - acc: 0.8055 - val_loss: 0.5111 - val_acc: 0.7856\n",
      "Epoch 137/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.4984 - acc: 0.8038 - val_loss: 0.4928 - val_acc: 0.8021\n",
      "Epoch 138/200\n",
      "2910/2910 [==============================] - 0s 171us/step - loss: 0.4921 - acc: 0.8113 - val_loss: 0.5200 - val_acc: 0.7763\n",
      "Epoch 139/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.4969 - acc: 0.8096 - val_loss: 0.5206 - val_acc: 0.7942\n",
      "Epoch 140/200\n",
      "2910/2910 [==============================] - 0s 112us/step - loss: 0.4906 - acc: 0.8107 - val_loss: 0.4977 - val_acc: 0.8000\n",
      "Epoch 141/200\n",
      "2910/2910 [==============================] - 0s 116us/step - loss: 0.4909 - acc: 0.8113 - val_loss: 0.4900 - val_acc: 0.8003\n",
      "Epoch 142/200\n",
      "2910/2910 [==============================] - 0s 100us/step - loss: 0.4848 - acc: 0.8175 - val_loss: 0.5240 - val_acc: 0.8027\n",
      "Epoch 143/200\n",
      "2910/2910 [==============================] - 0s 101us/step - loss: 0.4821 - acc: 0.8117 - val_loss: 0.4764 - val_acc: 0.8086\n",
      "Epoch 144/200\n",
      "2910/2910 [==============================] - 0s 103us/step - loss: 0.4783 - acc: 0.8186 - val_loss: 0.4810 - val_acc: 0.8124\n",
      "Epoch 145/200\n",
      "2910/2910 [==============================] - 0s 91us/step - loss: 0.4810 - acc: 0.8131 - val_loss: 0.5001 - val_acc: 0.7983\n",
      "Epoch 146/200\n",
      "2910/2910 [==============================] - 0s 106us/step - loss: 0.4779 - acc: 0.8192 - val_loss: 0.5077 - val_acc: 0.7924\n",
      "Epoch 147/200\n",
      "2910/2910 [==============================] - 0s 127us/step - loss: 0.4749 - acc: 0.8227 - val_loss: 0.4776 - val_acc: 0.8124\n",
      "Epoch 148/200\n",
      "2910/2910 [==============================] - 0s 103us/step - loss: 0.4756 - acc: 0.8137 - val_loss: 0.4734 - val_acc: 0.8213\n",
      "Epoch 149/200\n",
      "2910/2910 [==============================] - 0s 103us/step - loss: 0.4729 - acc: 0.8155 - val_loss: 0.4922 - val_acc: 0.8086\n",
      "Epoch 150/200\n",
      "2910/2910 [==============================] - 0s 103us/step - loss: 0.4706 - acc: 0.8261 - val_loss: 0.4939 - val_acc: 0.7907\n",
      "Epoch 151/200\n",
      "2910/2910 [==============================] - 0s 154us/step - loss: 0.4686 - acc: 0.8196 - val_loss: 0.4845 - val_acc: 0.8192\n",
      "Epoch 152/200\n",
      "2910/2910 [==============================] - 0s 91us/step - loss: 0.4669 - acc: 0.8227 - val_loss: 0.5228 - val_acc: 0.7873\n",
      "Epoch 153/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.4651 - acc: 0.8278 - val_loss: 0.4868 - val_acc: 0.8024\n",
      "Epoch 154/200\n",
      "2910/2910 [==============================] - 0s 91us/step - loss: 0.4607 - acc: 0.8278 - val_loss: 0.4740 - val_acc: 0.8144\n",
      "Epoch 155/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.4612 - acc: 0.8278 - val_loss: 0.4747 - val_acc: 0.8189\n",
      "Epoch 156/200\n",
      "2910/2910 [==============================] - 0s 142us/step - loss: 0.4557 - acc: 0.8265 - val_loss: 0.4722 - val_acc: 0.8179\n",
      "Epoch 157/200\n",
      "2910/2910 [==============================] - 1s 225us/step - loss: 0.4564 - acc: 0.8347 - val_loss: 0.4550 - val_acc: 0.8368\n",
      "Epoch 158/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.4505 - acc: 0.8333 - val_loss: 0.5449 - val_acc: 0.7622\n",
      "Epoch 159/200\n",
      "2910/2910 [==============================] - 1s 192us/step - loss: 0.4527 - acc: 0.8306 - val_loss: 0.4454 - val_acc: 0.8323\n",
      "Epoch 160/200\n",
      "2910/2910 [==============================] - 0s 160us/step - loss: 0.4489 - acc: 0.8309 - val_loss: 0.4542 - val_acc: 0.8234\n",
      "Epoch 161/200\n",
      "2910/2910 [==============================] - 1s 213us/step - loss: 0.4472 - acc: 0.8316 - val_loss: 0.4561 - val_acc: 0.8309\n",
      "Epoch 162/200\n",
      "2910/2910 [==============================] - 0s 169us/step - loss: 0.4474 - acc: 0.8292 - val_loss: 0.4697 - val_acc: 0.8065\n",
      "Epoch 163/200\n",
      "2910/2910 [==============================] - 1s 175us/step - loss: 0.4441 - acc: 0.8344 - val_loss: 0.4399 - val_acc: 0.8309\n",
      "Epoch 164/200\n",
      "2910/2910 [==============================] - 1s 205us/step - loss: 0.4393 - acc: 0.8320 - val_loss: 0.4875 - val_acc: 0.7904\n",
      "Epoch 165/200\n",
      "2910/2910 [==============================] - 0s 140us/step - loss: 0.4403 - acc: 0.8375 - val_loss: 0.4339 - val_acc: 0.8316\n",
      "Epoch 166/200\n",
      "2910/2910 [==============================] - 0s 99us/step - loss: 0.4360 - acc: 0.8402 - val_loss: 0.4386 - val_acc: 0.8313\n",
      "Epoch 167/200\n",
      "2910/2910 [==============================] - 0s 133us/step - loss: 0.4321 - acc: 0.8402 - val_loss: 0.4489 - val_acc: 0.8186\n",
      "Epoch 168/200\n",
      "2910/2910 [==============================] - 0s 163us/step - loss: 0.4340 - acc: 0.8395 - val_loss: 0.4293 - val_acc: 0.8337\n",
      "Epoch 169/200\n",
      "2910/2910 [==============================] - 0s 140us/step - loss: 0.4309 - acc: 0.8402 - val_loss: 0.4466 - val_acc: 0.8285\n",
      "Epoch 170/200\n",
      "2910/2910 [==============================] - 0s 110us/step - loss: 0.4282 - acc: 0.8385 - val_loss: 0.4300 - val_acc: 0.8405\n",
      "Epoch 171/200\n",
      "2910/2910 [==============================] - 0s 106us/step - loss: 0.4284 - acc: 0.8474 - val_loss: 0.4407 - val_acc: 0.8351\n",
      "Epoch 172/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.4234 - acc: 0.8412 - val_loss: 0.4483 - val_acc: 0.8402\n",
      "Epoch 173/200\n",
      "2910/2910 [==============================] - 0s 139us/step - loss: 0.4215 - acc: 0.8457 - val_loss: 0.4222 - val_acc: 0.8344\n",
      "Epoch 174/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.4197 - acc: 0.8405 - val_loss: 0.4447 - val_acc: 0.8326\n",
      "Epoch 175/200\n",
      "2910/2910 [==============================] - 0s 93us/step - loss: 0.4161 - acc: 0.8467 - val_loss: 0.4586 - val_acc: 0.8131\n",
      "Epoch 176/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.4163 - acc: 0.8447 - val_loss: 0.4260 - val_acc: 0.8351\n",
      "Epoch 177/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.4102 - acc: 0.8485 - val_loss: 0.4935 - val_acc: 0.8038\n",
      "Epoch 178/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.4123 - acc: 0.8509 - val_loss: 0.4249 - val_acc: 0.8313\n",
      "Epoch 179/200\n",
      "2910/2910 [==============================] - 0s 134us/step - loss: 0.4077 - acc: 0.8488 - val_loss: 0.3986 - val_acc: 0.8512\n",
      "Epoch 180/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.4086 - acc: 0.8498 - val_loss: 0.4174 - val_acc: 0.8388\n",
      "Epoch 181/200\n",
      "2910/2910 [==============================] - 0s 100us/step - loss: 0.4026 - acc: 0.8550 - val_loss: 0.4474 - val_acc: 0.8186\n",
      "Epoch 182/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.4028 - acc: 0.8515 - val_loss: 0.4227 - val_acc: 0.8405\n",
      "Epoch 183/200\n",
      "2910/2910 [==============================] - 0s 105us/step - loss: 0.4003 - acc: 0.8553 - val_loss: 0.3892 - val_acc: 0.8570\n",
      "Epoch 184/200\n",
      "2910/2910 [==============================] - 0s 103us/step - loss: 0.3954 - acc: 0.8567 - val_loss: 0.3915 - val_acc: 0.8564\n",
      "Epoch 185/200\n",
      "2910/2910 [==============================] - 0s 93us/step - loss: 0.3967 - acc: 0.8543 - val_loss: 0.3990 - val_acc: 0.8495\n",
      "Epoch 186/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.3958 - acc: 0.8560 - val_loss: 0.3986 - val_acc: 0.8481\n",
      "Epoch 187/200\n",
      "2910/2910 [==============================] - 0s 118us/step - loss: 0.3905 - acc: 0.8567 - val_loss: 0.3942 - val_acc: 0.8581\n",
      "Epoch 188/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.3912 - acc: 0.8584 - val_loss: 0.3834 - val_acc: 0.8605\n",
      "Epoch 189/200\n",
      "2910/2910 [==============================] - 0s 126us/step - loss: 0.3851 - acc: 0.8567 - val_loss: 0.3808 - val_acc: 0.8567\n",
      "Epoch 190/200\n",
      "2910/2910 [==============================] - 0s 100us/step - loss: 0.3846 - acc: 0.8612 - val_loss: 0.4286 - val_acc: 0.8357\n",
      "Epoch 191/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.3846 - acc: 0.8584 - val_loss: 0.4067 - val_acc: 0.8299\n",
      "Epoch 192/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.3796 - acc: 0.8584 - val_loss: 0.3836 - val_acc: 0.8529\n",
      "Epoch 193/200\n",
      "2910/2910 [==============================] - ETA: 0s - loss: 0.3754 - acc: 0.857 - 0s 68us/step - loss: 0.3772 - acc: 0.8557 - val_loss: 0.4295 - val_acc: 0.8179\n",
      "Epoch 194/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.3730 - acc: 0.8629 - val_loss: 0.3832 - val_acc: 0.8512\n",
      "Epoch 195/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.3689 - acc: 0.8670 - val_loss: 0.3982 - val_acc: 0.8454\n",
      "Epoch 196/200\n",
      "2910/2910 [==============================] - 0s 149us/step - loss: 0.3751 - acc: 0.8622 - val_loss: 0.4315 - val_acc: 0.8378\n",
      "Epoch 197/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.3721 - acc: 0.8619 - val_loss: 0.4635 - val_acc: 0.8141\n",
      "Epoch 198/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.3701 - acc: 0.8632 - val_loss: 0.3895 - val_acc: 0.8550\n",
      "Epoch 199/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.3634 - acc: 0.8636 - val_loss: 0.3921 - val_acc: 0.8440\n",
      "Epoch 200/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.3604 - acc: 0.8711 - val_loss: 0.3536 - val_acc: 0.8704\n",
      "2910/2910 [==============================] - 0s 74us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2910 samples, validate on 2910 samples\n",
      "Epoch 1/200\n",
      "2910/2910 [==============================] - 3s 1ms/step - loss: 0.9386 - acc: 0.6021 - val_loss: 0.9004 - val_acc: 0.6127\n",
      "Epoch 2/200\n",
      "2910/2910 [==============================] - 0s 51us/step - loss: 0.8910 - acc: 0.6127 - val_loss: 0.8781 - val_acc: 0.6155\n",
      "Epoch 3/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.8740 - acc: 0.6124 - val_loss: 0.8710 - val_acc: 0.6069\n",
      "Epoch 4/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.8612 - acc: 0.6168 - val_loss: 0.8639 - val_acc: 0.6165\n",
      "Epoch 5/200\n",
      "2910/2910 [==============================] - 0s 50us/step - loss: 0.8513 - acc: 0.6141 - val_loss: 0.8454 - val_acc: 0.6179\n",
      "Epoch 6/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.8452 - acc: 0.6144 - val_loss: 0.8419 - val_acc: 0.6199\n",
      "Epoch 7/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.8388 - acc: 0.6199 - val_loss: 0.8459 - val_acc: 0.6313\n",
      "Epoch 8/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.8344 - acc: 0.6247 - val_loss: 0.8313 - val_acc: 0.6216\n",
      "Epoch 9/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.8286 - acc: 0.6275 - val_loss: 0.8311 - val_acc: 0.6216\n",
      "Epoch 10/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.8241 - acc: 0.6299 - val_loss: 0.8277 - val_acc: 0.6309\n",
      "Epoch 11/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.8184 - acc: 0.6330 - val_loss: 0.8122 - val_acc: 0.6371\n",
      "Epoch 12/200\n",
      "2910/2910 [==============================] - 0s 49us/step - loss: 0.8125 - acc: 0.6392 - val_loss: 0.8168 - val_acc: 0.6337\n",
      "Epoch 13/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.8087 - acc: 0.6347 - val_loss: 0.8038 - val_acc: 0.6402\n",
      "Epoch 14/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.8035 - acc: 0.6381 - val_loss: 0.8023 - val_acc: 0.6351\n",
      "Epoch 15/200\n",
      "2910/2910 [==============================] - 0s 77us/step - loss: 0.7996 - acc: 0.6395 - val_loss: 0.7935 - val_acc: 0.6454\n",
      "Epoch 16/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.7962 - acc: 0.6440 - val_loss: 0.7918 - val_acc: 0.6447\n",
      "Epoch 17/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.7904 - acc: 0.6460 - val_loss: 0.7966 - val_acc: 0.6419\n",
      "Epoch 18/200\n",
      "2910/2910 [==============================] - 0s 111us/step - loss: 0.7877 - acc: 0.6433 - val_loss: 0.7796 - val_acc: 0.6433\n",
      "Epoch 19/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.7813 - acc: 0.6460 - val_loss: 0.7808 - val_acc: 0.6454\n",
      "Epoch 20/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.7795 - acc: 0.6478 - val_loss: 0.7793 - val_acc: 0.6454\n",
      "Epoch 21/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.7731 - acc: 0.6474 - val_loss: 0.7695 - val_acc: 0.6536\n",
      "Epoch 22/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.7677 - acc: 0.6591 - val_loss: 0.7606 - val_acc: 0.6502\n",
      "Epoch 23/200\n",
      "2910/2910 [==============================] - 0s 91us/step - loss: 0.7629 - acc: 0.6519 - val_loss: 0.7597 - val_acc: 0.6509\n",
      "Epoch 24/200\n",
      "2910/2910 [==============================] - 0s 77us/step - loss: 0.7588 - acc: 0.6564 - val_loss: 0.7563 - val_acc: 0.6567\n",
      "Epoch 25/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.7548 - acc: 0.6522 - val_loss: 0.7608 - val_acc: 0.6509\n",
      "Epoch 26/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.7528 - acc: 0.6581 - val_loss: 0.7441 - val_acc: 0.6546\n",
      "Epoch 27/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.7452 - acc: 0.6595 - val_loss: 0.7390 - val_acc: 0.6701\n",
      "Epoch 28/200\n",
      "2910/2910 [==============================] - 0s 102us/step - loss: 0.7408 - acc: 0.6653 - val_loss: 0.7485 - val_acc: 0.6581\n",
      "Epoch 29/200\n",
      "2910/2910 [==============================] - 0s 104us/step - loss: 0.7373 - acc: 0.6625 - val_loss: 0.7360 - val_acc: 0.6612\n",
      "Epoch 30/200\n",
      "2910/2910 [==============================] - 0s 129us/step - loss: 0.7319 - acc: 0.6708 - val_loss: 0.7315 - val_acc: 0.6725\n",
      "Epoch 31/200\n",
      "2910/2910 [==============================] - 0s 164us/step - loss: 0.7290 - acc: 0.6708 - val_loss: 0.7307 - val_acc: 0.6749\n",
      "Epoch 32/200\n",
      "2910/2910 [==============================] - 0s 115us/step - loss: 0.7250 - acc: 0.6735 - val_loss: 0.7226 - val_acc: 0.6746\n",
      "Epoch 33/200\n",
      "2910/2910 [==============================] - 0s 136us/step - loss: 0.7212 - acc: 0.6763 - val_loss: 0.7261 - val_acc: 0.6735\n",
      "Epoch 34/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.7170 - acc: 0.6749 - val_loss: 0.7200 - val_acc: 0.6880\n",
      "Epoch 35/200\n",
      "2910/2910 [==============================] - 0s 114us/step - loss: 0.7094 - acc: 0.6880 - val_loss: 0.7170 - val_acc: 0.6749\n",
      "Epoch 36/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.7059 - acc: 0.6794 - val_loss: 0.7034 - val_acc: 0.6835\n",
      "Epoch 37/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.7060 - acc: 0.6780 - val_loss: 0.7017 - val_acc: 0.6828\n",
      "Epoch 38/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.6996 - acc: 0.6852 - val_loss: 0.6980 - val_acc: 0.6777\n",
      "Epoch 39/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.6948 - acc: 0.6942 - val_loss: 0.6953 - val_acc: 0.6770\n",
      "Epoch 40/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.6936 - acc: 0.6869 - val_loss: 0.7010 - val_acc: 0.6838\n",
      "Epoch 41/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.6894 - acc: 0.6935 - val_loss: 0.7064 - val_acc: 0.6828\n",
      "Epoch 42/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.6862 - acc: 0.6962 - val_loss: 0.6797 - val_acc: 0.7010\n",
      "Epoch 43/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.6805 - acc: 0.7010 - val_loss: 0.6891 - val_acc: 0.6921\n",
      "Epoch 44/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.6773 - acc: 0.6997 - val_loss: 0.6746 - val_acc: 0.7096\n",
      "Epoch 45/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.6733 - acc: 0.7072 - val_loss: 0.6791 - val_acc: 0.6887\n",
      "Epoch 46/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.6697 - acc: 0.7089 - val_loss: 0.6874 - val_acc: 0.6869\n",
      "Epoch 47/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.6660 - acc: 0.7117 - val_loss: 0.6797 - val_acc: 0.6924\n",
      "Epoch 48/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.6598 - acc: 0.7110 - val_loss: 0.6616 - val_acc: 0.7093\n",
      "Epoch 49/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.6561 - acc: 0.7137 - val_loss: 0.6599 - val_acc: 0.7103\n",
      "Epoch 50/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.6539 - acc: 0.7192 - val_loss: 0.6482 - val_acc: 0.7223\n",
      "Epoch 51/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.6483 - acc: 0.7179 - val_loss: 0.6476 - val_acc: 0.7309\n",
      "Epoch 52/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.6450 - acc: 0.7227 - val_loss: 0.6420 - val_acc: 0.7275\n",
      "Epoch 53/200\n",
      "2910/2910 [==============================] - 0s 55us/step - loss: 0.6404 - acc: 0.7230 - val_loss: 0.6620 - val_acc: 0.7124\n",
      "Epoch 54/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.6353 - acc: 0.7268 - val_loss: 0.6299 - val_acc: 0.7351\n",
      "Epoch 55/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.6334 - acc: 0.7285 - val_loss: 0.6312 - val_acc: 0.7265\n",
      "Epoch 56/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.6266 - acc: 0.7340 - val_loss: 0.6239 - val_acc: 0.7323\n",
      "Epoch 57/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.6242 - acc: 0.7326 - val_loss: 0.6301 - val_acc: 0.7296\n",
      "Epoch 58/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.6188 - acc: 0.7330 - val_loss: 0.6189 - val_acc: 0.7292\n",
      "Epoch 59/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.6151 - acc: 0.7388 - val_loss: 0.6059 - val_acc: 0.7536\n",
      "Epoch 60/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.6116 - acc: 0.7419 - val_loss: 0.6024 - val_acc: 0.7436\n",
      "Epoch 61/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.6078 - acc: 0.7433 - val_loss: 0.6068 - val_acc: 0.7402\n",
      "Epoch 62/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.6039 - acc: 0.7440 - val_loss: 0.6118 - val_acc: 0.7443\n",
      "Epoch 63/200\n",
      "2910/2910 [==============================] - 0s 52us/step - loss: 0.6019 - acc: 0.7388 - val_loss: 0.5990 - val_acc: 0.7615\n",
      "Epoch 64/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.5961 - acc: 0.7502 - val_loss: 0.5978 - val_acc: 0.7519\n",
      "Epoch 65/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.5894 - acc: 0.7553 - val_loss: 0.6122 - val_acc: 0.7292\n",
      "Epoch 66/200\n",
      "2910/2910 [==============================] - 0s 75us/step - loss: 0.5891 - acc: 0.7474 - val_loss: 0.5824 - val_acc: 0.7595\n",
      "Epoch 67/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.5827 - acc: 0.7588 - val_loss: 0.5793 - val_acc: 0.7540\n",
      "Epoch 68/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.5783 - acc: 0.7595 - val_loss: 0.5789 - val_acc: 0.7588\n",
      "Epoch 69/200\n",
      "2910/2910 [==============================] - 0s 101us/step - loss: 0.5748 - acc: 0.7622 - val_loss: 0.5970 - val_acc: 0.7402\n",
      "Epoch 70/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.5726 - acc: 0.7574 - val_loss: 0.5767 - val_acc: 0.7643\n",
      "Epoch 71/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.5660 - acc: 0.7663 - val_loss: 0.5652 - val_acc: 0.7595\n",
      "Epoch 72/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.5661 - acc: 0.7667 - val_loss: 0.5663 - val_acc: 0.7653\n",
      "Epoch 73/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.5606 - acc: 0.7759 - val_loss: 0.5607 - val_acc: 0.7842\n",
      "Epoch 74/200\n",
      "2910/2910 [==============================] - 0s 102us/step - loss: 0.5549 - acc: 0.7766 - val_loss: 0.5491 - val_acc: 0.7753\n",
      "Epoch 75/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.5512 - acc: 0.7808 - val_loss: 0.5597 - val_acc: 0.7619\n",
      "Epoch 76/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.5479 - acc: 0.7804 - val_loss: 0.5472 - val_acc: 0.7876\n",
      "Epoch 77/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.5443 - acc: 0.7849 - val_loss: 0.5421 - val_acc: 0.7811\n",
      "Epoch 78/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.5419 - acc: 0.7835 - val_loss: 0.5350 - val_acc: 0.7859\n",
      "Epoch 79/200\n",
      "2910/2910 [==============================] - 0s 111us/step - loss: 0.5374 - acc: 0.7880 - val_loss: 0.5319 - val_acc: 0.7866\n",
      "Epoch 80/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.5346 - acc: 0.7907 - val_loss: 0.5417 - val_acc: 0.7643\n",
      "Epoch 81/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5289 - acc: 0.7935 - val_loss: 0.5254 - val_acc: 0.7856\n",
      "Epoch 82/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5248 - acc: 0.7928 - val_loss: 0.5305 - val_acc: 0.7866\n",
      "Epoch 83/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.5223 - acc: 0.7986 - val_loss: 0.5095 - val_acc: 0.7942\n",
      "Epoch 84/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.5165 - acc: 0.8034 - val_loss: 0.5366 - val_acc: 0.7859\n",
      "Epoch 85/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.5133 - acc: 0.8000 - val_loss: 0.5219 - val_acc: 0.7948\n",
      "Epoch 86/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.5091 - acc: 0.8027 - val_loss: 0.5393 - val_acc: 0.7835\n",
      "Epoch 87/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.5063 - acc: 0.8017 - val_loss: 0.4998 - val_acc: 0.8127\n",
      "Epoch 88/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.5012 - acc: 0.8055 - val_loss: 0.5092 - val_acc: 0.8024\n",
      "Epoch 89/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.4984 - acc: 0.8041 - val_loss: 0.5037 - val_acc: 0.8103\n",
      "Epoch 90/200\n",
      "2910/2910 [==============================] - 0s 55us/step - loss: 0.4935 - acc: 0.8082 - val_loss: 0.4856 - val_acc: 0.8148\n",
      "Epoch 91/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.4889 - acc: 0.8127 - val_loss: 0.4851 - val_acc: 0.8065\n",
      "Epoch 92/200\n",
      "2910/2910 [==============================] - 0s 80us/step - loss: 0.4849 - acc: 0.8103 - val_loss: 0.4891 - val_acc: 0.8072\n",
      "Epoch 93/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4834 - acc: 0.8120 - val_loss: 0.4719 - val_acc: 0.8175\n",
      "Epoch 94/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.4790 - acc: 0.8155 - val_loss: 0.4943 - val_acc: 0.7986\n",
      "Epoch 95/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4756 - acc: 0.8179 - val_loss: 0.4870 - val_acc: 0.8045\n",
      "Epoch 96/200\n",
      "2910/2910 [==============================] - 0s 54us/step - loss: 0.4721 - acc: 0.8141 - val_loss: 0.5074 - val_acc: 0.7921\n",
      "Epoch 97/200\n",
      "2910/2910 [==============================] - 0s 56us/step - loss: 0.4666 - acc: 0.8192 - val_loss: 0.5015 - val_acc: 0.7959\n",
      "Epoch 98/200\n",
      "2910/2910 [==============================] - 0s 53us/step - loss: 0.4609 - acc: 0.8203 - val_loss: 0.4546 - val_acc: 0.8227\n",
      "Epoch 99/200\n",
      "2910/2910 [==============================] - 0s 57us/step - loss: 0.4569 - acc: 0.8206 - val_loss: 0.4522 - val_acc: 0.8175\n",
      "Epoch 100/200\n",
      "2910/2910 [==============================] - 0s 59us/step - loss: 0.4539 - acc: 0.8189 - val_loss: 0.4879 - val_acc: 0.8076\n",
      "Epoch 101/200\n",
      "2910/2910 [==============================] - 0s 55us/step - loss: 0.4478 - acc: 0.8271 - val_loss: 0.4890 - val_acc: 0.7945\n",
      "Epoch 102/200\n",
      "2910/2910 [==============================] - 0s 60us/step - loss: 0.4472 - acc: 0.8247 - val_loss: 0.4363 - val_acc: 0.8330\n",
      "Epoch 103/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.4401 - acc: 0.8241 - val_loss: 0.4605 - val_acc: 0.8168\n",
      "Epoch 104/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.4402 - acc: 0.8296 - val_loss: 0.4385 - val_acc: 0.8299\n",
      "Epoch 105/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.4321 - acc: 0.8299 - val_loss: 0.4449 - val_acc: 0.8216\n",
      "Epoch 106/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.4317 - acc: 0.8278 - val_loss: 0.4468 - val_acc: 0.8210\n",
      "Epoch 107/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.4280 - acc: 0.8364 - val_loss: 0.4227 - val_acc: 0.8361\n",
      "Epoch 108/200\n",
      "2910/2910 [==============================] - 0s 84us/step - loss: 0.4204 - acc: 0.8385 - val_loss: 0.4452 - val_acc: 0.8265\n",
      "Epoch 109/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.4190 - acc: 0.8351 - val_loss: 0.4155 - val_acc: 0.8409\n",
      "Epoch 110/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.4150 - acc: 0.8368 - val_loss: 0.4263 - val_acc: 0.8450\n",
      "Epoch 111/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.4124 - acc: 0.8412 - val_loss: 0.4614 - val_acc: 0.8216\n",
      "Epoch 112/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.4073 - acc: 0.8395 - val_loss: 0.4068 - val_acc: 0.8354\n",
      "Epoch 113/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.4006 - acc: 0.8423 - val_loss: 0.4205 - val_acc: 0.8405\n",
      "Epoch 114/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.3967 - acc: 0.8474 - val_loss: 0.4693 - val_acc: 0.8100\n",
      "Epoch 115/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.3998 - acc: 0.8440 - val_loss: 0.4779 - val_acc: 0.8031\n",
      "Epoch 116/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.3939 - acc: 0.8488 - val_loss: 0.4021 - val_acc: 0.8471\n",
      "Epoch 117/200\n",
      "2910/2910 [==============================] - 0s 84us/step - loss: 0.3856 - acc: 0.8505 - val_loss: 0.4044 - val_acc: 0.8326\n",
      "Epoch 118/200\n",
      "2910/2910 [==============================] - 0s 74us/step - loss: 0.3843 - acc: 0.8529 - val_loss: 0.3844 - val_acc: 0.8443\n",
      "Epoch 119/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.3756 - acc: 0.8570 - val_loss: 0.4052 - val_acc: 0.8460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.3752 - acc: 0.8632 - val_loss: 0.4330 - val_acc: 0.8241\n",
      "Epoch 121/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.3700 - acc: 0.8680 - val_loss: 0.3712 - val_acc: 0.8708\n",
      "Epoch 122/200\n",
      "2910/2910 [==============================] - 0s 86us/step - loss: 0.3612 - acc: 0.8732 - val_loss: 0.3575 - val_acc: 0.8732\n",
      "Epoch 123/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.3586 - acc: 0.8732 - val_loss: 0.3621 - val_acc: 0.8773\n",
      "Epoch 124/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.3522 - acc: 0.8756 - val_loss: 0.3565 - val_acc: 0.8684\n",
      "Epoch 125/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.3494 - acc: 0.8729 - val_loss: 0.3611 - val_acc: 0.8674\n",
      "Epoch 126/200\n",
      "2910/2910 [==============================] - 0s 116us/step - loss: 0.3437 - acc: 0.8801 - val_loss: 0.3721 - val_acc: 0.8608\n",
      "Epoch 127/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.3393 - acc: 0.8780 - val_loss: 0.3413 - val_acc: 0.8698\n",
      "Epoch 128/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.3354 - acc: 0.8866 - val_loss: 0.3334 - val_acc: 0.8863\n",
      "Epoch 129/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.3277 - acc: 0.8863 - val_loss: 0.3269 - val_acc: 0.8845\n",
      "Epoch 130/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.3234 - acc: 0.8890 - val_loss: 0.3600 - val_acc: 0.8557\n",
      "Epoch 131/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.3189 - acc: 0.8849 - val_loss: 0.3295 - val_acc: 0.8773\n",
      "Epoch 132/200\n",
      "2910/2910 [==============================] - 0s 61us/step - loss: 0.3143 - acc: 0.8921 - val_loss: 0.3133 - val_acc: 0.8921\n",
      "Epoch 133/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.3063 - acc: 0.8952 - val_loss: 0.2967 - val_acc: 0.8997\n",
      "Epoch 134/200\n",
      "2910/2910 [==============================] - 0s 71us/step - loss: 0.3003 - acc: 0.9021 - val_loss: 0.2970 - val_acc: 0.8935\n",
      "Epoch 135/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.2967 - acc: 0.8990 - val_loss: 0.3050 - val_acc: 0.8948\n",
      "Epoch 136/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.2893 - acc: 0.9045 - val_loss: 0.3261 - val_acc: 0.8780\n",
      "Epoch 137/200\n",
      "2910/2910 [==============================] - 0s 80us/step - loss: 0.2905 - acc: 0.9014 - val_loss: 0.2837 - val_acc: 0.9186\n",
      "Epoch 138/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.2800 - acc: 0.9107 - val_loss: 0.2807 - val_acc: 0.9034\n",
      "Epoch 139/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.2773 - acc: 0.9110 - val_loss: 0.2781 - val_acc: 0.9103\n",
      "Epoch 140/200\n",
      "2910/2910 [==============================] - 0s 63us/step - loss: 0.2745 - acc: 0.9120 - val_loss: 0.2812 - val_acc: 0.9058\n",
      "Epoch 141/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.2686 - acc: 0.9162 - val_loss: 0.2711 - val_acc: 0.9017\n",
      "Epoch 142/200\n",
      "2910/2910 [==============================] - 0s 68us/step - loss: 0.2645 - acc: 0.9182 - val_loss: 0.2787 - val_acc: 0.9055\n",
      "Epoch 143/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.2570 - acc: 0.9210 - val_loss: 0.2687 - val_acc: 0.9137\n",
      "Epoch 144/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.2539 - acc: 0.9223 - val_loss: 0.2916 - val_acc: 0.8948\n",
      "Epoch 145/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.2504 - acc: 0.9237 - val_loss: 0.2416 - val_acc: 0.9289\n",
      "Epoch 146/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.2450 - acc: 0.9292 - val_loss: 0.2629 - val_acc: 0.9179\n",
      "Epoch 147/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.2424 - acc: 0.9247 - val_loss: 0.2354 - val_acc: 0.9316\n",
      "Epoch 148/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.2352 - acc: 0.9309 - val_loss: 0.2409 - val_acc: 0.9275\n",
      "Epoch 149/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.2340 - acc: 0.9333 - val_loss: 0.2405 - val_acc: 0.9227\n",
      "Epoch 150/200\n",
      "2910/2910 [==============================] - 0s 67us/step - loss: 0.2313 - acc: 0.9381 - val_loss: 0.2359 - val_acc: 0.9158\n",
      "Epoch 151/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.2263 - acc: 0.9351 - val_loss: 0.2270 - val_acc: 0.9347\n",
      "Epoch 152/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.2246 - acc: 0.9313 - val_loss: 0.2350 - val_acc: 0.9306\n",
      "Epoch 153/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.2261 - acc: 0.9326 - val_loss: 0.2251 - val_acc: 0.9292\n",
      "Epoch 154/200\n",
      "2910/2910 [==============================] - 0s 70us/step - loss: 0.2167 - acc: 0.9385 - val_loss: 0.2385 - val_acc: 0.9196\n",
      "Epoch 155/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.2135 - acc: 0.9381 - val_loss: 0.2252 - val_acc: 0.9247\n",
      "Epoch 156/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.2108 - acc: 0.9388 - val_loss: 0.2194 - val_acc: 0.9385\n",
      "Epoch 157/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.2058 - acc: 0.9399 - val_loss: 0.2007 - val_acc: 0.9436\n",
      "Epoch 158/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.2008 - acc: 0.9405 - val_loss: 0.2660 - val_acc: 0.9014\n",
      "Epoch 159/200\n",
      "2910/2910 [==============================] - 0s 76us/step - loss: 0.1978 - acc: 0.9450 - val_loss: 0.2362 - val_acc: 0.9134\n",
      "Epoch 160/200\n",
      "2910/2910 [==============================] - 0s 115us/step - loss: 0.1953 - acc: 0.9433 - val_loss: 0.2184 - val_acc: 0.9223\n",
      "Epoch 161/200\n",
      "2910/2910 [==============================] - 0s 94us/step - loss: 0.1916 - acc: 0.9447 - val_loss: 0.1908 - val_acc: 0.9450\n",
      "Epoch 162/200\n",
      "2910/2910 [==============================] - 0s 73us/step - loss: 0.1876 - acc: 0.9464 - val_loss: 0.1939 - val_acc: 0.9423\n",
      "Epoch 163/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.1853 - acc: 0.9495 - val_loss: 0.2134 - val_acc: 0.9296\n",
      "Epoch 164/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.1822 - acc: 0.9512 - val_loss: 0.1894 - val_acc: 0.9471\n",
      "Epoch 165/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.1789 - acc: 0.9509 - val_loss: 0.2392 - val_acc: 0.9137\n",
      "Epoch 166/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.1775 - acc: 0.9515 - val_loss: 0.1766 - val_acc: 0.9515\n",
      "Epoch 167/200\n",
      "2910/2910 [==============================] - 0s 77us/step - loss: 0.1731 - acc: 0.9529 - val_loss: 0.1770 - val_acc: 0.9519\n",
      "Epoch 168/200\n",
      "2910/2910 [==============================] - 0s 84us/step - loss: 0.1724 - acc: 0.9540 - val_loss: 0.1780 - val_acc: 0.9467\n",
      "Epoch 169/200\n",
      "2910/2910 [==============================] - 0s 110us/step - loss: 0.1672 - acc: 0.9540 - val_loss: 0.2232 - val_acc: 0.9100\n",
      "Epoch 170/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.1702 - acc: 0.9502 - val_loss: 0.1600 - val_acc: 0.9591\n",
      "Epoch 171/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.1647 - acc: 0.9533 - val_loss: 0.1653 - val_acc: 0.9509\n",
      "Epoch 172/200\n",
      "2910/2910 [==============================] - 0s 96us/step - loss: 0.1618 - acc: 0.9557 - val_loss: 0.1916 - val_acc: 0.9378\n",
      "Epoch 173/200\n",
      "2910/2910 [==============================] - 0s 78us/step - loss: 0.1630 - acc: 0.9536 - val_loss: 0.1577 - val_acc: 0.9588\n",
      "Epoch 174/200\n",
      "2910/2910 [==============================] - 0s 87us/step - loss: 0.1568 - acc: 0.9577 - val_loss: 0.1610 - val_acc: 0.9577\n",
      "Epoch 175/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.1555 - acc: 0.9591 - val_loss: 0.2033 - val_acc: 0.9282\n",
      "Epoch 176/200\n",
      "2910/2910 [==============================] - 0s 98us/step - loss: 0.1544 - acc: 0.9581 - val_loss: 0.1463 - val_acc: 0.9629\n",
      "Epoch 177/200\n",
      "2910/2910 [==============================] - 0s 120us/step - loss: 0.1486 - acc: 0.9598 - val_loss: 0.1599 - val_acc: 0.9557\n",
      "Epoch 178/200\n",
      "2910/2910 [==============================] - 0s 93us/step - loss: 0.1474 - acc: 0.9595 - val_loss: 0.1498 - val_acc: 0.9601\n",
      "Epoch 179/200\n",
      "2910/2910 [==============================] - 0s 114us/step - loss: 0.1476 - acc: 0.9581 - val_loss: 0.1489 - val_acc: 0.9574\n",
      "Epoch 180/200\n",
      "2910/2910 [==============================] - 0s 90us/step - loss: 0.1429 - acc: 0.9629 - val_loss: 0.1531 - val_acc: 0.9553\n",
      "Epoch 181/200\n",
      "2910/2910 [==============================] - 0s 81us/step - loss: 0.1430 - acc: 0.9605 - val_loss: 0.1462 - val_acc: 0.9605\n",
      "Epoch 182/200\n",
      "2910/2910 [==============================] - 0s 97us/step - loss: 0.1404 - acc: 0.9605 - val_loss: 0.1414 - val_acc: 0.9636\n",
      "Epoch 183/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.1385 - acc: 0.9605 - val_loss: 0.1390 - val_acc: 0.9619\n",
      "Epoch 184/200\n",
      "2910/2910 [==============================] - 0s 95us/step - loss: 0.1367 - acc: 0.9619 - val_loss: 0.1405 - val_acc: 0.9584\n",
      "Epoch 185/200\n",
      "2910/2910 [==============================] - 0s 83us/step - loss: 0.1383 - acc: 0.9595 - val_loss: 0.1459 - val_acc: 0.9529\n",
      "Epoch 186/200\n",
      "2910/2910 [==============================] - 0s 88us/step - loss: 0.1317 - acc: 0.9625 - val_loss: 0.1410 - val_acc: 0.9560\n",
      "Epoch 187/200\n",
      "2910/2910 [==============================] - 0s 85us/step - loss: 0.1307 - acc: 0.9625 - val_loss: 0.1417 - val_acc: 0.9564\n",
      "Epoch 188/200\n",
      "2910/2910 [==============================] - 0s 82us/step - loss: 0.1311 - acc: 0.9636 - val_loss: 0.1242 - val_acc: 0.9656\n",
      "Epoch 189/200\n",
      "2910/2910 [==============================] - 0s 72us/step - loss: 0.1261 - acc: 0.9660 - val_loss: 0.1220 - val_acc: 0.9708\n",
      "Epoch 190/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.1246 - acc: 0.9663 - val_loss: 0.1442 - val_acc: 0.9536\n",
      "Epoch 191/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.1247 - acc: 0.9674 - val_loss: 0.1362 - val_acc: 0.9570\n",
      "Epoch 192/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.1213 - acc: 0.9691 - val_loss: 0.1171 - val_acc: 0.9701\n",
      "Epoch 193/200\n",
      "2910/2910 [==============================] - 0s 92us/step - loss: 0.1197 - acc: 0.9670 - val_loss: 0.1109 - val_acc: 0.9725\n",
      "Epoch 194/200\n",
      "2910/2910 [==============================] - 0s 66us/step - loss: 0.1172 - acc: 0.9677 - val_loss: 0.1430 - val_acc: 0.9560\n",
      "Epoch 195/200\n",
      "2910/2910 [==============================] - 0s 65us/step - loss: 0.1192 - acc: 0.9674 - val_loss: 0.1546 - val_acc: 0.9481\n",
      "Epoch 196/200\n",
      "2910/2910 [==============================] - 0s 89us/step - loss: 0.1182 - acc: 0.9680 - val_loss: 0.1168 - val_acc: 0.9674\n",
      "Epoch 197/200\n",
      "2910/2910 [==============================] - 0s 79us/step - loss: 0.1117 - acc: 0.9698 - val_loss: 0.1105 - val_acc: 0.9722\n",
      "Epoch 198/200\n",
      "2910/2910 [==============================] - 0s 64us/step - loss: 0.1101 - acc: 0.9715 - val_loss: 0.1299 - val_acc: 0.9667\n",
      "Epoch 199/200\n",
      "2910/2910 [==============================] - 0s 62us/step - loss: 0.1125 - acc: 0.9708 - val_loss: 0.1212 - val_acc: 0.9643\n",
      "Epoch 200/200\n",
      "2910/2910 [==============================] - 0s 69us/step - loss: 0.1088 - acc: 0.9704 - val_loss: 0.1021 - val_acc: 0.9708\n",
      "2910/2910 [==============================] - 0s 133us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 1164 samples\n",
      "Epoch 1/200\n",
      "1164/1164 [==============================] - 3s 2ms/step - loss: 1.1266 - acc: 0.3230 - val_loss: 0.9976 - val_acc: 0.4897\n",
      "Epoch 2/200\n",
      "1164/1164 [==============================] - 0s 102us/step - loss: 0.9739 - acc: 0.5378 - val_loss: 0.9436 - val_acc: 0.5730\n",
      "Epoch 3/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.9354 - acc: 0.5859 - val_loss: 0.9170 - val_acc: 0.5954\n",
      "Epoch 4/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.9116 - acc: 0.5988 - val_loss: 0.8979 - val_acc: 0.6022\n",
      "Epoch 5/200\n",
      "1164/1164 [==============================] - 0s 74us/step - loss: 0.8950 - acc: 0.6057 - val_loss: 0.8874 - val_acc: 0.6117\n",
      "Epoch 6/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.8861 - acc: 0.6117 - val_loss: 0.8739 - val_acc: 0.6091\n",
      "Epoch 7/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.8755 - acc: 0.6125 - val_loss: 0.8680 - val_acc: 0.6091\n",
      "Epoch 8/200\n",
      "1164/1164 [==============================] - 0s 136us/step - loss: 0.8674 - acc: 0.6100 - val_loss: 0.8580 - val_acc: 0.6125\n",
      "Epoch 9/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.8633 - acc: 0.6151 - val_loss: 0.8522 - val_acc: 0.6134\n",
      "Epoch 10/200\n",
      "1164/1164 [==============================] - 0s 56us/step - loss: 0.8561 - acc: 0.6151 - val_loss: 0.8455 - val_acc: 0.6177\n",
      "Epoch 11/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.8503 - acc: 0.6168 - val_loss: 0.8400 - val_acc: 0.6203\n",
      "Epoch 12/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.8419 - acc: 0.6263 - val_loss: 0.8360 - val_acc: 0.6194\n",
      "Epoch 13/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.8392 - acc: 0.6246 - val_loss: 0.8286 - val_acc: 0.6237\n",
      "Epoch 14/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.8318 - acc: 0.6246 - val_loss: 0.8235 - val_acc: 0.6306\n",
      "Epoch 15/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.8281 - acc: 0.6237 - val_loss: 0.8183 - val_acc: 0.6271\n",
      "Epoch 16/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.8243 - acc: 0.6263 - val_loss: 0.8151 - val_acc: 0.6383\n",
      "Epoch 17/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.8172 - acc: 0.6297 - val_loss: 0.8108 - val_acc: 0.6383\n",
      "Epoch 18/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.8116 - acc: 0.6289 - val_loss: 0.8049 - val_acc: 0.6271\n",
      "Epoch 19/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.8092 - acc: 0.6289 - val_loss: 0.8007 - val_acc: 0.6203\n",
      "Epoch 20/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.8033 - acc: 0.6271 - val_loss: 0.7957 - val_acc: 0.6495\n",
      "Epoch 21/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.7984 - acc: 0.6426 - val_loss: 0.7874 - val_acc: 0.6383\n",
      "Epoch 22/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.7933 - acc: 0.6426 - val_loss: 0.7843 - val_acc: 0.6314\n",
      "Epoch 23/200\n",
      "1164/1164 [==============================] - 0s 108us/step - loss: 0.7900 - acc: 0.6435 - val_loss: 0.7808 - val_acc: 0.6289\n",
      "Epoch 24/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.7840 - acc: 0.6452 - val_loss: 0.7758 - val_acc: 0.6426\n",
      "Epoch 25/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.7826 - acc: 0.6460 - val_loss: 0.7678 - val_acc: 0.6512\n",
      "Epoch 26/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.7739 - acc: 0.6521 - val_loss: 0.7627 - val_acc: 0.6572\n",
      "Epoch 27/200\n",
      "1164/1164 [==============================] - 0s 55us/step - loss: 0.7682 - acc: 0.6503 - val_loss: 0.7594 - val_acc: 0.6495\n",
      "Epoch 28/200\n",
      "1164/1164 [==============================] - 0s 119us/step - loss: 0.7651 - acc: 0.6503 - val_loss: 0.7546 - val_acc: 0.6615\n",
      "Epoch 29/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.7581 - acc: 0.6564 - val_loss: 0.7485 - val_acc: 0.6615\n",
      "Epoch 30/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.7561 - acc: 0.6529 - val_loss: 0.7508 - val_acc: 0.6607\n",
      "Epoch 31/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.7523 - acc: 0.6572 - val_loss: 0.7409 - val_acc: 0.6675\n",
      "Epoch 32/200\n",
      "1164/1164 [==============================] - 0s 57us/step - loss: 0.7467 - acc: 0.6598 - val_loss: 0.7408 - val_acc: 0.6701\n",
      "Epoch 33/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.7418 - acc: 0.6649 - val_loss: 0.7304 - val_acc: 0.6753\n",
      "Epoch 34/200\n",
      "1164/1164 [==============================] - 0s 122us/step - loss: 0.7374 - acc: 0.6684 - val_loss: 0.7320 - val_acc: 0.6615\n",
      "Epoch 35/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.7346 - acc: 0.6598 - val_loss: 0.7237 - val_acc: 0.6641\n",
      "Epoch 36/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.7274 - acc: 0.6684 - val_loss: 0.7189 - val_acc: 0.6684\n",
      "Epoch 37/200\n",
      "1164/1164 [==============================] - 0s 98us/step - loss: 0.7222 - acc: 0.6727 - val_loss: 0.7137 - val_acc: 0.6916\n",
      "Epoch 38/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.7206 - acc: 0.6684 - val_loss: 0.7104 - val_acc: 0.6744\n",
      "Epoch 39/200\n",
      "1164/1164 [==============================] - 0s 113us/step - loss: 0.7143 - acc: 0.6838 - val_loss: 0.7044 - val_acc: 0.6856\n",
      "Epoch 40/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.7086 - acc: 0.6830 - val_loss: 0.7009 - val_acc: 0.6942\n",
      "Epoch 41/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.7023 - acc: 0.6873 - val_loss: 0.6978 - val_acc: 0.6993\n",
      "Epoch 42/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.6999 - acc: 0.6899 - val_loss: 0.6878 - val_acc: 0.6933\n",
      "Epoch 43/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.6935 - acc: 0.6985 - val_loss: 0.6874 - val_acc: 0.6796\n",
      "Epoch 44/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.6916 - acc: 0.6916 - val_loss: 0.6810 - val_acc: 0.7079\n",
      "Epoch 45/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.6865 - acc: 0.6985 - val_loss: 0.6756 - val_acc: 0.6985\n",
      "Epoch 46/200\n",
      "1164/1164 [==============================] - 0s 79us/step - loss: 0.6814 - acc: 0.7002 - val_loss: 0.6733 - val_acc: 0.6916\n",
      "Epoch 47/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.6825 - acc: 0.6942 - val_loss: 0.6716 - val_acc: 0.7251\n",
      "Epoch 48/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6726 - acc: 0.7165 - val_loss: 0.6616 - val_acc: 0.7251\n",
      "Epoch 49/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.6731 - acc: 0.7122 - val_loss: 0.6601 - val_acc: 0.7380\n",
      "Epoch 50/200\n",
      "1164/1164 [==============================] - 0s 103us/step - loss: 0.6663 - acc: 0.7208 - val_loss: 0.6607 - val_acc: 0.7174\n",
      "Epoch 51/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.6604 - acc: 0.7285 - val_loss: 0.6521 - val_acc: 0.7036\n",
      "Epoch 52/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.6587 - acc: 0.7165 - val_loss: 0.6505 - val_acc: 0.7182\n",
      "Epoch 53/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.6551 - acc: 0.7259 - val_loss: 0.6472 - val_acc: 0.7414\n",
      "Epoch 54/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.6479 - acc: 0.7277 - val_loss: 0.6545 - val_acc: 0.7148\n",
      "Epoch 55/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.6505 - acc: 0.7199 - val_loss: 0.6445 - val_acc: 0.7328\n",
      "Epoch 56/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.6476 - acc: 0.7216 - val_loss: 0.6372 - val_acc: 0.7569\n",
      "Epoch 57/200\n",
      "1164/1164 [==============================] - 0s 201us/step - loss: 0.6422 - acc: 0.7363 - val_loss: 0.6368 - val_acc: 0.7354\n",
      "Epoch 58/200\n",
      "1164/1164 [==============================] - 0s 117us/step - loss: 0.6366 - acc: 0.7397 - val_loss: 0.6299 - val_acc: 0.7277\n",
      "Epoch 59/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6388 - acc: 0.7311 - val_loss: 0.6297 - val_acc: 0.7595\n",
      "Epoch 60/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.6300 - acc: 0.7414 - val_loss: 0.6244 - val_acc: 0.7431\n",
      "Epoch 61/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.6265 - acc: 0.7534 - val_loss: 0.6134 - val_acc: 0.7483\n",
      "Epoch 62/200\n",
      "1164/1164 [==============================] - 0s 127us/step - loss: 0.6244 - acc: 0.7405 - val_loss: 0.6154 - val_acc: 0.7766\n",
      "Epoch 63/200\n",
      "1164/1164 [==============================] - 0s 129us/step - loss: 0.6201 - acc: 0.7466 - val_loss: 0.6261 - val_acc: 0.7259\n",
      "Epoch 64/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.6197 - acc: 0.7500 - val_loss: 0.6173 - val_acc: 0.7474\n",
      "Epoch 65/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.6122 - acc: 0.7577 - val_loss: 0.6105 - val_acc: 0.7663\n",
      "Epoch 66/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.6111 - acc: 0.7509 - val_loss: 0.6053 - val_acc: 0.7534\n",
      "Epoch 67/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.6095 - acc: 0.7603 - val_loss: 0.5921 - val_acc: 0.7732\n",
      "Epoch 68/200\n",
      "1164/1164 [==============================] - 0s 98us/step - loss: 0.6056 - acc: 0.7500 - val_loss: 0.5948 - val_acc: 0.7612\n",
      "Epoch 69/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.6052 - acc: 0.7543 - val_loss: 0.5961 - val_acc: 0.7646\n",
      "Epoch 70/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.5993 - acc: 0.7655 - val_loss: 0.6038 - val_acc: 0.7388\n",
      "Epoch 71/200\n",
      "1164/1164 [==============================] - 0s 71us/step - loss: 0.5921 - acc: 0.7560 - val_loss: 0.5916 - val_acc: 0.7663\n",
      "Epoch 72/200\n",
      "1164/1164 [==============================] - 0s 83us/step - loss: 0.5943 - acc: 0.7586 - val_loss: 0.5885 - val_acc: 0.7620\n",
      "Epoch 73/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.5960 - acc: 0.7612 - val_loss: 0.5899 - val_acc: 0.7637\n",
      "Epoch 74/200\n",
      "1164/1164 [==============================] - 0s 71us/step - loss: 0.5845 - acc: 0.7741 - val_loss: 0.5881 - val_acc: 0.7646\n",
      "Epoch 75/200\n",
      "1164/1164 [==============================] - 0s 126us/step - loss: 0.5858 - acc: 0.7603 - val_loss: 0.5726 - val_acc: 0.7818\n",
      "Epoch 76/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.5772 - acc: 0.7706 - val_loss: 0.5809 - val_acc: 0.7637\n",
      "Epoch 77/200\n",
      "1164/1164 [==============================] - 0s 131us/step - loss: 0.5779 - acc: 0.7655 - val_loss: 0.5673 - val_acc: 0.7835\n",
      "Epoch 78/200\n",
      "1164/1164 [==============================] - 0s 102us/step - loss: 0.5716 - acc: 0.7801 - val_loss: 0.5639 - val_acc: 0.7809\n",
      "Epoch 79/200\n",
      "1164/1164 [==============================] - 0s 120us/step - loss: 0.5722 - acc: 0.7723 - val_loss: 0.5705 - val_acc: 0.7706\n",
      "Epoch 80/200\n",
      "1164/1164 [==============================] - 0s 148us/step - loss: 0.5716 - acc: 0.7784 - val_loss: 0.5766 - val_acc: 0.7844\n",
      "Epoch 81/200\n",
      "1164/1164 [==============================] - 0s 205us/step - loss: 0.5687 - acc: 0.7749 - val_loss: 0.5688 - val_acc: 0.7500\n",
      "Epoch 82/200\n",
      "1164/1164 [==============================] - 0s 113us/step - loss: 0.5656 - acc: 0.7698 - val_loss: 0.5582 - val_acc: 0.7818\n",
      "Epoch 83/200\n",
      "1164/1164 [==============================] - 0s 78us/step - loss: 0.5614 - acc: 0.7852 - val_loss: 0.5546 - val_acc: 0.7878\n",
      "Epoch 84/200\n",
      "1164/1164 [==============================] - 0s 126us/step - loss: 0.5571 - acc: 0.7801 - val_loss: 0.5695 - val_acc: 0.7663\n",
      "Epoch 85/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.5618 - acc: 0.7723 - val_loss: 0.5414 - val_acc: 0.7938\n",
      "Epoch 86/200\n",
      "1164/1164 [==============================] - 0s 122us/step - loss: 0.5509 - acc: 0.7921 - val_loss: 0.5493 - val_acc: 0.7809\n",
      "Epoch 87/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.5448 - acc: 0.7964 - val_loss: 0.5423 - val_acc: 0.7990\n",
      "Epoch 88/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.5520 - acc: 0.7895 - val_loss: 0.5341 - val_acc: 0.7973\n",
      "Epoch 89/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.5457 - acc: 0.7878 - val_loss: 0.5476 - val_acc: 0.7844\n",
      "Epoch 90/200\n",
      "1164/1164 [==============================] - 0s 53us/step - loss: 0.5437 - acc: 0.7895 - val_loss: 0.5367 - val_acc: 0.8007\n",
      "Epoch 91/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.5415 - acc: 0.8015 - val_loss: 0.5293 - val_acc: 0.7835\n",
      "Epoch 92/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.5441 - acc: 0.7869 - val_loss: 0.5265 - val_acc: 0.8041\n",
      "Epoch 93/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.5362 - acc: 0.7981 - val_loss: 0.5239 - val_acc: 0.7955\n",
      "Epoch 94/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.5315 - acc: 0.7990 - val_loss: 0.5272 - val_acc: 0.7998\n",
      "Epoch 95/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.5308 - acc: 0.8041 - val_loss: 0.5187 - val_acc: 0.8076\n",
      "Epoch 96/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.5274 - acc: 0.8024 - val_loss: 0.5394 - val_acc: 0.7895\n",
      "Epoch 97/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.5266 - acc: 0.8050 - val_loss: 0.5193 - val_acc: 0.7844\n",
      "Epoch 98/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.5277 - acc: 0.7998 - val_loss: 0.5146 - val_acc: 0.8239\n",
      "Epoch 99/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.5213 - acc: 0.8058 - val_loss: 0.5241 - val_acc: 0.8127\n",
      "Epoch 100/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.5202 - acc: 0.8033 - val_loss: 0.5165 - val_acc: 0.8153\n",
      "Epoch 101/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.5194 - acc: 0.7973 - val_loss: 0.5269 - val_acc: 0.7930\n",
      "Epoch 102/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.5167 - acc: 0.7998 - val_loss: 0.5015 - val_acc: 0.8127\n",
      "Epoch 103/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.5099 - acc: 0.8101 - val_loss: 0.4989 - val_acc: 0.8162\n",
      "Epoch 104/200\n",
      "1164/1164 [==============================] - 0s 105us/step - loss: 0.5091 - acc: 0.8119 - val_loss: 0.5042 - val_acc: 0.7981\n",
      "Epoch 105/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.5056 - acc: 0.8136 - val_loss: 0.4945 - val_acc: 0.8162\n",
      "Epoch 106/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.5027 - acc: 0.8119 - val_loss: 0.5093 - val_acc: 0.7998\n",
      "Epoch 107/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.5030 - acc: 0.8076 - val_loss: 0.4927 - val_acc: 0.8119\n",
      "Epoch 108/200\n",
      "1164/1164 [==============================] - 0s 81us/step - loss: 0.4932 - acc: 0.8196 - val_loss: 0.4844 - val_acc: 0.8162\n",
      "Epoch 109/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.4948 - acc: 0.8153 - val_loss: 0.4883 - val_acc: 0.8067\n",
      "Epoch 110/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4937 - acc: 0.8153 - val_loss: 0.4936 - val_acc: 0.8024\n",
      "Epoch 111/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4899 - acc: 0.8050 - val_loss: 0.4888 - val_acc: 0.8170\n",
      "Epoch 112/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.4952 - acc: 0.8101 - val_loss: 0.4897 - val_acc: 0.8247\n",
      "Epoch 113/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4875 - acc: 0.8136 - val_loss: 0.4784 - val_acc: 0.8351\n",
      "Epoch 114/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.4964 - acc: 0.8179 - val_loss: 0.4803 - val_acc: 0.8119\n",
      "Epoch 115/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.4803 - acc: 0.8179 - val_loss: 0.4772 - val_acc: 0.8333\n",
      "Epoch 116/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4800 - acc: 0.8273 - val_loss: 0.4693 - val_acc: 0.8299\n",
      "Epoch 117/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4783 - acc: 0.8247 - val_loss: 0.4723 - val_acc: 0.8110\n",
      "Epoch 118/200\n",
      "1164/1164 [==============================] - 0s 77us/step - loss: 0.4759 - acc: 0.8256 - val_loss: 0.4648 - val_acc: 0.8351\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4815 - acc: 0.8162 - val_loss: 0.4721 - val_acc: 0.8247\n",
      "Epoch 120/200\n",
      "1164/1164 [==============================] - 0s 106us/step - loss: 0.4775 - acc: 0.8273 - val_loss: 0.4737 - val_acc: 0.8119\n",
      "Epoch 121/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.4711 - acc: 0.8290 - val_loss: 0.4757 - val_acc: 0.8170\n",
      "Epoch 122/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.4802 - acc: 0.8153 - val_loss: 0.4577 - val_acc: 0.8454\n",
      "Epoch 123/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.4649 - acc: 0.8273 - val_loss: 0.5036 - val_acc: 0.8033\n",
      "Epoch 124/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4722 - acc: 0.8256 - val_loss: 0.4734 - val_acc: 0.8196\n",
      "Epoch 125/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4639 - acc: 0.8316 - val_loss: 0.4531 - val_acc: 0.8411\n",
      "Epoch 126/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4643 - acc: 0.8239 - val_loss: 0.4602 - val_acc: 0.8127\n",
      "Epoch 127/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.4623 - acc: 0.8222 - val_loss: 0.4517 - val_acc: 0.8402\n",
      "Epoch 128/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.4604 - acc: 0.8299 - val_loss: 0.4551 - val_acc: 0.8419\n",
      "Epoch 129/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4560 - acc: 0.8393 - val_loss: 0.4606 - val_acc: 0.8119\n",
      "Epoch 130/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4588 - acc: 0.8342 - val_loss: 0.4468 - val_acc: 0.8385\n",
      "Epoch 131/200\n",
      "1164/1164 [==============================] - 0s 57us/step - loss: 0.4596 - acc: 0.8308 - val_loss: 0.4621 - val_acc: 0.8196\n",
      "Epoch 132/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.4591 - acc: 0.8273 - val_loss: 0.4524 - val_acc: 0.8282\n",
      "Epoch 133/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.4550 - acc: 0.8316 - val_loss: 0.4538 - val_acc: 0.8316\n",
      "Epoch 134/200\n",
      "1164/1164 [==============================] - 0s 56us/step - loss: 0.4511 - acc: 0.8393 - val_loss: 0.4423 - val_acc: 0.8282\n",
      "Epoch 135/200\n",
      "1164/1164 [==============================] - 0s 55us/step - loss: 0.4477 - acc: 0.8359 - val_loss: 0.4422 - val_acc: 0.8402\n",
      "Epoch 136/200\n",
      "1164/1164 [==============================] - 0s 54us/step - loss: 0.4464 - acc: 0.8351 - val_loss: 0.4359 - val_acc: 0.8376\n",
      "Epoch 137/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4423 - acc: 0.8393 - val_loss: 0.4378 - val_acc: 0.8428\n",
      "Epoch 138/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.4451 - acc: 0.8402 - val_loss: 0.4354 - val_acc: 0.8402\n",
      "Epoch 139/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.4466 - acc: 0.8385 - val_loss: 0.4291 - val_acc: 0.8445\n",
      "Epoch 140/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4392 - acc: 0.8376 - val_loss: 0.4294 - val_acc: 0.8419\n",
      "Epoch 141/200\n",
      "1164/1164 [==============================] - 0s 57us/step - loss: 0.4371 - acc: 0.8393 - val_loss: 0.4349 - val_acc: 0.8505\n",
      "Epoch 142/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4386 - acc: 0.8419 - val_loss: 0.4278 - val_acc: 0.8540\n",
      "Epoch 143/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4350 - acc: 0.8454 - val_loss: 0.4583 - val_acc: 0.8316\n",
      "Epoch 144/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4413 - acc: 0.8419 - val_loss: 0.4236 - val_acc: 0.8557\n",
      "Epoch 145/200\n",
      "1164/1164 [==============================] - 0s 83us/step - loss: 0.4300 - acc: 0.8514 - val_loss: 0.4278 - val_acc: 0.8445\n",
      "Epoch 146/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4293 - acc: 0.8454 - val_loss: 0.4271 - val_acc: 0.8471\n",
      "Epoch 147/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.4333 - acc: 0.8342 - val_loss: 0.4214 - val_acc: 0.8436\n",
      "Epoch 148/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.4317 - acc: 0.8419 - val_loss: 0.4158 - val_acc: 0.8505\n",
      "Epoch 149/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.4271 - acc: 0.8445 - val_loss: 0.4256 - val_acc: 0.8428\n",
      "Epoch 150/200\n",
      "1164/1164 [==============================] - 0s 123us/step - loss: 0.4281 - acc: 0.8445 - val_loss: 0.4136 - val_acc: 0.8591\n",
      "Epoch 151/200\n",
      "1164/1164 [==============================] - 0s 82us/step - loss: 0.4282 - acc: 0.8522 - val_loss: 0.4187 - val_acc: 0.8462\n",
      "Epoch 152/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.4221 - acc: 0.8565 - val_loss: 0.4129 - val_acc: 0.8540\n",
      "Epoch 153/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.4279 - acc: 0.8411 - val_loss: 0.4125 - val_acc: 0.8505\n",
      "Epoch 154/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.4247 - acc: 0.8488 - val_loss: 0.4175 - val_acc: 0.8497\n",
      "Epoch 155/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.4269 - acc: 0.8454 - val_loss: 0.4147 - val_acc: 0.8660\n",
      "Epoch 156/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.4202 - acc: 0.8462 - val_loss: 0.4211 - val_acc: 0.8591\n",
      "Epoch 157/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4202 - acc: 0.8514 - val_loss: 0.4064 - val_acc: 0.8591\n",
      "Epoch 158/200\n",
      "1164/1164 [==============================] - 0s 81us/step - loss: 0.4214 - acc: 0.8436 - val_loss: 0.4064 - val_acc: 0.8643\n",
      "Epoch 159/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4142 - acc: 0.8540 - val_loss: 0.4068 - val_acc: 0.8608\n",
      "Epoch 160/200\n",
      "1164/1164 [==============================] - 0s 55us/step - loss: 0.4119 - acc: 0.8565 - val_loss: 0.4120 - val_acc: 0.8540\n",
      "Epoch 161/200\n",
      "1164/1164 [==============================] - 0s 57us/step - loss: 0.4151 - acc: 0.8540 - val_loss: 0.4038 - val_acc: 0.8600\n",
      "Epoch 162/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.4081 - acc: 0.8625 - val_loss: 0.4098 - val_acc: 0.8497\n",
      "Epoch 163/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4107 - acc: 0.8531 - val_loss: 0.4095 - val_acc: 0.8471\n",
      "Epoch 164/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4126 - acc: 0.8488 - val_loss: 0.3988 - val_acc: 0.8488\n",
      "Epoch 165/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.4054 - acc: 0.8540 - val_loss: 0.4004 - val_acc: 0.8634\n",
      "Epoch 166/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4017 - acc: 0.8582 - val_loss: 0.4204 - val_acc: 0.8359\n",
      "Epoch 167/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4066 - acc: 0.8522 - val_loss: 0.3936 - val_acc: 0.8565\n",
      "Epoch 168/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.4008 - acc: 0.8548 - val_loss: 0.4078 - val_acc: 0.8505\n",
      "Epoch 169/200\n",
      "1164/1164 [==============================] - 0s 56us/step - loss: 0.4059 - acc: 0.8574 - val_loss: 0.3942 - val_acc: 0.8634\n",
      "Epoch 170/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4028 - acc: 0.8617 - val_loss: 0.3904 - val_acc: 0.8729\n",
      "Epoch 171/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4053 - acc: 0.8557 - val_loss: 0.3951 - val_acc: 0.8574\n",
      "Epoch 172/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4013 - acc: 0.8531 - val_loss: 0.3873 - val_acc: 0.8634\n",
      "Epoch 173/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.3973 - acc: 0.8540 - val_loss: 0.3941 - val_acc: 0.8574\n",
      "Epoch 174/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.4005 - acc: 0.8600 - val_loss: 0.3891 - val_acc: 0.8591\n",
      "Epoch 175/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.3970 - acc: 0.8694 - val_loss: 0.3804 - val_acc: 0.8703\n",
      "Epoch 176/200\n",
      "1164/1164 [==============================] - 0s 104us/step - loss: 0.3970 - acc: 0.8565 - val_loss: 0.3852 - val_acc: 0.8531\n",
      "Epoch 177/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.3990 - acc: 0.8591 - val_loss: 0.3824 - val_acc: 0.8660\n",
      "Epoch 178/200\n",
      "1164/1164 [==============================] - 0s 98us/step - loss: 0.3914 - acc: 0.8651 - val_loss: 0.3795 - val_acc: 0.8677\n",
      "Epoch 179/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.3893 - acc: 0.8617 - val_loss: 0.3860 - val_acc: 0.8651\n",
      "Epoch 180/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.3937 - acc: 0.8591 - val_loss: 0.3803 - val_acc: 0.8694\n",
      "Epoch 181/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.3882 - acc: 0.8617 - val_loss: 0.4133 - val_acc: 0.8368\n",
      "Epoch 182/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.3896 - acc: 0.8600 - val_loss: 0.3771 - val_acc: 0.8694\n",
      "Epoch 183/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.3830 - acc: 0.8643 - val_loss: 0.3796 - val_acc: 0.8660\n",
      "Epoch 184/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.3838 - acc: 0.8720 - val_loss: 0.3787 - val_acc: 0.8634\n",
      "Epoch 185/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.3849 - acc: 0.8660 - val_loss: 0.3717 - val_acc: 0.8780\n",
      "Epoch 186/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.3862 - acc: 0.8634 - val_loss: 0.3741 - val_acc: 0.8737\n",
      "Epoch 187/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.3873 - acc: 0.8591 - val_loss: 0.3762 - val_acc: 0.8754\n",
      "Epoch 188/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.3915 - acc: 0.8643 - val_loss: 0.3756 - val_acc: 0.8625\n",
      "Epoch 189/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.3829 - acc: 0.8617 - val_loss: 0.3773 - val_acc: 0.8720\n",
      "Epoch 190/200\n",
      "1164/1164 [==============================] - 0s 57us/step - loss: 0.3757 - acc: 0.8686 - val_loss: 0.3651 - val_acc: 0.8823\n",
      "Epoch 191/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.3807 - acc: 0.8668 - val_loss: 0.3643 - val_acc: 0.8806\n",
      "Epoch 192/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.3781 - acc: 0.8686 - val_loss: 0.3724 - val_acc: 0.8651\n",
      "Epoch 193/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.3786 - acc: 0.8677 - val_loss: 0.3735 - val_acc: 0.8660\n",
      "Epoch 194/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.3753 - acc: 0.8617 - val_loss: 0.3686 - val_acc: 0.8806\n",
      "Epoch 195/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.3714 - acc: 0.8729 - val_loss: 0.3673 - val_acc: 0.8771\n",
      "Epoch 196/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.3712 - acc: 0.8686 - val_loss: 0.3904 - val_acc: 0.8565\n",
      "Epoch 197/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.3760 - acc: 0.8703 - val_loss: 0.3701 - val_acc: 0.8643\n",
      "Epoch 198/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.3673 - acc: 0.8754 - val_loss: 0.3730 - val_acc: 0.8720\n",
      "Epoch 199/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.3751 - acc: 0.8643 - val_loss: 0.3745 - val_acc: 0.8703\n",
      "Epoch 200/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.3677 - acc: 0.8737 - val_loss: 0.3586 - val_acc: 0.8746\n",
      "4656/4656 [==============================] - 0s 69us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 1164 samples\n",
      "Epoch 1/200\n",
      "1164/1164 [==============================] - 3s 2ms/step - loss: 1.3647 - acc: 0.2448 - val_loss: 1.0480 - val_acc: 0.4450\n",
      "Epoch 2/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.9958 - acc: 0.5284 - val_loss: 0.9545 - val_acc: 0.5662\n",
      "Epoch 3/200\n",
      "1164/1164 [==============================] - 0s 242us/step - loss: 0.9417 - acc: 0.5790 - val_loss: 0.9195 - val_acc: 0.5997\n",
      "Epoch 4/200\n",
      "1164/1164 [==============================] - 0s 133us/step - loss: 0.9141 - acc: 0.6040 - val_loss: 0.8973 - val_acc: 0.6143\n",
      "Epoch 5/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.8944 - acc: 0.6168 - val_loss: 0.8807 - val_acc: 0.6186\n",
      "Epoch 6/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.8815 - acc: 0.6194 - val_loss: 0.8688 - val_acc: 0.6186\n",
      "Epoch 7/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.8679 - acc: 0.6203 - val_loss: 0.8600 - val_acc: 0.6211\n",
      "Epoch 8/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.8616 - acc: 0.6220 - val_loss: 0.8528 - val_acc: 0.6229\n",
      "Epoch 9/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.8543 - acc: 0.6220 - val_loss: 0.8474 - val_acc: 0.6220\n",
      "Epoch 10/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.8516 - acc: 0.6280 - val_loss: 0.8419 - val_acc: 0.6271\n",
      "Epoch 11/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.8433 - acc: 0.6263 - val_loss: 0.8382 - val_acc: 0.6246\n",
      "Epoch 12/200\n",
      "1164/1164 [==============================] - 0s 148us/step - loss: 0.8420 - acc: 0.6246 - val_loss: 0.8321 - val_acc: 0.6237\n",
      "Epoch 13/200\n",
      "1164/1164 [==============================] - 0s 94us/step - loss: 0.8365 - acc: 0.6254 - val_loss: 0.8385 - val_acc: 0.6306\n",
      "Epoch 14/200\n",
      "1164/1164 [==============================] - 0s 142us/step - loss: 0.8354 - acc: 0.6246 - val_loss: 0.8254 - val_acc: 0.6229\n",
      "Epoch 15/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.8280 - acc: 0.6220 - val_loss: 0.8242 - val_acc: 0.6220\n",
      "Epoch 16/200\n",
      "1164/1164 [==============================] - 0s 77us/step - loss: 0.8246 - acc: 0.6237 - val_loss: 0.8156 - val_acc: 0.6323\n",
      "Epoch 17/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.8194 - acc: 0.6254 - val_loss: 0.8122 - val_acc: 0.6263\n",
      "Epoch 18/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.8188 - acc: 0.6280 - val_loss: 0.8110 - val_acc: 0.6263\n",
      "Epoch 19/200\n",
      "1164/1164 [==============================] - 0s 122us/step - loss: 0.8130 - acc: 0.6263 - val_loss: 0.8055 - val_acc: 0.6357\n",
      "Epoch 20/200\n",
      "1164/1164 [==============================] - 0s 151us/step - loss: 0.8072 - acc: 0.6314 - val_loss: 0.8080 - val_acc: 0.6332\n",
      "Epoch 21/200\n",
      "1164/1164 [==============================] - 0s 120us/step - loss: 0.8070 - acc: 0.6323 - val_loss: 0.8010 - val_acc: 0.6400\n",
      "Epoch 22/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.8039 - acc: 0.6332 - val_loss: 0.7947 - val_acc: 0.6306\n",
      "Epoch 23/200\n",
      "1164/1164 [==============================] - 0s 135us/step - loss: 0.7985 - acc: 0.6332 - val_loss: 0.7903 - val_acc: 0.6340\n",
      "Epoch 24/200\n",
      "1164/1164 [==============================] - 0s 150us/step - loss: 0.7937 - acc: 0.6314 - val_loss: 0.7878 - val_acc: 0.6443\n",
      "Epoch 25/200\n",
      "1164/1164 [==============================] - ETA: 0s - loss: 0.7819 - acc: 0.652 - 0s 169us/step - loss: 0.7951 - acc: 0.6426 - val_loss: 0.7895 - val_acc: 0.6349\n",
      "Epoch 26/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.7880 - acc: 0.6426 - val_loss: 0.7810 - val_acc: 0.6314\n",
      "Epoch 27/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.7834 - acc: 0.6435 - val_loss: 0.7724 - val_acc: 0.6538\n",
      "Epoch 28/200\n",
      "1164/1164 [==============================] - 0s 80us/step - loss: 0.7794 - acc: 0.6486 - val_loss: 0.7767 - val_acc: 0.6607\n",
      "Epoch 29/200\n",
      "1164/1164 [==============================] - 0s 122us/step - loss: 0.7768 - acc: 0.6529 - val_loss: 0.7719 - val_acc: 0.6589\n",
      "Epoch 30/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.7734 - acc: 0.6564 - val_loss: 0.7649 - val_acc: 0.6624\n",
      "Epoch 31/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.7720 - acc: 0.6546 - val_loss: 0.7601 - val_acc: 0.6624\n",
      "Epoch 32/200\n",
      "1164/1164 [==============================] - 0s 103us/step - loss: 0.7657 - acc: 0.6581 - val_loss: 0.7545 - val_acc: 0.6598\n",
      "Epoch 33/200\n",
      "1164/1164 [==============================] - 0s 121us/step - loss: 0.7604 - acc: 0.6624 - val_loss: 0.7586 - val_acc: 0.6546\n",
      "Epoch 34/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.7619 - acc: 0.6632 - val_loss: 0.7480 - val_acc: 0.6649\n",
      "Epoch 35/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.7601 - acc: 0.6649 - val_loss: 0.7437 - val_acc: 0.6761\n",
      "Epoch 36/200\n",
      "1164/1164 [==============================] - 0s 102us/step - loss: 0.7529 - acc: 0.6804 - val_loss: 0.7442 - val_acc: 0.6667\n",
      "Epoch 37/200\n",
      "1164/1164 [==============================] - 0s 115us/step - loss: 0.7473 - acc: 0.6761 - val_loss: 0.7381 - val_acc: 0.6830\n",
      "Epoch 38/200\n",
      "1164/1164 [==============================] - 0s 132us/step - loss: 0.7444 - acc: 0.6667 - val_loss: 0.7454 - val_acc: 0.6804\n",
      "Epoch 39/200\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 0.7444 - acc: 0.6838 - val_loss: 0.7295 - val_acc: 0.6838\n",
      "Epoch 40/200\n",
      "1164/1164 [==============================] - 0s 135us/step - loss: 0.7405 - acc: 0.6856 - val_loss: 0.7292 - val_acc: 0.6907\n",
      "Epoch 41/200\n",
      "1164/1164 [==============================] - 0s 108us/step - loss: 0.7351 - acc: 0.6967 - val_loss: 0.7327 - val_acc: 0.6838\n",
      "Epoch 42/200\n",
      "1164/1164 [==============================] - 0s 149us/step - loss: 0.7310 - acc: 0.6864 - val_loss: 0.7202 - val_acc: 0.7027\n",
      "Epoch 43/200\n",
      "1164/1164 [==============================] - 0s 83us/step - loss: 0.7314 - acc: 0.6933 - val_loss: 0.7226 - val_acc: 0.7165\n",
      "Epoch 44/200\n",
      "1164/1164 [==============================] - 0s 119us/step - loss: 0.7258 - acc: 0.7045 - val_loss: 0.7181 - val_acc: 0.7113\n",
      "Epoch 45/200\n",
      "1164/1164 [==============================] - 0s 94us/step - loss: 0.7211 - acc: 0.7096 - val_loss: 0.7104 - val_acc: 0.7122\n",
      "Epoch 46/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.7192 - acc: 0.7105 - val_loss: 0.7145 - val_acc: 0.7002\n",
      "Epoch 47/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.7180 - acc: 0.7045 - val_loss: 0.7093 - val_acc: 0.7199\n",
      "Epoch 48/200\n",
      "1164/1164 [==============================] - 0s 133us/step - loss: 0.7128 - acc: 0.7096 - val_loss: 0.7021 - val_acc: 0.7174\n",
      "Epoch 49/200\n",
      "1164/1164 [==============================] - 0s 117us/step - loss: 0.7095 - acc: 0.7105 - val_loss: 0.7046 - val_acc: 0.7174\n",
      "Epoch 50/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.7057 - acc: 0.7199 - val_loss: 0.6948 - val_acc: 0.7208\n",
      "Epoch 51/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.7049 - acc: 0.7182 - val_loss: 0.6917 - val_acc: 0.7208\n",
      "Epoch 52/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.6967 - acc: 0.7199 - val_loss: 0.6874 - val_acc: 0.7345\n",
      "Epoch 53/200\n",
      "1164/1164 [==============================] - 0s 97us/step - loss: 0.6950 - acc: 0.7277 - val_loss: 0.6861 - val_acc: 0.7182\n",
      "Epoch 54/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.6908 - acc: 0.7311 - val_loss: 0.6815 - val_acc: 0.7139\n",
      "Epoch 55/200\n",
      "1164/1164 [==============================] - 0s 137us/step - loss: 0.6883 - acc: 0.7191 - val_loss: 0.6800 - val_acc: 0.7311\n",
      "Epoch 56/200\n",
      "1164/1164 [==============================] - 0s 112us/step - loss: 0.6854 - acc: 0.7242 - val_loss: 0.6912 - val_acc: 0.7182\n",
      "Epoch 57/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.6816 - acc: 0.7182 - val_loss: 0.6675 - val_acc: 0.7337\n",
      "Epoch 58/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.6730 - acc: 0.7277 - val_loss: 0.6694 - val_acc: 0.7259\n",
      "Epoch 59/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.6741 - acc: 0.7268 - val_loss: 0.6611 - val_acc: 0.7380\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 97us/step - loss: 0.6714 - acc: 0.7371 - val_loss: 0.6694 - val_acc: 0.7234\n",
      "Epoch 61/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.6674 - acc: 0.7337 - val_loss: 0.6546 - val_acc: 0.7552\n",
      "Epoch 62/200\n",
      "1164/1164 [==============================] - 0s 79us/step - loss: 0.6710 - acc: 0.7431 - val_loss: 0.6479 - val_acc: 0.7543\n",
      "Epoch 63/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.6582 - acc: 0.7388 - val_loss: 0.6483 - val_acc: 0.7620\n",
      "Epoch 64/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.6587 - acc: 0.7491 - val_loss: 0.6623 - val_acc: 0.7277\n",
      "Epoch 65/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.6606 - acc: 0.7337 - val_loss: 0.6473 - val_acc: 0.7337\n",
      "Epoch 66/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.6515 - acc: 0.7448 - val_loss: 0.6560 - val_acc: 0.7397\n",
      "Epoch 67/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.6442 - acc: 0.7491 - val_loss: 0.6358 - val_acc: 0.7603\n",
      "Epoch 68/200\n",
      "1164/1164 [==============================] - 0s 241us/step - loss: 0.6432 - acc: 0.7491 - val_loss: 0.6394 - val_acc: 0.7423\n",
      "Epoch 69/200\n",
      "1164/1164 [==============================] - 0s 124us/step - loss: 0.6431 - acc: 0.7466 - val_loss: 0.6364 - val_acc: 0.7423\n",
      "Epoch 70/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.6420 - acc: 0.7448 - val_loss: 0.6256 - val_acc: 0.7595\n",
      "Epoch 71/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6328 - acc: 0.7483 - val_loss: 0.6336 - val_acc: 0.7457\n",
      "Epoch 72/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6397 - acc: 0.7431 - val_loss: 0.6208 - val_acc: 0.7517\n",
      "Epoch 73/200\n",
      "1164/1164 [==============================] - 0s 119us/step - loss: 0.6347 - acc: 0.7363 - val_loss: 0.6179 - val_acc: 0.7595\n",
      "Epoch 74/200\n",
      "1164/1164 [==============================] - 0s 128us/step - loss: 0.6285 - acc: 0.7509 - val_loss: 0.6458 - val_acc: 0.7320\n",
      "Epoch 75/200\n",
      "1164/1164 [==============================] - 0s 114us/step - loss: 0.6240 - acc: 0.7569 - val_loss: 0.6123 - val_acc: 0.7689\n",
      "Epoch 76/200\n",
      "1164/1164 [==============================] - 0s 148us/step - loss: 0.6183 - acc: 0.7552 - val_loss: 0.6150 - val_acc: 0.7543\n",
      "Epoch 77/200\n",
      "1164/1164 [==============================] - 0s 110us/step - loss: 0.6180 - acc: 0.7466 - val_loss: 0.6155 - val_acc: 0.7517\n",
      "Epoch 78/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.6193 - acc: 0.7586 - val_loss: 0.6190 - val_acc: 0.7560\n",
      "Epoch 79/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.6174 - acc: 0.7560 - val_loss: 0.5986 - val_acc: 0.7655\n",
      "Epoch 80/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6103 - acc: 0.7560 - val_loss: 0.5969 - val_acc: 0.7680\n",
      "Epoch 81/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.6111 - acc: 0.7560 - val_loss: 0.5968 - val_acc: 0.7766\n",
      "Epoch 82/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.6126 - acc: 0.7517 - val_loss: 0.5982 - val_acc: 0.7698\n",
      "Epoch 83/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.6072 - acc: 0.7595 - val_loss: 0.6067 - val_acc: 0.7560\n",
      "Epoch 84/200\n",
      "1164/1164 [==============================] - 0s 113us/step - loss: 0.6011 - acc: 0.7663 - val_loss: 0.5953 - val_acc: 0.7509\n",
      "Epoch 85/200\n",
      "1164/1164 [==============================] - 0s 103us/step - loss: 0.5933 - acc: 0.7646 - val_loss: 0.5997 - val_acc: 0.7560\n",
      "Epoch 86/200\n",
      "1164/1164 [==============================] - 0s 94us/step - loss: 0.5950 - acc: 0.7603 - val_loss: 0.5835 - val_acc: 0.7603\n",
      "Epoch 87/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6039 - acc: 0.7577 - val_loss: 0.5802 - val_acc: 0.7612\n",
      "Epoch 88/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.5928 - acc: 0.7629 - val_loss: 0.5897 - val_acc: 0.7629\n",
      "Epoch 89/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.5902 - acc: 0.7612 - val_loss: 0.5827 - val_acc: 0.7680\n",
      "Epoch 90/200\n",
      "1164/1164 [==============================] - 0s 193us/step - loss: 0.5935 - acc: 0.7577 - val_loss: 0.5723 - val_acc: 0.7689\n",
      "Epoch 91/200\n",
      "1164/1164 [==============================] - 0s 192us/step - loss: 0.5882 - acc: 0.7603 - val_loss: 0.5753 - val_acc: 0.7758\n",
      "Epoch 92/200\n",
      "1164/1164 [==============================] - 0s 144us/step - loss: 0.5875 - acc: 0.7612 - val_loss: 0.5723 - val_acc: 0.7766\n",
      "Epoch 93/200\n",
      "1164/1164 [==============================] - 0s 122us/step - loss: 0.5767 - acc: 0.7680 - val_loss: 0.5659 - val_acc: 0.7826\n",
      "Epoch 94/200\n",
      "1164/1164 [==============================] - 0s 82us/step - loss: 0.5767 - acc: 0.7672 - val_loss: 0.5641 - val_acc: 0.7766\n",
      "Epoch 95/200\n",
      "1164/1164 [==============================] - 0s 212us/step - loss: 0.5742 - acc: 0.7706 - val_loss: 0.5698 - val_acc: 0.7689\n",
      "Epoch 96/200\n",
      "1164/1164 [==============================] - 0s 81us/step - loss: 0.5788 - acc: 0.7741 - val_loss: 0.5774 - val_acc: 0.7852\n",
      "Epoch 97/200\n",
      "1164/1164 [==============================] - 0s 108us/step - loss: 0.5717 - acc: 0.7689 - val_loss: 0.5603 - val_acc: 0.7861\n",
      "Epoch 98/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.5629 - acc: 0.7852 - val_loss: 0.5566 - val_acc: 0.7835\n",
      "Epoch 99/200\n",
      "1164/1164 [==============================] - 0s 182us/step - loss: 0.5647 - acc: 0.7715 - val_loss: 0.5679 - val_acc: 0.7706\n",
      "Epoch 100/200\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 0.5699 - acc: 0.7741 - val_loss: 0.5524 - val_acc: 0.7758\n",
      "Epoch 101/200\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 0.5667 - acc: 0.7698 - val_loss: 0.5484 - val_acc: 0.7844\n",
      "Epoch 102/200\n",
      "1164/1164 [==============================] - 0s 194us/step - loss: 0.5625 - acc: 0.7732 - val_loss: 0.5745 - val_acc: 0.7792\n",
      "Epoch 103/200\n",
      "1164/1164 [==============================] - 0s 92us/step - loss: 0.5580 - acc: 0.7809 - val_loss: 0.5593 - val_acc: 0.7887\n",
      "Epoch 104/200\n",
      "1164/1164 [==============================] - 0s 106us/step - loss: 0.5570 - acc: 0.7809 - val_loss: 0.5598 - val_acc: 0.7749\n",
      "Epoch 105/200\n",
      "1164/1164 [==============================] - 0s 121us/step - loss: 0.5611 - acc: 0.7775 - val_loss: 0.5422 - val_acc: 0.7930\n",
      "Epoch 106/200\n",
      "1164/1164 [==============================] - 0s 114us/step - loss: 0.5560 - acc: 0.7766 - val_loss: 0.5584 - val_acc: 0.7826\n",
      "Epoch 107/200\n",
      "1164/1164 [==============================] - 0s 116us/step - loss: 0.5508 - acc: 0.7895 - val_loss: 0.5475 - val_acc: 0.7775\n",
      "Epoch 108/200\n",
      "1164/1164 [==============================] - 0s 254us/step - loss: 0.5481 - acc: 0.7930 - val_loss: 0.5396 - val_acc: 0.7973\n",
      "Epoch 109/200\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 0.5452 - acc: 0.7887 - val_loss: 0.5390 - val_acc: 0.7878\n",
      "Epoch 110/200\n",
      "1164/1164 [==============================] - 0s 188us/step - loss: 0.5428 - acc: 0.7947 - val_loss: 0.5422 - val_acc: 0.7947\n",
      "Epoch 111/200\n",
      "1164/1164 [==============================] - 0s 180us/step - loss: 0.5425 - acc: 0.7930 - val_loss: 0.5312 - val_acc: 0.7938\n",
      "Epoch 112/200\n",
      "1164/1164 [==============================] - 0s 143us/step - loss: 0.5428 - acc: 0.7912 - val_loss: 0.5491 - val_acc: 0.7835\n",
      "Epoch 113/200\n",
      "1164/1164 [==============================] - 0s 152us/step - loss: 0.5399 - acc: 0.7895 - val_loss: 0.5419 - val_acc: 0.7826\n",
      "Epoch 114/200\n",
      "1164/1164 [==============================] - 0s 104us/step - loss: 0.5404 - acc: 0.7861 - val_loss: 0.5292 - val_acc: 0.7921\n",
      "Epoch 115/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.5392 - acc: 0.7861 - val_loss: 0.5267 - val_acc: 0.8058\n",
      "Epoch 116/200\n",
      "1164/1164 [==============================] - 0s 182us/step - loss: 0.5338 - acc: 0.7973 - val_loss: 0.5227 - val_acc: 0.8076\n",
      "Epoch 117/200\n",
      "1164/1164 [==============================] - 0s 172us/step - loss: 0.5294 - acc: 0.8041 - val_loss: 0.5385 - val_acc: 0.7844\n",
      "Epoch 118/200\n",
      "1164/1164 [==============================] - 0s 97us/step - loss: 0.5448 - acc: 0.7947 - val_loss: 0.5175 - val_acc: 0.8015\n",
      "Epoch 119/200\n",
      "1164/1164 [==============================] - 0s 181us/step - loss: 0.5229 - acc: 0.8007 - val_loss: 0.5141 - val_acc: 0.8179\n",
      "Epoch 120/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.5276 - acc: 0.8033 - val_loss: 0.5104 - val_acc: 0.8084\n",
      "Epoch 121/200\n",
      "1164/1164 [==============================] - 0s 92us/step - loss: 0.5220 - acc: 0.8050 - val_loss: 0.5190 - val_acc: 0.8101\n",
      "Epoch 122/200\n",
      "1164/1164 [==============================] - 0s 77us/step - loss: 0.5296 - acc: 0.7973 - val_loss: 0.5145 - val_acc: 0.7955\n",
      "Epoch 123/200\n",
      "1164/1164 [==============================] - 0s 114us/step - loss: 0.5176 - acc: 0.8007 - val_loss: 0.5094 - val_acc: 0.8093\n",
      "Epoch 124/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.5123 - acc: 0.8144 - val_loss: 0.5115 - val_acc: 0.8239\n",
      "Epoch 125/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.5144 - acc: 0.8127 - val_loss: 0.5057 - val_acc: 0.8136\n",
      "Epoch 126/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.5184 - acc: 0.8015 - val_loss: 0.5150 - val_acc: 0.8076\n",
      "Epoch 127/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.5109 - acc: 0.8136 - val_loss: 0.5027 - val_acc: 0.8093\n",
      "Epoch 128/200\n",
      "1164/1164 [==============================] - 0s 77us/step - loss: 0.5134 - acc: 0.8050 - val_loss: 0.4927 - val_acc: 0.8230\n",
      "Epoch 129/200\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 0.5033 - acc: 0.7998 - val_loss: 0.5009 - val_acc: 0.8076\n",
      "Epoch 130/200\n",
      "1164/1164 [==============================] - 0s 126us/step - loss: 0.5031 - acc: 0.8058 - val_loss: 0.4974 - val_acc: 0.8179\n",
      "Epoch 131/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.5072 - acc: 0.8041 - val_loss: 0.4932 - val_acc: 0.8187\n",
      "Epoch 132/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.5007 - acc: 0.8187 - val_loss: 0.4895 - val_acc: 0.8170\n",
      "Epoch 133/200\n",
      "1164/1164 [==============================] - 0s 74us/step - loss: 0.5044 - acc: 0.8110 - val_loss: 0.5007 - val_acc: 0.8179\n",
      "Epoch 134/200\n",
      "1164/1164 [==============================] - 0s 94us/step - loss: 0.5016 - acc: 0.8170 - val_loss: 0.4934 - val_acc: 0.8076\n",
      "Epoch 135/200\n",
      "1164/1164 [==============================] - 0s 102us/step - loss: 0.4905 - acc: 0.8204 - val_loss: 0.5115 - val_acc: 0.8162\n",
      "Epoch 136/200\n",
      "1164/1164 [==============================] - 0s 249us/step - loss: 0.4951 - acc: 0.8247 - val_loss: 0.4847 - val_acc: 0.8265\n",
      "Epoch 137/200\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 0.4953 - acc: 0.8170 - val_loss: 0.4811 - val_acc: 0.8247\n",
      "Epoch 138/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.4876 - acc: 0.8170 - val_loss: 0.4769 - val_acc: 0.8316\n",
      "Epoch 139/200\n",
      "1164/1164 [==============================] - 0s 151us/step - loss: 0.4925 - acc: 0.8187 - val_loss: 0.4837 - val_acc: 0.8170\n",
      "Epoch 140/200\n",
      "1164/1164 [==============================] - 0s 202us/step - loss: 0.4899 - acc: 0.8179 - val_loss: 0.4739 - val_acc: 0.8162\n",
      "Epoch 141/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.4905 - acc: 0.8204 - val_loss: 0.4905 - val_acc: 0.8265\n",
      "Epoch 142/200\n",
      "1164/1164 [==============================] - 0s 171us/step - loss: 0.4906 - acc: 0.8170 - val_loss: 0.4794 - val_acc: 0.8162\n",
      "Epoch 143/200\n",
      "1164/1164 [==============================] - 0s 110us/step - loss: 0.4856 - acc: 0.8187 - val_loss: 0.4843 - val_acc: 0.8290\n",
      "Epoch 144/200\n",
      "1164/1164 [==============================] - 0s 99us/step - loss: 0.4841 - acc: 0.8230 - val_loss: 0.4667 - val_acc: 0.8290\n",
      "Epoch 145/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4827 - acc: 0.8222 - val_loss: 0.4777 - val_acc: 0.8153\n",
      "Epoch 146/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.4770 - acc: 0.8265 - val_loss: 0.4691 - val_acc: 0.8230\n",
      "Epoch 147/200\n",
      "1164/1164 [==============================] - 0s 78us/step - loss: 0.4765 - acc: 0.8239 - val_loss: 0.5117 - val_acc: 0.7990\n",
      "Epoch 148/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.4739 - acc: 0.8342 - val_loss: 0.4759 - val_acc: 0.8376\n",
      "Epoch 149/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.4822 - acc: 0.8290 - val_loss: 0.4626 - val_acc: 0.8411\n",
      "Epoch 150/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.4662 - acc: 0.8436 - val_loss: 0.4724 - val_acc: 0.8359\n",
      "Epoch 151/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.4797 - acc: 0.8222 - val_loss: 0.4570 - val_acc: 0.8411\n",
      "Epoch 152/200\n",
      "1164/1164 [==============================] - 0s 78us/step - loss: 0.4648 - acc: 0.8359 - val_loss: 0.4606 - val_acc: 0.8325\n",
      "Epoch 153/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.4671 - acc: 0.8299 - val_loss: 0.4674 - val_acc: 0.8419\n",
      "Epoch 154/200\n",
      "1164/1164 [==============================] - 0s 171us/step - loss: 0.4645 - acc: 0.8402 - val_loss: 0.4589 - val_acc: 0.8445\n",
      "Epoch 155/200\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 0.4660 - acc: 0.8325 - val_loss: 0.4599 - val_acc: 0.8368\n",
      "Epoch 156/200\n",
      "1164/1164 [==============================] - 0s 146us/step - loss: 0.4575 - acc: 0.8436 - val_loss: 0.4489 - val_acc: 0.8376\n",
      "Epoch 157/200\n",
      "1164/1164 [==============================] - 0s 188us/step - loss: 0.4581 - acc: 0.8368 - val_loss: 0.4500 - val_acc: 0.8393\n",
      "Epoch 158/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.4597 - acc: 0.8385 - val_loss: 0.4602 - val_acc: 0.8402\n",
      "Epoch 159/200\n",
      "1164/1164 [==============================] - 0s 121us/step - loss: 0.4568 - acc: 0.8411 - val_loss: 0.4457 - val_acc: 0.8376\n",
      "Epoch 160/200\n",
      "1164/1164 [==============================] - 0s 135us/step - loss: 0.4585 - acc: 0.8368 - val_loss: 0.4395 - val_acc: 0.8436\n",
      "Epoch 161/200\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 0.4531 - acc: 0.8419 - val_loss: 0.4409 - val_acc: 0.8522\n",
      "Epoch 162/200\n",
      "1164/1164 [==============================] - 0s 127us/step - loss: 0.4510 - acc: 0.8471 - val_loss: 0.4418 - val_acc: 0.8479\n",
      "Epoch 163/200\n",
      "1164/1164 [==============================] - 0s 134us/step - loss: 0.4509 - acc: 0.8471 - val_loss: 0.4504 - val_acc: 0.8385\n",
      "Epoch 164/200\n",
      "1164/1164 [==============================] - 0s 125us/step - loss: 0.4535 - acc: 0.8393 - val_loss: 0.4348 - val_acc: 0.8531\n",
      "Epoch 165/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.4547 - acc: 0.8368 - val_loss: 0.4353 - val_acc: 0.8436\n",
      "Epoch 166/200\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 0.4439 - acc: 0.8385 - val_loss: 0.4389 - val_acc: 0.8411\n",
      "Epoch 167/200\n",
      "1164/1164 [==============================] - 0s 80us/step - loss: 0.4429 - acc: 0.8445 - val_loss: 0.4502 - val_acc: 0.8522\n",
      "Epoch 168/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4474 - acc: 0.8385 - val_loss: 0.4349 - val_acc: 0.8428\n",
      "Epoch 169/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4441 - acc: 0.8445 - val_loss: 0.4420 - val_acc: 0.8333\n",
      "Epoch 170/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.4348 - acc: 0.8497 - val_loss: 0.4238 - val_acc: 0.8591\n",
      "Epoch 171/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.4396 - acc: 0.8436 - val_loss: 0.4346 - val_acc: 0.8479\n",
      "Epoch 172/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.4382 - acc: 0.8471 - val_loss: 0.4261 - val_acc: 0.8488\n",
      "Epoch 173/200\n",
      "1164/1164 [==============================] - 0s 92us/step - loss: 0.4323 - acc: 0.8514 - val_loss: 0.4268 - val_acc: 0.8514\n",
      "Epoch 174/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.4327 - acc: 0.8479 - val_loss: 0.4238 - val_acc: 0.8497\n",
      "Epoch 175/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4299 - acc: 0.8454 - val_loss: 0.4489 - val_acc: 0.8393\n",
      "Epoch 176/200\n",
      "1164/1164 [==============================] - 0s 114us/step - loss: 0.4337 - acc: 0.8488 - val_loss: 0.4280 - val_acc: 0.8436\n",
      "Epoch 177/200\n",
      "1164/1164 [==============================] - 0s 154us/step - loss: 0.4259 - acc: 0.8497 - val_loss: 0.4129 - val_acc: 0.8608\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 73us/step - loss: 0.4335 - acc: 0.8454 - val_loss: 0.4244 - val_acc: 0.8462\n",
      "Epoch 179/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4425 - acc: 0.8265 - val_loss: 0.4085 - val_acc: 0.8608\n",
      "Epoch 180/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.4240 - acc: 0.8540 - val_loss: 0.4081 - val_acc: 0.8643\n",
      "Epoch 181/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4209 - acc: 0.8505 - val_loss: 0.4143 - val_acc: 0.8574\n",
      "Epoch 182/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4229 - acc: 0.8488 - val_loss: 0.4571 - val_acc: 0.8273\n",
      "Epoch 183/200\n",
      "1164/1164 [==============================] - 0s 83us/step - loss: 0.4260 - acc: 0.8514 - val_loss: 0.4105 - val_acc: 0.8574\n",
      "Epoch 184/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.4246 - acc: 0.8514 - val_loss: 0.4092 - val_acc: 0.8531\n",
      "Epoch 185/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.4138 - acc: 0.8565 - val_loss: 0.4071 - val_acc: 0.8522\n",
      "Epoch 186/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.4131 - acc: 0.8574 - val_loss: 0.4223 - val_acc: 0.8574\n",
      "Epoch 187/200\n",
      "1164/1164 [==============================] - 0s 58us/step - loss: 0.4140 - acc: 0.8557 - val_loss: 0.4010 - val_acc: 0.8617\n",
      "Epoch 188/200\n",
      "1164/1164 [==============================] - 0s 74us/step - loss: 0.4111 - acc: 0.8505 - val_loss: 0.4220 - val_acc: 0.8385\n",
      "Epoch 189/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.4184 - acc: 0.8445 - val_loss: 0.4035 - val_acc: 0.8686\n",
      "Epoch 190/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4161 - acc: 0.8497 - val_loss: 0.3944 - val_acc: 0.8729\n",
      "Epoch 191/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4061 - acc: 0.8557 - val_loss: 0.4045 - val_acc: 0.8557\n",
      "Epoch 192/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.4140 - acc: 0.8514 - val_loss: 0.4010 - val_acc: 0.8557\n",
      "Epoch 193/200\n",
      "1164/1164 [==============================] - 0s 80us/step - loss: 0.4010 - acc: 0.8660 - val_loss: 0.3944 - val_acc: 0.8591\n",
      "Epoch 194/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4011 - acc: 0.8591 - val_loss: 0.4100 - val_acc: 0.8505\n",
      "Epoch 195/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.3995 - acc: 0.8660 - val_loss: 0.3953 - val_acc: 0.8600\n",
      "Epoch 196/200\n",
      "1164/1164 [==============================] - 0s 185us/step - loss: 0.4022 - acc: 0.8557 - val_loss: 0.4102 - val_acc: 0.8591\n",
      "Epoch 197/200\n",
      "1164/1164 [==============================] - 0s 78us/step - loss: 0.4053 - acc: 0.8582 - val_loss: 0.4004 - val_acc: 0.8540\n",
      "Epoch 198/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.4011 - acc: 0.8617 - val_loss: 0.4201 - val_acc: 0.8402\n",
      "Epoch 199/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.3995 - acc: 0.8617 - val_loss: 0.3931 - val_acc: 0.8660\n",
      "Epoch 200/200\n",
      "1164/1164 [==============================] - 0s 83us/step - loss: 0.3960 - acc: 0.8668 - val_loss: 0.3812 - val_acc: 0.8634\n",
      "4656/4656 [==============================] - 1s 119us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 1164 samples\n",
      "Epoch 1/200\n",
      "1164/1164 [==============================] - 3s 2ms/step - loss: 1.3177 - acc: 0.2302 - val_loss: 1.1365 - val_acc: 0.3651\n",
      "Epoch 2/200\n",
      "1164/1164 [==============================] - 0s 110us/step - loss: 1.0708 - acc: 0.4570 - val_loss: 0.9983 - val_acc: 0.5387\n",
      "Epoch 3/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.9704 - acc: 0.5765 - val_loss: 0.9323 - val_acc: 0.5851\n",
      "Epoch 4/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.9192 - acc: 0.6082 - val_loss: 0.8984 - val_acc: 0.6108\n",
      "Epoch 5/200\n",
      "1164/1164 [==============================] - 0s 117us/step - loss: 0.8929 - acc: 0.6065 - val_loss: 0.8793 - val_acc: 0.6065\n",
      "Epoch 6/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.8772 - acc: 0.6143 - val_loss: 0.8673 - val_acc: 0.6151\n",
      "Epoch 7/200\n",
      "1164/1164 [==============================] - 0s 124us/step - loss: 0.8654 - acc: 0.6203 - val_loss: 0.8565 - val_acc: 0.6160\n",
      "Epoch 8/200\n",
      "1164/1164 [==============================] - 0s 146us/step - loss: 0.8584 - acc: 0.6194 - val_loss: 0.8486 - val_acc: 0.6160\n",
      "Epoch 9/200\n",
      "1164/1164 [==============================] - 0s 94us/step - loss: 0.8487 - acc: 0.6186 - val_loss: 0.8442 - val_acc: 0.6143\n",
      "Epoch 10/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.8445 - acc: 0.6134 - val_loss: 0.8382 - val_acc: 0.6186\n",
      "Epoch 11/200\n",
      "1164/1164 [==============================] - 0s 74us/step - loss: 0.8398 - acc: 0.6160 - val_loss: 0.8309 - val_acc: 0.6211\n",
      "Epoch 12/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.8335 - acc: 0.6203 - val_loss: 0.8271 - val_acc: 0.6211\n",
      "Epoch 13/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.8302 - acc: 0.6203 - val_loss: 0.8199 - val_acc: 0.6229\n",
      "Epoch 14/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.8248 - acc: 0.6229 - val_loss: 0.8170 - val_acc: 0.6229\n",
      "Epoch 15/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.8203 - acc: 0.6237 - val_loss: 0.8123 - val_acc: 0.6203\n",
      "Epoch 16/200\n",
      "1164/1164 [==============================] - 0s 104us/step - loss: 0.8155 - acc: 0.6237 - val_loss: 0.8074 - val_acc: 0.6280\n",
      "Epoch 17/200\n",
      "1164/1164 [==============================] - 0s 97us/step - loss: 0.8119 - acc: 0.6306 - val_loss: 0.8051 - val_acc: 0.6194\n",
      "Epoch 18/200\n",
      "1164/1164 [==============================] - 0s 107us/step - loss: 0.8075 - acc: 0.6323 - val_loss: 0.7989 - val_acc: 0.6392\n",
      "Epoch 19/200\n",
      "1164/1164 [==============================] - 0s 119us/step - loss: 0.8054 - acc: 0.6306 - val_loss: 0.7946 - val_acc: 0.6392\n",
      "Epoch 20/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.7988 - acc: 0.6340 - val_loss: 0.7903 - val_acc: 0.6375\n",
      "Epoch 21/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.7960 - acc: 0.6392 - val_loss: 0.7935 - val_acc: 0.6383\n",
      "Epoch 22/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.7995 - acc: 0.6409 - val_loss: 0.7872 - val_acc: 0.6546\n",
      "Epoch 23/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.7913 - acc: 0.6400 - val_loss: 0.7797 - val_acc: 0.6469\n",
      "Epoch 24/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.7843 - acc: 0.6486 - val_loss: 0.7814 - val_acc: 0.6478\n",
      "Epoch 25/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.7856 - acc: 0.6469 - val_loss: 0.7791 - val_acc: 0.6349\n",
      "Epoch 26/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.7808 - acc: 0.6486 - val_loss: 0.7724 - val_acc: 0.6529\n",
      "Epoch 27/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.7782 - acc: 0.6469 - val_loss: 0.7713 - val_acc: 0.6624\n",
      "Epoch 28/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.7722 - acc: 0.6564 - val_loss: 0.7634 - val_acc: 0.6692\n",
      "Epoch 29/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.7698 - acc: 0.6555 - val_loss: 0.7608 - val_acc: 0.6684\n",
      "Epoch 30/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.7664 - acc: 0.6572 - val_loss: 0.7620 - val_acc: 0.6727\n",
      "Epoch 31/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.7621 - acc: 0.6632 - val_loss: 0.7550 - val_acc: 0.6701\n",
      "Epoch 32/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.7593 - acc: 0.6675 - val_loss: 0.7540 - val_acc: 0.6641\n",
      "Epoch 33/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.7550 - acc: 0.6649 - val_loss: 0.7459 - val_acc: 0.6701\n",
      "Epoch 34/200\n",
      "1164/1164 [==============================] - 0s 74us/step - loss: 0.7522 - acc: 0.6753 - val_loss: 0.7438 - val_acc: 0.6787\n",
      "Epoch 35/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.7501 - acc: 0.6658 - val_loss: 0.7474 - val_acc: 0.6624\n",
      "Epoch 36/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.7450 - acc: 0.6675 - val_loss: 0.7381 - val_acc: 0.6796\n",
      "Epoch 37/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.7404 - acc: 0.6710 - val_loss: 0.7321 - val_acc: 0.6804\n",
      "Epoch 38/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.7406 - acc: 0.6727 - val_loss: 0.7321 - val_acc: 0.6838\n",
      "Epoch 39/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.7359 - acc: 0.6727 - val_loss: 0.7325 - val_acc: 0.6692\n",
      "Epoch 40/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.7335 - acc: 0.6744 - val_loss: 0.7256 - val_acc: 0.6796\n",
      "Epoch 41/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.7302 - acc: 0.6770 - val_loss: 0.7221 - val_acc: 0.6778\n",
      "Epoch 42/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.7255 - acc: 0.6701 - val_loss: 0.7242 - val_acc: 0.6753\n",
      "Epoch 43/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.7237 - acc: 0.6770 - val_loss: 0.7155 - val_acc: 0.6847\n",
      "Epoch 44/200\n",
      "1164/1164 [==============================] - 0s 78us/step - loss: 0.7221 - acc: 0.6787 - val_loss: 0.7071 - val_acc: 0.6847\n",
      "Epoch 45/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.7155 - acc: 0.6804 - val_loss: 0.7080 - val_acc: 0.6916\n",
      "Epoch 46/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.7142 - acc: 0.6813 - val_loss: 0.7058 - val_acc: 0.6813\n",
      "Epoch 47/200\n",
      "1164/1164 [==============================] - 0s 73us/step - loss: 0.7067 - acc: 0.6890 - val_loss: 0.7016 - val_acc: 0.6881\n",
      "Epoch 48/200\n",
      "1164/1164 [==============================] - 0s 298us/step - loss: 0.7046 - acc: 0.6838 - val_loss: 0.7031 - val_acc: 0.6873\n",
      "Epoch 49/200\n",
      "1164/1164 [==============================] - 0s 87us/step - loss: 0.7024 - acc: 0.6873 - val_loss: 0.6916 - val_acc: 0.6985\n",
      "Epoch 50/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.7000 - acc: 0.6890 - val_loss: 0.6993 - val_acc: 0.6890\n",
      "Epoch 51/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.6989 - acc: 0.6950 - val_loss: 0.6887 - val_acc: 0.6933\n",
      "Epoch 52/200\n",
      "1164/1164 [==============================] - 0s 116us/step - loss: 0.6906 - acc: 0.6899 - val_loss: 0.6870 - val_acc: 0.6985\n",
      "Epoch 53/200\n",
      "1164/1164 [==============================] - 0s 104us/step - loss: 0.6894 - acc: 0.7010 - val_loss: 0.6794 - val_acc: 0.7002\n",
      "Epoch 54/200\n",
      "1164/1164 [==============================] - 0s 212us/step - loss: 0.6860 - acc: 0.7045 - val_loss: 0.6770 - val_acc: 0.6942\n",
      "Epoch 55/200\n",
      "1164/1164 [==============================] - 0s 124us/step - loss: 0.6828 - acc: 0.6907 - val_loss: 0.6775 - val_acc: 0.7010\n",
      "Epoch 56/200\n",
      "1164/1164 [==============================] - 0s 131us/step - loss: 0.6796 - acc: 0.6907 - val_loss: 0.6726 - val_acc: 0.7139\n",
      "Epoch 57/200\n",
      "1164/1164 [==============================] - 0s 178us/step - loss: 0.6808 - acc: 0.7088 - val_loss: 0.6623 - val_acc: 0.7131\n",
      "Epoch 58/200\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 0.6741 - acc: 0.7045 - val_loss: 0.6650 - val_acc: 0.7225\n",
      "Epoch 59/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.6720 - acc: 0.7139 - val_loss: 0.6567 - val_acc: 0.7234\n",
      "Epoch 60/200\n",
      "1164/1164 [==============================] - 0s 124us/step - loss: 0.6667 - acc: 0.7242 - val_loss: 0.6555 - val_acc: 0.7191\n",
      "Epoch 61/200\n",
      "1164/1164 [==============================] - 0s 101us/step - loss: 0.6649 - acc: 0.7070 - val_loss: 0.6562 - val_acc: 0.7182\n",
      "Epoch 62/200\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 0.6591 - acc: 0.7139 - val_loss: 0.6458 - val_acc: 0.7277\n",
      "Epoch 63/200\n",
      "1164/1164 [==============================] - 0s 135us/step - loss: 0.6539 - acc: 0.7131 - val_loss: 0.6487 - val_acc: 0.7139\n",
      "Epoch 64/200\n",
      "1164/1164 [==============================] - 0s 99us/step - loss: 0.6549 - acc: 0.7182 - val_loss: 0.6385 - val_acc: 0.7380\n",
      "Epoch 65/200\n",
      "1164/1164 [==============================] - 0s 116us/step - loss: 0.6491 - acc: 0.7234 - val_loss: 0.6353 - val_acc: 0.7397\n",
      "Epoch 66/200\n",
      "1164/1164 [==============================] - 0s 102us/step - loss: 0.6460 - acc: 0.7199 - val_loss: 0.6425 - val_acc: 0.7234\n",
      "Epoch 67/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.6452 - acc: 0.7225 - val_loss: 0.6285 - val_acc: 0.7251\n",
      "Epoch 68/200\n",
      "1164/1164 [==============================] - 0s 115us/step - loss: 0.6395 - acc: 0.7208 - val_loss: 0.6395 - val_acc: 0.7105\n",
      "Epoch 69/200\n",
      "1164/1164 [==============================] - 0s 153us/step - loss: 0.6440 - acc: 0.7251 - val_loss: 0.6313 - val_acc: 0.7388\n",
      "Epoch 70/200\n",
      "1164/1164 [==============================] - 0s 130us/step - loss: 0.6342 - acc: 0.7285 - val_loss: 0.6229 - val_acc: 0.7285\n",
      "Epoch 71/200\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 0.6373 - acc: 0.7242 - val_loss: 0.6255 - val_acc: 0.7131\n",
      "Epoch 72/200\n",
      "1164/1164 [==============================] - 0s 141us/step - loss: 0.6332 - acc: 0.7199 - val_loss: 0.6159 - val_acc: 0.7268\n",
      "Epoch 73/200\n",
      "1164/1164 [==============================] - 0s 137us/step - loss: 0.6224 - acc: 0.7285 - val_loss: 0.6135 - val_acc: 0.7328\n",
      "Epoch 74/200\n",
      "1164/1164 [==============================] - 0s 143us/step - loss: 0.6207 - acc: 0.7345 - val_loss: 0.6060 - val_acc: 0.7569\n",
      "Epoch 75/200\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 0.6172 - acc: 0.7302 - val_loss: 0.6202 - val_acc: 0.7277\n",
      "Epoch 76/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.6182 - acc: 0.7337 - val_loss: 0.6092 - val_acc: 0.7363\n",
      "Epoch 77/200\n",
      "1164/1164 [==============================] - 0s 124us/step - loss: 0.6190 - acc: 0.7345 - val_loss: 0.6205 - val_acc: 0.7397\n",
      "Epoch 78/200\n",
      "1164/1164 [==============================] - 0s 100us/step - loss: 0.6109 - acc: 0.7466 - val_loss: 0.5997 - val_acc: 0.7440\n",
      "Epoch 79/200\n",
      "1164/1164 [==============================] - 0s 114us/step - loss: 0.6100 - acc: 0.7405 - val_loss: 0.6050 - val_acc: 0.7371\n",
      "Epoch 80/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.6059 - acc: 0.7440 - val_loss: 0.6019 - val_acc: 0.7345\n",
      "Epoch 81/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.6002 - acc: 0.7431 - val_loss: 0.6098 - val_acc: 0.7440\n",
      "Epoch 82/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.6013 - acc: 0.7457 - val_loss: 0.5966 - val_acc: 0.7620\n",
      "Epoch 83/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.5969 - acc: 0.7509 - val_loss: 0.5898 - val_acc: 0.7320\n",
      "Epoch 84/200\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 0.5917 - acc: 0.7534 - val_loss: 0.5819 - val_acc: 0.7680\n",
      "Epoch 85/200\n",
      "1164/1164 [==============================] - 0s 122us/step - loss: 0.5866 - acc: 0.7689 - val_loss: 0.5952 - val_acc: 0.7466\n",
      "Epoch 86/200\n",
      "1164/1164 [==============================] - 0s 106us/step - loss: 0.5873 - acc: 0.7637 - val_loss: 0.5780 - val_acc: 0.7543\n",
      "Epoch 87/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.5822 - acc: 0.7595 - val_loss: 0.5791 - val_acc: 0.7818\n",
      "Epoch 88/200\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 0.5841 - acc: 0.7500 - val_loss: 0.5728 - val_acc: 0.7758\n",
      "Epoch 89/200\n",
      "1164/1164 [==============================] - 0s 116us/step - loss: 0.5789 - acc: 0.7612 - val_loss: 0.5734 - val_acc: 0.7698\n",
      "Epoch 90/200\n",
      "1164/1164 [==============================] - 0s 115us/step - loss: 0.5780 - acc: 0.7698 - val_loss: 0.5702 - val_acc: 0.7509\n",
      "Epoch 91/200\n",
      "1164/1164 [==============================] - 0s 212us/step - loss: 0.5749 - acc: 0.7629 - val_loss: 0.5692 - val_acc: 0.7646\n",
      "Epoch 92/200\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 0.5772 - acc: 0.7534 - val_loss: 0.5630 - val_acc: 0.7706\n",
      "Epoch 93/200\n",
      "1164/1164 [==============================] - 0s 108us/step - loss: 0.5679 - acc: 0.7689 - val_loss: 0.5638 - val_acc: 0.7723\n",
      "Epoch 94/200\n",
      "1164/1164 [==============================] - 0s 179us/step - loss: 0.5686 - acc: 0.7715 - val_loss: 0.5552 - val_acc: 0.7698\n",
      "Epoch 95/200\n",
      "1164/1164 [==============================] - 0s 89us/step - loss: 0.5649 - acc: 0.7741 - val_loss: 0.5577 - val_acc: 0.7895\n",
      "Epoch 96/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.5611 - acc: 0.7852 - val_loss: 0.5631 - val_acc: 0.7552\n",
      "Epoch 97/200\n",
      "1164/1164 [==============================] - 0s 190us/step - loss: 0.5611 - acc: 0.7672 - val_loss: 0.5514 - val_acc: 0.7852\n",
      "Epoch 98/200\n",
      "1164/1164 [==============================] - 0s 212us/step - loss: 0.5581 - acc: 0.7818 - val_loss: 0.5595 - val_acc: 0.7534\n",
      "Epoch 99/200\n",
      "1164/1164 [==============================] - 0s 126us/step - loss: 0.5543 - acc: 0.7801 - val_loss: 0.5423 - val_acc: 0.7818\n",
      "Epoch 100/200\n",
      "1164/1164 [==============================] - 0s 105us/step - loss: 0.5551 - acc: 0.7741 - val_loss: 0.5364 - val_acc: 0.7878\n",
      "Epoch 101/200\n",
      "1164/1164 [==============================] - 0s 121us/step - loss: 0.5417 - acc: 0.7895 - val_loss: 0.5669 - val_acc: 0.7741\n",
      "Epoch 102/200\n",
      "1164/1164 [==============================] - 0s 97us/step - loss: 0.5513 - acc: 0.7741 - val_loss: 0.5370 - val_acc: 0.7844\n",
      "Epoch 103/200\n",
      "1164/1164 [==============================] - 0s 90us/step - loss: 0.5419 - acc: 0.7852 - val_loss: 0.5303 - val_acc: 0.7930\n",
      "Epoch 104/200\n",
      "1164/1164 [==============================] - 0s 98us/step - loss: 0.5434 - acc: 0.7818 - val_loss: 0.5398 - val_acc: 0.7981\n",
      "Epoch 105/200\n",
      "1164/1164 [==============================] - 0s 93us/step - loss: 0.5391 - acc: 0.7844 - val_loss: 0.5250 - val_acc: 0.8058\n",
      "Epoch 106/200\n",
      "1164/1164 [==============================] - 0s 97us/step - loss: 0.5460 - acc: 0.7758 - val_loss: 0.5227 - val_acc: 0.8067\n",
      "Epoch 107/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.5342 - acc: 0.7869 - val_loss: 0.5190 - val_acc: 0.8007\n",
      "Epoch 108/200\n",
      "1164/1164 [==============================] - 0s 85us/step - loss: 0.5374 - acc: 0.7861 - val_loss: 0.5223 - val_acc: 0.8015\n",
      "Epoch 109/200\n",
      "1164/1164 [==============================] - 0s 146us/step - loss: 0.5268 - acc: 0.8015 - val_loss: 0.5226 - val_acc: 0.7930\n",
      "Epoch 110/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.5260 - acc: 0.7964 - val_loss: 0.5146 - val_acc: 0.8067\n",
      "Epoch 111/200\n",
      "1164/1164 [==============================] - 0s 96us/step - loss: 0.5240 - acc: 0.7904 - val_loss: 0.5248 - val_acc: 0.7766\n",
      "Epoch 112/200\n",
      "1164/1164 [==============================] - 0s 91us/step - loss: 0.5250 - acc: 0.7964 - val_loss: 0.5088 - val_acc: 0.8033\n",
      "Epoch 113/200\n",
      "1164/1164 [==============================] - 0s 87us/step - loss: 0.5192 - acc: 0.7912 - val_loss: 0.5103 - val_acc: 0.7981\n",
      "Epoch 114/200\n",
      "1164/1164 [==============================] - 0s 88us/step - loss: 0.5151 - acc: 0.8007 - val_loss: 0.5122 - val_acc: 0.7921\n",
      "Epoch 115/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.5143 - acc: 0.8024 - val_loss: 0.5114 - val_acc: 0.7947\n",
      "Epoch 116/200\n",
      "1164/1164 [==============================] - 0s 130us/step - loss: 0.5148 - acc: 0.8050 - val_loss: 0.5072 - val_acc: 0.8058\n",
      "Epoch 117/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.5118 - acc: 0.7981 - val_loss: 0.5041 - val_acc: 0.8041\n",
      "Epoch 118/200\n",
      "1164/1164 [==============================] - 0s 121us/step - loss: 0.5064 - acc: 0.8041 - val_loss: 0.5031 - val_acc: 0.7930\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 117us/step - loss: 0.5081 - acc: 0.8024 - val_loss: 0.4972 - val_acc: 0.8093\n",
      "Epoch 120/200\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 0.5047 - acc: 0.8076 - val_loss: 0.5018 - val_acc: 0.8041\n",
      "Epoch 121/200\n",
      "1164/1164 [==============================] - 0s 104us/step - loss: 0.5035 - acc: 0.7964 - val_loss: 0.4943 - val_acc: 0.8119\n",
      "Epoch 122/200\n",
      "1164/1164 [==============================] - 0s 112us/step - loss: 0.5052 - acc: 0.7973 - val_loss: 0.4870 - val_acc: 0.8110\n",
      "Epoch 123/200\n",
      "1164/1164 [==============================] - 0s 133us/step - loss: 0.5043 - acc: 0.8024 - val_loss: 0.4879 - val_acc: 0.8076\n",
      "Epoch 124/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.4965 - acc: 0.7998 - val_loss: 0.5211 - val_acc: 0.7869\n",
      "Epoch 125/200\n",
      "1164/1164 [==============================] - 0s 111us/step - loss: 0.4990 - acc: 0.8119 - val_loss: 0.4910 - val_acc: 0.8015\n",
      "Epoch 126/200\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 0.4933 - acc: 0.8084 - val_loss: 0.4802 - val_acc: 0.8170\n",
      "Epoch 127/200\n",
      "1164/1164 [==============================] - 0s 152us/step - loss: 0.4968 - acc: 0.7955 - val_loss: 0.4913 - val_acc: 0.8136\n",
      "Epoch 128/200\n",
      "1164/1164 [==============================] - 0s 106us/step - loss: 0.4852 - acc: 0.8136 - val_loss: 0.4814 - val_acc: 0.8136\n",
      "Epoch 129/200\n",
      "1164/1164 [==============================] - 0s 102us/step - loss: 0.4930 - acc: 0.8041 - val_loss: 0.4945 - val_acc: 0.8127\n",
      "Epoch 130/200\n",
      "1164/1164 [==============================] - 0s 95us/step - loss: 0.4906 - acc: 0.8093 - val_loss: 0.4757 - val_acc: 0.8110\n",
      "Epoch 131/200\n",
      "1164/1164 [==============================] - ETA: 0s - loss: 0.4222 - acc: 0.850 - 0s 73us/step - loss: 0.4922 - acc: 0.7990 - val_loss: 0.4739 - val_acc: 0.8153\n",
      "Epoch 132/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4829 - acc: 0.8179 - val_loss: 0.4910 - val_acc: 0.7878\n",
      "Epoch 133/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.4912 - acc: 0.8007 - val_loss: 0.4670 - val_acc: 0.8256\n",
      "Epoch 134/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4852 - acc: 0.8084 - val_loss: 0.4665 - val_acc: 0.8273\n",
      "Epoch 135/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.4823 - acc: 0.8187 - val_loss: 0.5014 - val_acc: 0.8015\n",
      "Epoch 136/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.4812 - acc: 0.8256 - val_loss: 0.4610 - val_acc: 0.8179\n",
      "Epoch 137/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4767 - acc: 0.8084 - val_loss: 0.4576 - val_acc: 0.8222\n",
      "Epoch 138/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4682 - acc: 0.8204 - val_loss: 0.4722 - val_acc: 0.8204\n",
      "Epoch 139/200\n",
      "1164/1164 [==============================] - 0s 109us/step - loss: 0.4812 - acc: 0.8136 - val_loss: 0.4730 - val_acc: 0.8110\n",
      "Epoch 140/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.4678 - acc: 0.8136 - val_loss: 0.4561 - val_acc: 0.8239\n",
      "Epoch 141/200\n",
      "1164/1164 [==============================] - 0s 124us/step - loss: 0.4646 - acc: 0.8187 - val_loss: 0.4591 - val_acc: 0.8247\n",
      "Epoch 142/200\n",
      "1164/1164 [==============================] - 0s 135us/step - loss: 0.4690 - acc: 0.8153 - val_loss: 0.4629 - val_acc: 0.8179\n",
      "Epoch 143/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4620 - acc: 0.8187 - val_loss: 0.4505 - val_acc: 0.8179\n",
      "Epoch 144/200\n",
      "1164/1164 [==============================] - 0s 81us/step - loss: 0.4644 - acc: 0.8153 - val_loss: 0.4543 - val_acc: 0.8385\n",
      "Epoch 145/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.4622 - acc: 0.8196 - val_loss: 0.4515 - val_acc: 0.8385\n",
      "Epoch 146/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4652 - acc: 0.8222 - val_loss: 0.4602 - val_acc: 0.8162\n",
      "Epoch 147/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.4558 - acc: 0.8187 - val_loss: 0.4457 - val_acc: 0.8170\n",
      "Epoch 148/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.4584 - acc: 0.8282 - val_loss: 0.4402 - val_acc: 0.8402\n",
      "Epoch 149/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.4527 - acc: 0.8308 - val_loss: 0.4696 - val_acc: 0.8187\n",
      "Epoch 150/200\n",
      "1164/1164 [==============================] - 0s 74us/step - loss: 0.4541 - acc: 0.8325 - val_loss: 0.4988 - val_acc: 0.7887\n",
      "Epoch 151/200\n",
      "1164/1164 [==============================] - 0s 82us/step - loss: 0.4574 - acc: 0.8179 - val_loss: 0.4348 - val_acc: 0.8411\n",
      "Epoch 152/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4467 - acc: 0.8342 - val_loss: 0.4374 - val_acc: 0.8454\n",
      "Epoch 153/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4430 - acc: 0.8342 - val_loss: 0.4310 - val_acc: 0.8385\n",
      "Epoch 154/200\n",
      "1164/1164 [==============================] - 0s 84us/step - loss: 0.4438 - acc: 0.8308 - val_loss: 0.4367 - val_acc: 0.8325\n",
      "Epoch 155/200\n",
      "1164/1164 [==============================] - 0s 70us/step - loss: 0.4482 - acc: 0.8316 - val_loss: 0.4355 - val_acc: 0.8462\n",
      "Epoch 156/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4439 - acc: 0.8402 - val_loss: 0.4474 - val_acc: 0.8162\n",
      "Epoch 157/200\n",
      "1164/1164 [==============================] - 0s 72us/step - loss: 0.4423 - acc: 0.8351 - val_loss: 0.4297 - val_acc: 0.8368\n",
      "Epoch 158/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4407 - acc: 0.8299 - val_loss: 0.4363 - val_acc: 0.8479\n",
      "Epoch 159/200\n",
      "1164/1164 [==============================] - 0s 94us/step - loss: 0.4431 - acc: 0.8333 - val_loss: 0.4261 - val_acc: 0.8402\n",
      "Epoch 160/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.4396 - acc: 0.8316 - val_loss: 0.4348 - val_acc: 0.8127\n",
      "Epoch 161/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.4359 - acc: 0.8265 - val_loss: 0.4317 - val_acc: 0.8479\n",
      "Epoch 162/200\n",
      "1164/1164 [==============================] - 0s 73us/step - loss: 0.4390 - acc: 0.8368 - val_loss: 0.4204 - val_acc: 0.8557\n",
      "Epoch 163/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.4305 - acc: 0.8488 - val_loss: 0.4358 - val_acc: 0.8196\n",
      "Epoch 164/200\n",
      "1164/1164 [==============================] - 0s 76us/step - loss: 0.4264 - acc: 0.8419 - val_loss: 0.4193 - val_acc: 0.8454\n",
      "Epoch 165/200\n",
      "1164/1164 [==============================] - 0s 81us/step - loss: 0.4292 - acc: 0.8462 - val_loss: 0.4320 - val_acc: 0.8273\n",
      "Epoch 166/200\n",
      "1164/1164 [==============================] - ETA: 0s - loss: 0.4258 - acc: 0.843 - 0s 89us/step - loss: 0.4249 - acc: 0.8419 - val_loss: 0.4295 - val_acc: 0.8531\n",
      "Epoch 167/200\n",
      "1164/1164 [==============================] - 0s 92us/step - loss: 0.4266 - acc: 0.8479 - val_loss: 0.4359 - val_acc: 0.8445\n",
      "Epoch 168/200\n",
      "1164/1164 [==============================] - 0s 103us/step - loss: 0.4331 - acc: 0.8368 - val_loss: 0.4571 - val_acc: 0.8239\n",
      "Epoch 169/200\n",
      "1164/1164 [==============================] - 0s 108us/step - loss: 0.4263 - acc: 0.8393 - val_loss: 0.4138 - val_acc: 0.8359\n",
      "Epoch 170/200\n",
      "1164/1164 [==============================] - 0s 151us/step - loss: 0.4288 - acc: 0.8402 - val_loss: 0.4133 - val_acc: 0.8454\n",
      "Epoch 171/200\n",
      "1164/1164 [==============================] - 0s 83us/step - loss: 0.4216 - acc: 0.8402 - val_loss: 0.4092 - val_acc: 0.8548\n",
      "Epoch 172/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.4230 - acc: 0.8393 - val_loss: 0.4176 - val_acc: 0.8368\n",
      "Epoch 173/200\n",
      "1164/1164 [==============================] - ETA: 0s - loss: 0.4501 - acc: 0.830 - 0s 70us/step - loss: 0.4194 - acc: 0.8411 - val_loss: 0.4252 - val_acc: 0.8445\n",
      "Epoch 174/200\n",
      "1164/1164 [==============================] - 0s 66us/step - loss: 0.4185 - acc: 0.8445 - val_loss: 0.4113 - val_acc: 0.8351\n",
      "Epoch 175/200\n",
      "1164/1164 [==============================] - 0s 86us/step - loss: 0.4141 - acc: 0.8505 - val_loss: 0.4052 - val_acc: 0.8557\n",
      "Epoch 176/200\n",
      "1164/1164 [==============================] - 0s 75us/step - loss: 0.4197 - acc: 0.8445 - val_loss: 0.4520 - val_acc: 0.8265\n",
      "Epoch 177/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4189 - acc: 0.8402 - val_loss: 0.4158 - val_acc: 0.8411\n",
      "Epoch 178/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.4150 - acc: 0.8479 - val_loss: 0.3987 - val_acc: 0.8677\n",
      "Epoch 179/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.4156 - acc: 0.8488 - val_loss: 0.3942 - val_acc: 0.8660\n",
      "Epoch 180/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4086 - acc: 0.8479 - val_loss: 0.3981 - val_acc: 0.8634\n",
      "Epoch 181/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4092 - acc: 0.8522 - val_loss: 0.3927 - val_acc: 0.8462\n",
      "Epoch 182/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4076 - acc: 0.8488 - val_loss: 0.4110 - val_acc: 0.8411\n",
      "Epoch 183/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.4056 - acc: 0.8471 - val_loss: 0.3953 - val_acc: 0.8471\n",
      "Epoch 184/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.4061 - acc: 0.8497 - val_loss: 0.3986 - val_acc: 0.8497\n",
      "Epoch 185/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.3993 - acc: 0.8531 - val_loss: 0.4007 - val_acc: 0.8651\n",
      "Epoch 186/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.4060 - acc: 0.8608 - val_loss: 0.3882 - val_acc: 0.8625\n",
      "Epoch 187/200\n",
      "1164/1164 [==============================] - 0s 59us/step - loss: 0.4030 - acc: 0.8488 - val_loss: 0.4078 - val_acc: 0.8402\n",
      "Epoch 188/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.4106 - acc: 0.8445 - val_loss: 0.3990 - val_acc: 0.8582\n",
      "Epoch 189/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.4043 - acc: 0.8479 - val_loss: 0.3986 - val_acc: 0.8505\n",
      "Epoch 190/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.4064 - acc: 0.8514 - val_loss: 0.3962 - val_acc: 0.8531\n",
      "Epoch 191/200\n",
      "1164/1164 [==============================] - 0s 67us/step - loss: 0.3952 - acc: 0.8514 - val_loss: 0.3819 - val_acc: 0.8522\n",
      "Epoch 192/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.3946 - acc: 0.8557 - val_loss: 0.3801 - val_acc: 0.8634\n",
      "Epoch 193/200\n",
      "1164/1164 [==============================] - 0s 68us/step - loss: 0.3884 - acc: 0.8625 - val_loss: 0.3842 - val_acc: 0.8677\n",
      "Epoch 194/200\n",
      "1164/1164 [==============================] - 0s 62us/step - loss: 0.3977 - acc: 0.8557 - val_loss: 0.3964 - val_acc: 0.8505\n",
      "Epoch 195/200\n",
      "1164/1164 [==============================] - 0s 64us/step - loss: 0.3941 - acc: 0.8600 - val_loss: 0.3807 - val_acc: 0.8557\n",
      "Epoch 196/200\n",
      "1164/1164 [==============================] - 0s 69us/step - loss: 0.3925 - acc: 0.8591 - val_loss: 0.3781 - val_acc: 0.8600\n",
      "Epoch 197/200\n",
      "1164/1164 [==============================] - 0s 63us/step - loss: 0.3904 - acc: 0.8582 - val_loss: 0.3707 - val_acc: 0.8729\n",
      "Epoch 198/200\n",
      "1164/1164 [==============================] - 0s 61us/step - loss: 0.3970 - acc: 0.8608 - val_loss: 0.3710 - val_acc: 0.8686\n",
      "Epoch 199/200\n",
      "1164/1164 [==============================] - 0s 60us/step - loss: 0.3899 - acc: 0.8488 - val_loss: 0.3742 - val_acc: 0.8720\n",
      "Epoch 200/200\n",
      "1164/1164 [==============================] - 0s 65us/step - loss: 0.3841 - acc: 0.8557 - val_loss: 0.3906 - val_acc: 0.8522\n",
      "4656/4656 [==============================] - 0s 75us/step\n",
      "[0.9579037800687286, 0.7719358531879806, 0.646549255441008]\n"
     ]
    }
   ],
   "source": [
    "ave_acc_eva_ann=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(eva_x, eva_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        model = create_model(X_train1.shape[1],y_train1.shape[1])\n",
    "        history = model.fit(X_train1, y_train1,validation_data=(X_train1,y_train1),batch_size=100,epochs=200)\n",
    "        score += model.evaluate(X_test1,y_test1,verbose=1)[1]\n",
    "    ave_acc_eva_ann.append(score/3.0)\n",
    "    \n",
    "print ave_acc_eva_ann  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28573 samples, validate on 28573 samples\n",
      "Epoch 1/300\n",
      "28573/28573 [==============================] - 3s 100us/step - loss: 3.1731 - acc: 0.0952 - val_loss: 2.8154 - val_acc: 0.1239\n",
      "Epoch 2/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 2.5572 - acc: 0.2013 - val_loss: 2.2882 - val_acc: 0.2809\n",
      "Epoch 3/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 2.0612 - acc: 0.3548 - val_loss: 1.8633 - val_acc: 0.4169\n",
      "Epoch 4/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.7130 - acc: 0.4417 - val_loss: 1.5746 - val_acc: 0.4904\n",
      "Epoch 5/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.5029 - acc: 0.4892 - val_loss: 1.5138 - val_acc: 0.4634\n",
      "Epoch 6/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 1.3622 - acc: 0.5269 - val_loss: 1.3431 - val_acc: 0.4938\n",
      "Epoch 7/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 1.2993 - acc: 0.5065 - val_loss: 1.3753 - val_acc: 0.4677\n",
      "Epoch 8/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.2499 - acc: 0.5149 - val_loss: 1.2012 - val_acc: 0.5705\n",
      "Epoch 9/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 1.1688 - acc: 0.5515 - val_loss: 1.2503 - val_acc: 0.4630\n",
      "Epoch 10/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 1.1390 - acc: 0.5423 - val_loss: 1.0475 - val_acc: 0.5857\n",
      "Epoch 11/300\n",
      "28573/28573 [==============================] - 1s 28us/step - loss: 1.0731 - acc: 0.5791 - val_loss: 1.0516 - val_acc: 0.5829\n",
      "Epoch 12/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 1.0196 - acc: 0.5871 - val_loss: 1.0884 - val_acc: 0.5320\n",
      "Epoch 13/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.9861 - acc: 0.6028 - val_loss: 0.8896 - val_acc: 0.6739\n",
      "Epoch 14/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.9304 - acc: 0.6330 - val_loss: 0.9075 - val_acc: 0.6103\n",
      "Epoch 15/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.8966 - acc: 0.6531 - val_loss: 0.8047 - val_acc: 0.7162\n",
      "Epoch 16/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.8251 - acc: 0.7015 - val_loss: 0.8130 - val_acc: 0.7336\n",
      "Epoch 17/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.7726 - acc: 0.7194 - val_loss: 0.7208 - val_acc: 0.7672\n",
      "Epoch 18/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.7130 - acc: 0.7637 - val_loss: 0.7004 - val_acc: 0.7637\n",
      "Epoch 19/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.6908 - acc: 0.7664 - val_loss: 0.6689 - val_acc: 0.7629\n",
      "Epoch 20/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.6593 - acc: 0.7861 - val_loss: 0.6862 - val_acc: 0.7564\n",
      "Epoch 21/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.6337 - acc: 0.7969 - val_loss: 0.6186 - val_acc: 0.8043\n",
      "Epoch 22/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.6138 - acc: 0.8041 - val_loss: 0.5976 - val_acc: 0.8173\n",
      "Epoch 23/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.5950 - acc: 0.8127 - val_loss: 0.5822 - val_acc: 0.8227\n",
      "Epoch 24/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.5799 - acc: 0.8179 - val_loss: 0.5709 - val_acc: 0.8348\n",
      "Epoch 25/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.5637 - acc: 0.8242 - val_loss: 0.5532 - val_acc: 0.8250\n",
      "Epoch 26/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.5488 - acc: 0.8310 - val_loss: 0.5376 - val_acc: 0.8300\n",
      "Epoch 27/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.5356 - acc: 0.8341 - val_loss: 0.5364 - val_acc: 0.8278\n",
      "Epoch 28/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.5230 - acc: 0.8391 - val_loss: 0.5131 - val_acc: 0.8389\n",
      "Epoch 29/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.5118 - acc: 0.8417 - val_loss: 0.5035 - val_acc: 0.8478\n",
      "Epoch 30/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.4998 - acc: 0.8445 - val_loss: 0.5026 - val_acc: 0.8399\n",
      "Epoch 31/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.4903 - acc: 0.8487 - val_loss: 0.4839 - val_acc: 0.8502\n",
      "Epoch 32/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.4805 - acc: 0.8492 - val_loss: 0.4795 - val_acc: 0.8415\n",
      "Epoch 33/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.4721 - acc: 0.8512 - val_loss: 0.4752 - val_acc: 0.8573\n",
      "Epoch 34/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4619 - acc: 0.8542 - val_loss: 0.4565 - val_acc: 0.8569\n",
      "Epoch 35/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.4538 - acc: 0.8556 - val_loss: 0.4463 - val_acc: 0.8580\n",
      "Epoch 36/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4455 - acc: 0.8573 - val_loss: 0.4423 - val_acc: 0.8622\n",
      "Epoch 37/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.4372 - acc: 0.8588 - val_loss: 0.4380 - val_acc: 0.8588\n",
      "Epoch 38/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.4305 - acc: 0.8584 - val_loss: 0.4285 - val_acc: 0.8549\n",
      "Epoch 39/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.4227 - acc: 0.8604 - val_loss: 0.4133 - val_acc: 0.8587\n",
      "Epoch 40/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.4140 - acc: 0.8608 - val_loss: 0.4117 - val_acc: 0.8644\n",
      "Epoch 41/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.4094 - acc: 0.8648 - val_loss: 0.4063 - val_acc: 0.8714\n",
      "Epoch 42/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.4011 - acc: 0.8644 - val_loss: 0.3966 - val_acc: 0.8633\n",
      "Epoch 43/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3950 - acc: 0.8654 - val_loss: 0.4039 - val_acc: 0.8590\n",
      "Epoch 44/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.3890 - acc: 0.8659 - val_loss: 0.3832 - val_acc: 0.8695\n",
      "Epoch 45/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3821 - acc: 0.8691 - val_loss: 0.3825 - val_acc: 0.8649\n",
      "Epoch 46/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3790 - acc: 0.8691 - val_loss: 0.3742 - val_acc: 0.8684\n",
      "Epoch 47/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3710 - acc: 0.8709 - val_loss: 0.3693 - val_acc: 0.8669\n",
      "Epoch 48/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3658 - acc: 0.8712 - val_loss: 0.3681 - val_acc: 0.8668\n",
      "Epoch 49/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.3600 - acc: 0.8748 - val_loss: 0.3617 - val_acc: 0.8809\n",
      "Epoch 50/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3557 - acc: 0.8760 - val_loss: 0.3494 - val_acc: 0.8735\n",
      "Epoch 51/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3503 - acc: 0.8778 - val_loss: 0.3484 - val_acc: 0.8818\n",
      "Epoch 52/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.3452 - acc: 0.8782 - val_loss: 0.3442 - val_acc: 0.8733\n",
      "Epoch 53/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.3415 - acc: 0.8810 - val_loss: 0.3344 - val_acc: 0.8844\n",
      "Epoch 54/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.3362 - acc: 0.8813 - val_loss: 0.3327 - val_acc: 0.8805\n",
      "Epoch 55/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.3303 - acc: 0.8862 - val_loss: 0.3306 - val_acc: 0.8843\n",
      "Epoch 56/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.3277 - acc: 0.8857 - val_loss: 0.3237 - val_acc: 0.8867\n",
      "Epoch 57/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.3243 - acc: 0.8871 - val_loss: 0.3173 - val_acc: 0.8903\n",
      "Epoch 58/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3192 - acc: 0.8887 - val_loss: 0.3166 - val_acc: 0.8855\n",
      "Epoch 59/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3158 - acc: 0.8905 - val_loss: 0.3113 - val_acc: 0.8804\n",
      "Epoch 60/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.3121 - acc: 0.8920 - val_loss: 0.3124 - val_acc: 0.8964\n",
      "Epoch 61/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3083 - acc: 0.8940 - val_loss: 0.3068 - val_acc: 0.8934\n",
      "Epoch 62/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3036 - acc: 0.8949 - val_loss: 0.2996 - val_acc: 0.8987\n",
      "Epoch 63/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3011 - acc: 0.8982 - val_loss: 0.2984 - val_acc: 0.8897\n",
      "Epoch 64/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2970 - acc: 0.8998 - val_loss: 0.2935 - val_acc: 0.9013\n",
      "Epoch 65/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.2964 - acc: 0.8985 - val_loss: 0.2938 - val_acc: 0.9000\n",
      "Epoch 66/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2905 - acc: 0.9006 - val_loss: 0.2910 - val_acc: 0.9083\n",
      "Epoch 67/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.2889 - acc: 0.9014 - val_loss: 0.2837 - val_acc: 0.8999\n",
      "Epoch 68/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2870 - acc: 0.9009 - val_loss: 0.2784 - val_acc: 0.9047\n",
      "Epoch 69/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.2823 - acc: 0.9044 - val_loss: 0.2887 - val_acc: 0.9116\n",
      "Epoch 70/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2799 - acc: 0.9044 - val_loss: 0.2756 - val_acc: 0.9015\n",
      "Epoch 71/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.2759 - acc: 0.9068 - val_loss: 0.2702 - val_acc: 0.9156\n",
      "Epoch 72/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2716 - acc: 0.9077 - val_loss: 0.2666 - val_acc: 0.9116\n",
      "Epoch 73/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2705 - acc: 0.9091 - val_loss: 0.2797 - val_acc: 0.9030\n",
      "Epoch 74/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2680 - acc: 0.9081 - val_loss: 0.2739 - val_acc: 0.9018\n",
      "Epoch 75/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2660 - acc: 0.9084 - val_loss: 0.2659 - val_acc: 0.9143\n",
      "Epoch 76/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2637 - acc: 0.9092 - val_loss: 0.2587 - val_acc: 0.9054\n",
      "Epoch 77/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2603 - acc: 0.9115 - val_loss: 0.2582 - val_acc: 0.9118\n",
      "Epoch 78/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.2584 - acc: 0.9120 - val_loss: 0.2522 - val_acc: 0.9232\n",
      "Epoch 79/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2567 - acc: 0.9132 - val_loss: 0.2574 - val_acc: 0.9068\n",
      "Epoch 80/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2523 - acc: 0.9141 - val_loss: 0.2548 - val_acc: 0.9050\n",
      "Epoch 81/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2494 - acc: 0.9156 - val_loss: 0.2565 - val_acc: 0.9009\n",
      "Epoch 82/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2494 - acc: 0.9131 - val_loss: 0.2461 - val_acc: 0.9189\n",
      "Epoch 83/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2471 - acc: 0.9163 - val_loss: 0.2460 - val_acc: 0.9098\n",
      "Epoch 84/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2428 - acc: 0.9183 - val_loss: 0.2446 - val_acc: 0.9115\n",
      "Epoch 85/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2399 - acc: 0.9185 - val_loss: 0.2416 - val_acc: 0.9165\n",
      "Epoch 86/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2394 - acc: 0.9179 - val_loss: 0.2388 - val_acc: 0.9104\n",
      "Epoch 87/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2362 - acc: 0.9197 - val_loss: 0.2525 - val_acc: 0.9092\n",
      "Epoch 88/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2365 - acc: 0.9192 - val_loss: 0.2331 - val_acc: 0.9210\n",
      "Epoch 89/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2357 - acc: 0.9189 - val_loss: 0.2441 - val_acc: 0.9132\n",
      "Epoch 90/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2308 - acc: 0.9199 - val_loss: 0.2295 - val_acc: 0.9147\n",
      "Epoch 91/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2294 - acc: 0.9200 - val_loss: 0.2279 - val_acc: 0.9123\n",
      "Epoch 92/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2290 - acc: 0.9212 - val_loss: 0.2233 - val_acc: 0.9254\n",
      "Epoch 93/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2244 - acc: 0.9234 - val_loss: 0.2257 - val_acc: 0.9223\n",
      "Epoch 94/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.2248 - acc: 0.9208 - val_loss: 0.2307 - val_acc: 0.9160\n",
      "Epoch 95/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.2227 - acc: 0.9226 - val_loss: 0.2259 - val_acc: 0.9304\n",
      "Epoch 96/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2206 - acc: 0.9235 - val_loss: 0.2150 - val_acc: 0.9352\n",
      "Epoch 97/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2193 - acc: 0.9234 - val_loss: 0.2238 - val_acc: 0.9173\n",
      "Epoch 98/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.2163 - acc: 0.9259 - val_loss: 0.2153 - val_acc: 0.9287\n",
      "Epoch 99/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.2166 - acc: 0.9241 - val_loss: 0.2139 - val_acc: 0.9388\n",
      "Epoch 100/300\n",
      "28573/28573 [==============================] - 1s 35us/step - loss: 0.2144 - acc: 0.9238 - val_loss: 0.2090 - val_acc: 0.9296\n",
      "Epoch 101/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.2123 - acc: 0.9255 - val_loss: 0.2099 - val_acc: 0.9293\n",
      "Epoch 102/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.2110 - acc: 0.9244 - val_loss: 0.2121 - val_acc: 0.9195\n",
      "Epoch 103/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2112 - acc: 0.9247 - val_loss: 0.2058 - val_acc: 0.9287\n",
      "Epoch 104/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.2055 - acc: 0.9296 - val_loss: 0.2136 - val_acc: 0.9229\n",
      "Epoch 105/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.2072 - acc: 0.9249 - val_loss: 0.1989 - val_acc: 0.9294\n",
      "Epoch 106/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2035 - acc: 0.9328 - val_loss: 0.1984 - val_acc: 0.9290\n",
      "Epoch 107/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2045 - acc: 0.9282 - val_loss: 0.2007 - val_acc: 0.9327\n",
      "Epoch 108/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2047 - acc: 0.9263 - val_loss: 0.1979 - val_acc: 0.9388\n",
      "Epoch 109/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2002 - acc: 0.9308 - val_loss: 0.2073 - val_acc: 0.9271\n",
      "Epoch 110/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1989 - acc: 0.9314 - val_loss: 0.1922 - val_acc: 0.9383\n",
      "Epoch 111/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9335 - val_loss: 0.1921 - val_acc: 0.9325\n",
      "Epoch 112/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9313 - val_loss: 0.1976 - val_acc: 0.9403\n",
      "Epoch 113/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1948 - acc: 0.9323 - val_loss: 0.1925 - val_acc: 0.9307\n",
      "Epoch 114/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.1915 - acc: 0.9356 - val_loss: 0.1923 - val_acc: 0.9346\n",
      "Epoch 115/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1932 - acc: 0.9348 - val_loss: 0.1962 - val_acc: 0.9249\n",
      "Epoch 116/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1902 - acc: 0.9338 - val_loss: 0.1885 - val_acc: 0.9408\n",
      "Epoch 117/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1904 - acc: 0.9353 - val_loss: 0.1907 - val_acc: 0.9264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1880 - acc: 0.9346 - val_loss: 0.1879 - val_acc: 0.9301\n",
      "Epoch 119/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1867 - acc: 0.9355 - val_loss: 0.1864 - val_acc: 0.9397\n",
      "Epoch 120/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.1862 - acc: 0.9365 - val_loss: 0.1848 - val_acc: 0.9349\n",
      "Epoch 121/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.1845 - acc: 0.9383 - val_loss: 0.1854 - val_acc: 0.9408\n",
      "Epoch 122/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1828 - acc: 0.9400 - val_loss: 0.1817 - val_acc: 0.9490\n",
      "Epoch 123/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1833 - acc: 0.9374 - val_loss: 0.1935 - val_acc: 0.9431\n",
      "Epoch 124/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1823 - acc: 0.9366 - val_loss: 0.1864 - val_acc: 0.9325\n",
      "Epoch 125/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.1821 - acc: 0.9364 - val_loss: 0.1774 - val_acc: 0.9370\n",
      "Epoch 126/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1790 - acc: 0.9391 - val_loss: 0.1779 - val_acc: 0.9241\n",
      "Epoch 127/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1783 - acc: 0.9390 - val_loss: 0.1988 - val_acc: 0.9259\n",
      "Epoch 128/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1797 - acc: 0.9381 - val_loss: 0.1815 - val_acc: 0.9455\n",
      "Epoch 129/300\n",
      "28573/28573 [==============================] - 1s 30us/step - loss: 0.1754 - acc: 0.9417 - val_loss: 0.1769 - val_acc: 0.9289\n",
      "Epoch 130/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1752 - acc: 0.9413 - val_loss: 0.1782 - val_acc: 0.9276\n",
      "Epoch 131/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.1735 - acc: 0.9417 - val_loss: 0.1719 - val_acc: 0.9422\n",
      "Epoch 132/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1753 - acc: 0.9392 - val_loss: 0.1693 - val_acc: 0.9448\n",
      "Epoch 133/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1711 - acc: 0.9447 - val_loss: 0.1703 - val_acc: 0.9345\n",
      "Epoch 134/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1729 - acc: 0.9405 - val_loss: 0.1692 - val_acc: 0.9413\n",
      "Epoch 135/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1703 - acc: 0.9409 - val_loss: 0.1644 - val_acc: 0.9483\n",
      "Epoch 136/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1699 - acc: 0.9428 - val_loss: 0.1736 - val_acc: 0.9332\n",
      "Epoch 137/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1700 - acc: 0.9421 - val_loss: 0.1617 - val_acc: 0.9540\n",
      "Epoch 138/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1679 - acc: 0.9436 - val_loss: 0.1769 - val_acc: 0.9279\n",
      "Epoch 139/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1660 - acc: 0.9444 - val_loss: 0.1692 - val_acc: 0.9363\n",
      "Epoch 140/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1661 - acc: 0.9437 - val_loss: 0.1765 - val_acc: 0.9321\n",
      "Epoch 141/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1642 - acc: 0.9448 - val_loss: 0.1703 - val_acc: 0.9501\n",
      "Epoch 142/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1623 - acc: 0.9483 - val_loss: 0.1679 - val_acc: 0.9279\n",
      "Epoch 143/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1628 - acc: 0.9465 - val_loss: 0.1556 - val_acc: 0.9560\n",
      "Epoch 144/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1627 - acc: 0.9460 - val_loss: 0.1683 - val_acc: 0.9510\n",
      "Epoch 145/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1604 - acc: 0.9478 - val_loss: 0.1604 - val_acc: 0.9553\n",
      "Epoch 146/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1604 - acc: 0.9460 - val_loss: 0.1722 - val_acc: 0.9248\n",
      "Epoch 147/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1602 - acc: 0.9480 - val_loss: 0.1649 - val_acc: 0.9362\n",
      "Epoch 148/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1577 - acc: 0.9493 - val_loss: 0.1754 - val_acc: 0.9253\n",
      "Epoch 149/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1586 - acc: 0.9477 - val_loss: 0.1617 - val_acc: 0.9510\n",
      "Epoch 150/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1568 - acc: 0.9491 - val_loss: 0.1518 - val_acc: 0.9577\n",
      "Epoch 151/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1584 - acc: 0.9476 - val_loss: 0.1549 - val_acc: 0.9568\n",
      "Epoch 152/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1549 - acc: 0.9496 - val_loss: 0.1493 - val_acc: 0.9607\n",
      "Epoch 153/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1526 - acc: 0.9516 - val_loss: 0.1530 - val_acc: 0.9483\n",
      "Epoch 154/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1546 - acc: 0.9493 - val_loss: 0.1489 - val_acc: 0.9495\n",
      "Epoch 155/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1521 - acc: 0.9519 - val_loss: 0.1516 - val_acc: 0.9540\n",
      "Epoch 156/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1518 - acc: 0.9496 - val_loss: 0.1559 - val_acc: 0.9398\n",
      "Epoch 157/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1557 - acc: 0.9479 - val_loss: 0.1514 - val_acc: 0.9558\n",
      "Epoch 158/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1472 - acc: 0.9551 - val_loss: 0.1589 - val_acc: 0.9315\n",
      "Epoch 159/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1494 - acc: 0.9532 - val_loss: 0.1499 - val_acc: 0.9488\n",
      "Epoch 160/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1508 - acc: 0.9489 - val_loss: 0.1502 - val_acc: 0.9560\n",
      "Epoch 161/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1456 - acc: 0.9547 - val_loss: 0.1445 - val_acc: 0.9577\n",
      "Epoch 162/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1450 - acc: 0.9561 - val_loss: 0.1451 - val_acc: 0.9528\n",
      "Epoch 163/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1478 - acc: 0.9528 - val_loss: 0.1442 - val_acc: 0.9476\n",
      "Epoch 164/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1476 - acc: 0.9514 - val_loss: 0.1411 - val_acc: 0.9555\n",
      "Epoch 165/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1480 - acc: 0.9502 - val_loss: 0.1456 - val_acc: 0.9445\n",
      "Epoch 166/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1492 - acc: 0.9495 - val_loss: 0.1446 - val_acc: 0.9511\n",
      "Epoch 167/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1444 - acc: 0.9545 - val_loss: 0.1406 - val_acc: 0.9621\n",
      "Epoch 168/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1422 - acc: 0.9559 - val_loss: 0.1427 - val_acc: 0.9481\n",
      "Epoch 169/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1404 - acc: 0.9574 - val_loss: 0.1514 - val_acc: 0.9507\n",
      "Epoch 170/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1447 - acc: 0.9524 - val_loss: 0.1414 - val_acc: 0.9615\n",
      "Epoch 171/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1391 - acc: 0.9577 - val_loss: 0.1385 - val_acc: 0.9570\n",
      "Epoch 172/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1407 - acc: 0.9549 - val_loss: 0.1509 - val_acc: 0.9554\n",
      "Epoch 173/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1424 - acc: 0.9532 - val_loss: 0.1416 - val_acc: 0.9562\n",
      "Epoch 174/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1379 - acc: 0.9583 - val_loss: 0.1414 - val_acc: 0.9547\n",
      "Epoch 175/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1393 - acc: 0.9553 - val_loss: 0.1341 - val_acc: 0.9602\n",
      "Epoch 176/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1385 - acc: 0.9572 - val_loss: 0.1374 - val_acc: 0.9515\n",
      "Epoch 177/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.1399 - acc: 0.9535 - val_loss: 0.1434 - val_acc: 0.9460\n",
      "Epoch 178/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1372 - acc: 0.9559 - val_loss: 0.1312 - val_acc: 0.9655\n",
      "Epoch 179/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1349 - acc: 0.9588 - val_loss: 0.1316 - val_acc: 0.9577\n",
      "Epoch 180/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1341 - acc: 0.9585 - val_loss: 0.1400 - val_acc: 0.9523\n",
      "Epoch 181/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1360 - acc: 0.9571 - val_loss: 0.1398 - val_acc: 0.9458\n",
      "Epoch 182/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1378 - acc: 0.9533 - val_loss: 0.1340 - val_acc: 0.9592\n",
      "Epoch 183/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1361 - acc: 0.9563 - val_loss: 0.1478 - val_acc: 0.9581\n",
      "Epoch 184/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1332 - acc: 0.9602 - val_loss: 0.1286 - val_acc: 0.9582\n",
      "Epoch 185/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1346 - acc: 0.9563 - val_loss: 0.1385 - val_acc: 0.9515\n",
      "Epoch 186/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1309 - acc: 0.9588 - val_loss: 0.1314 - val_acc: 0.9611\n",
      "Epoch 187/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1320 - acc: 0.9575 - val_loss: 0.1281 - val_acc: 0.9609\n",
      "Epoch 188/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1307 - acc: 0.9602 - val_loss: 0.1381 - val_acc: 0.9537\n",
      "Epoch 189/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1304 - acc: 0.9594 - val_loss: 0.1309 - val_acc: 0.9594\n",
      "Epoch 190/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1332 - acc: 0.9560 - val_loss: 0.1320 - val_acc: 0.9568\n",
      "Epoch 191/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1281 - acc: 0.9608 - val_loss: 0.1226 - val_acc: 0.9641\n",
      "Epoch 192/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1292 - acc: 0.9590 - val_loss: 0.1294 - val_acc: 0.9545\n",
      "Epoch 193/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1271 - acc: 0.9611 - val_loss: 0.1385 - val_acc: 0.9446\n",
      "Epoch 194/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1304 - acc: 0.9554 - val_loss: 0.1240 - val_acc: 0.9623\n",
      "Epoch 195/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1246 - acc: 0.9623 - val_loss: 0.1324 - val_acc: 0.9473\n",
      "Epoch 196/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1271 - acc: 0.9601 - val_loss: 0.1227 - val_acc: 0.9629\n",
      "Epoch 197/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1305 - acc: 0.9557 - val_loss: 0.1354 - val_acc: 0.9568\n",
      "Epoch 198/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1242 - acc: 0.9621 - val_loss: 0.1278 - val_acc: 0.9592\n",
      "Epoch 199/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1304 - acc: 0.9556 - val_loss: 0.1283 - val_acc: 0.9570\n",
      "Epoch 200/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1246 - acc: 0.9622 - val_loss: 0.1301 - val_acc: 0.9634\n",
      "Epoch 201/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1245 - acc: 0.9590 - val_loss: 0.1346 - val_acc: 0.9399\n",
      "Epoch 202/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1213 - acc: 0.9629 - val_loss: 0.1201 - val_acc: 0.9631\n",
      "Epoch 203/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1210 - acc: 0.9633 - val_loss: 0.1246 - val_acc: 0.9625\n",
      "Epoch 204/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1232 - acc: 0.9602 - val_loss: 0.1186 - val_acc: 0.9682\n",
      "Epoch 205/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1232 - acc: 0.9622 - val_loss: 0.1326 - val_acc: 0.9562\n",
      "Epoch 206/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1250 - acc: 0.9594 - val_loss: 0.1261 - val_acc: 0.9529\n",
      "Epoch 207/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1193 - acc: 0.9634 - val_loss: 0.1248 - val_acc: 0.9584\n",
      "Epoch 208/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1229 - acc: 0.9606 - val_loss: 0.1190 - val_acc: 0.9656\n",
      "Epoch 209/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1174 - acc: 0.9642 - val_loss: 0.1220 - val_acc: 0.9604\n",
      "Epoch 210/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1199 - acc: 0.9619 - val_loss: 0.1181 - val_acc: 0.9626\n",
      "Epoch 211/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1173 - acc: 0.9656 - val_loss: 0.1207 - val_acc: 0.9596\n",
      "Epoch 212/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1243 - acc: 0.9593 - val_loss: 0.1117 - val_acc: 0.9668\n",
      "Epoch 213/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1174 - acc: 0.9633 - val_loss: 0.1157 - val_acc: 0.9647\n",
      "Epoch 214/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1156 - acc: 0.9647 - val_loss: 0.1241 - val_acc: 0.9629\n",
      "Epoch 215/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1237 - acc: 0.9593 - val_loss: 0.1170 - val_acc: 0.9655\n",
      "Epoch 216/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1133 - acc: 0.9656 - val_loss: 0.1114 - val_acc: 0.9671\n",
      "Epoch 217/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1159 - acc: 0.9634 - val_loss: 0.1094 - val_acc: 0.9689\n",
      "Epoch 218/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1159 - acc: 0.9641 - val_loss: 0.1189 - val_acc: 0.9581\n",
      "Epoch 219/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1144 - acc: 0.9636 - val_loss: 0.1115 - val_acc: 0.9687\n",
      "Epoch 220/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1178 - acc: 0.9619 - val_loss: 0.1294 - val_acc: 0.9514\n",
      "Epoch 221/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1191 - acc: 0.9613 - val_loss: 0.1258 - val_acc: 0.9547\n",
      "Epoch 222/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1151 - acc: 0.9630 - val_loss: 0.1163 - val_acc: 0.9607\n",
      "Epoch 223/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1121 - acc: 0.9662 - val_loss: 0.1092 - val_acc: 0.9687\n",
      "Epoch 224/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1198 - acc: 0.9611 - val_loss: 0.1099 - val_acc: 0.9676\n",
      "Epoch 225/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1123 - acc: 0.9647 - val_loss: 0.1128 - val_acc: 0.9655\n",
      "Epoch 226/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1153 - acc: 0.9622 - val_loss: 0.1075 - val_acc: 0.9716\n",
      "Epoch 227/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1114 - acc: 0.9658 - val_loss: 0.1095 - val_acc: 0.9655\n",
      "Epoch 228/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1182 - acc: 0.9591 - val_loss: 0.1100 - val_acc: 0.9680\n",
      "Epoch 229/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1076 - acc: 0.9684 - val_loss: 0.1142 - val_acc: 0.9654\n",
      "Epoch 230/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1100 - acc: 0.9664 - val_loss: 0.1054 - val_acc: 0.9680\n",
      "Epoch 231/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1106 - acc: 0.9650 - val_loss: 0.1105 - val_acc: 0.9675\n",
      "Epoch 232/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1268 - acc: 0.9587 - val_loss: 0.1038 - val_acc: 0.9699\n",
      "Epoch 233/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1092 - acc: 0.9663 - val_loss: 0.1061 - val_acc: 0.9695\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1062 - acc: 0.9676 - val_loss: 0.1059 - val_acc: 0.9686\n",
      "Epoch 235/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1117 - acc: 0.9622 - val_loss: 0.1023 - val_acc: 0.9709\n",
      "Epoch 236/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1060 - acc: 0.9679 - val_loss: 0.1129 - val_acc: 0.9597\n",
      "Epoch 237/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1094 - acc: 0.9657 - val_loss: 0.1234 - val_acc: 0.9520\n",
      "Epoch 238/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1091 - acc: 0.9662 - val_loss: 0.1065 - val_acc: 0.9683\n",
      "Epoch 239/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1053 - acc: 0.9680 - val_loss: 0.1086 - val_acc: 0.9626\n",
      "Epoch 240/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1103 - acc: 0.9645 - val_loss: 0.1005 - val_acc: 0.9697\n",
      "Epoch 241/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1052 - acc: 0.9680 - val_loss: 0.1041 - val_acc: 0.9700\n",
      "Epoch 242/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1074 - acc: 0.9665 - val_loss: 0.1022 - val_acc: 0.9717\n",
      "Epoch 243/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1229 - acc: 0.9596 - val_loss: 0.1065 - val_acc: 0.9642\n",
      "Epoch 244/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.1040 - acc: 0.9680 - val_loss: 0.1029 - val_acc: 0.9662\n",
      "Epoch 245/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1022 - acc: 0.9699 - val_loss: 0.1038 - val_acc: 0.9696\n",
      "Epoch 246/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1064 - acc: 0.9658 - val_loss: 0.1040 - val_acc: 0.9670\n",
      "Epoch 247/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1018 - acc: 0.9697 - val_loss: 0.0996 - val_acc: 0.9720\n",
      "Epoch 248/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1055 - acc: 0.9657 - val_loss: 0.0996 - val_acc: 0.9687\n",
      "Epoch 249/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0997 - acc: 0.9693 - val_loss: 0.0997 - val_acc: 0.9671\n",
      "Epoch 250/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1259 - acc: 0.9616 - val_loss: 0.0989 - val_acc: 0.9697\n",
      "Epoch 251/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1020 - acc: 0.9688 - val_loss: 0.0980 - val_acc: 0.9698\n",
      "Epoch 252/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0992 - acc: 0.9700 - val_loss: 0.0964 - val_acc: 0.9708\n",
      "Epoch 253/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1036 - acc: 0.9675 - val_loss: 0.1000 - val_acc: 0.9664\n",
      "Epoch 254/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1015 - acc: 0.9688 - val_loss: 0.0978 - val_acc: 0.9727\n",
      "Epoch 255/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1008 - acc: 0.9690 - val_loss: 0.1070 - val_acc: 0.9658\n",
      "Epoch 256/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1014 - acc: 0.9690 - val_loss: 0.1013 - val_acc: 0.9645\n",
      "Epoch 257/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1060 - acc: 0.9670 - val_loss: 0.0958 - val_acc: 0.9684\n",
      "Epoch 258/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1006 - acc: 0.9684 - val_loss: 0.0945 - val_acc: 0.9739\n",
      "Epoch 259/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1077 - acc: 0.9647 - val_loss: 0.0956 - val_acc: 0.9684\n",
      "Epoch 260/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.0963 - acc: 0.9716 - val_loss: 0.0956 - val_acc: 0.9698\n",
      "Epoch 261/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.0996 - acc: 0.9685 - val_loss: 0.1058 - val_acc: 0.9713\n",
      "Epoch 262/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1035 - acc: 0.9666 - val_loss: 0.0963 - val_acc: 0.9764\n",
      "Epoch 263/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1005 - acc: 0.9676 - val_loss: 0.0986 - val_acc: 0.9682\n",
      "Epoch 264/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0956 - acc: 0.9717 - val_loss: 0.0935 - val_acc: 0.9719\n",
      "Epoch 265/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0996 - acc: 0.9696 - val_loss: 0.0942 - val_acc: 0.9742\n",
      "Epoch 266/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1172 - acc: 0.9605 - val_loss: 0.0980 - val_acc: 0.9642\n",
      "Epoch 267/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.0976 - acc: 0.9694 - val_loss: 0.0914 - val_acc: 0.9735\n",
      "Epoch 268/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0970 - acc: 0.9690 - val_loss: 0.1016 - val_acc: 0.9665\n",
      "Epoch 269/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0964 - acc: 0.9694 - val_loss: 0.0940 - val_acc: 0.9740\n",
      "Epoch 270/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0948 - acc: 0.9720 - val_loss: 0.0964 - val_acc: 0.9657\n",
      "Epoch 271/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0955 - acc: 0.9712 - val_loss: 0.1076 - val_acc: 0.9651\n",
      "Epoch 272/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0952 - acc: 0.9711 - val_loss: 0.0915 - val_acc: 0.9751\n",
      "Epoch 273/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0988 - acc: 0.9692 - val_loss: 0.1319 - val_acc: 0.9429\n",
      "Epoch 274/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1075 - acc: 0.9655 - val_loss: 0.0921 - val_acc: 0.9719\n",
      "Epoch 275/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0948 - acc: 0.9704 - val_loss: 0.1056 - val_acc: 0.9678\n",
      "Epoch 276/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0967 - acc: 0.9689 - val_loss: 0.0893 - val_acc: 0.9728\n",
      "Epoch 277/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0935 - acc: 0.9711 - val_loss: 0.1016 - val_acc: 0.9677\n",
      "Epoch 278/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0976 - acc: 0.9688 - val_loss: 0.0903 - val_acc: 0.9775\n",
      "Epoch 279/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0925 - acc: 0.9728 - val_loss: 0.0973 - val_acc: 0.9683\n",
      "Epoch 280/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0947 - acc: 0.9698 - val_loss: 0.0907 - val_acc: 0.9760\n",
      "Epoch 281/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0944 - acc: 0.9691 - val_loss: 0.0901 - val_acc: 0.9704\n",
      "Epoch 282/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0910 - acc: 0.9725 - val_loss: 0.0879 - val_acc: 0.9749\n",
      "Epoch 283/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1046 - acc: 0.9667 - val_loss: 0.0913 - val_acc: 0.9728\n",
      "Epoch 284/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.0915 - acc: 0.9722 - val_loss: 0.0978 - val_acc: 0.9679\n",
      "Epoch 285/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0933 - acc: 0.9699 - val_loss: 0.0865 - val_acc: 0.9748\n",
      "Epoch 286/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0909 - acc: 0.9720 - val_loss: 0.0947 - val_acc: 0.9686\n",
      "Epoch 287/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0951 - acc: 0.9711 - val_loss: 0.0901 - val_acc: 0.9741\n",
      "Epoch 288/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.0924 - acc: 0.9721 - val_loss: 0.0863 - val_acc: 0.9776\n",
      "Epoch 289/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0903 - acc: 0.9719 - val_loss: 0.0866 - val_acc: 0.9755\n",
      "Epoch 290/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0940 - acc: 0.9700 - val_loss: 0.0875 - val_acc: 0.9720\n",
      "Epoch 291/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0904 - acc: 0.9715 - val_loss: 0.0854 - val_acc: 0.9739\n",
      "Epoch 292/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.0921 - acc: 0.9714 - val_loss: 0.1093 - val_acc: 0.9693\n",
      "Epoch 293/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0960 - acc: 0.9696 - val_loss: 0.1055 - val_acc: 0.9703\n",
      "Epoch 294/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1326 - acc: 0.9610 - val_loss: 0.0947 - val_acc: 0.9606\n",
      "Epoch 295/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0896 - acc: 0.9715 - val_loss: 0.0829 - val_acc: 0.9779\n",
      "Epoch 296/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0872 - acc: 0.9733 - val_loss: 0.1079 - val_acc: 0.9591\n",
      "Epoch 297/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0885 - acc: 0.9736 - val_loss: 0.0866 - val_acc: 0.9717\n",
      "Epoch 298/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0915 - acc: 0.9706 - val_loss: 0.0839 - val_acc: 0.9795\n",
      "Epoch 299/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0893 - acc: 0.9715 - val_loss: 0.0847 - val_acc: 0.9731\n",
      "Epoch 300/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.0874 - acc: 0.9723 - val_loss: 0.0843 - val_acc: 0.9786\n",
      "7144/7144 [==============================] - 1s 103us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28573 samples, validate on 28573 samples\n",
      "Epoch 1/300\n",
      "28573/28573 [==============================] - 3s 114us/step - loss: 3.1221 - acc: 0.0843 - val_loss: 2.7955 - val_acc: 0.0840\n",
      "Epoch 2/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 2.5636 - acc: 0.1849 - val_loss: 2.3158 - val_acc: 0.2593\n",
      "Epoch 3/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 2.0829 - acc: 0.3425 - val_loss: 1.8628 - val_acc: 0.4042\n",
      "Epoch 4/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 1.7122 - acc: 0.4568 - val_loss: 1.5711 - val_acc: 0.4982\n",
      "Epoch 5/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 1.4709 - acc: 0.5340 - val_loss: 1.3794 - val_acc: 0.5064\n",
      "Epoch 6/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 1.3134 - acc: 0.5730 - val_loss: 1.2610 - val_acc: 0.5828\n",
      "Epoch 7/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 1.1941 - acc: 0.6092 - val_loss: 1.1468 - val_acc: 0.6021\n",
      "Epoch 8/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.1184 - acc: 0.6130 - val_loss: 1.1328 - val_acc: 0.5725\n",
      "Epoch 9/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.0625 - acc: 0.6284 - val_loss: 1.0131 - val_acc: 0.6510\n",
      "Epoch 10/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.9792 - acc: 0.6658 - val_loss: 0.9674 - val_acc: 0.6480\n",
      "Epoch 11/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.9329 - acc: 0.6827 - val_loss: 0.9554 - val_acc: 0.6676\n",
      "Epoch 12/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.9129 - acc: 0.6620 - val_loss: 0.9090 - val_acc: 0.6438\n",
      "Epoch 13/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.8700 - acc: 0.6780 - val_loss: 0.8141 - val_acc: 0.7168\n",
      "Epoch 14/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.8339 - acc: 0.6971 - val_loss: 0.8192 - val_acc: 0.6590\n",
      "Epoch 15/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.7909 - acc: 0.7226 - val_loss: 0.7611 - val_acc: 0.7621\n",
      "Epoch 16/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.7462 - acc: 0.7590 - val_loss: 0.7192 - val_acc: 0.7947\n",
      "Epoch 17/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.7198 - acc: 0.7729 - val_loss: 0.6764 - val_acc: 0.8105\n",
      "Epoch 18/300\n",
      "28573/28573 [==============================] - 1s 30us/step - loss: 0.6760 - acc: 0.7999 - val_loss: 0.6780 - val_acc: 0.7675\n",
      "Epoch 19/300\n",
      "28573/28573 [==============================] - 1s 36us/step - loss: 0.6520 - acc: 0.8042 - val_loss: 0.6528 - val_acc: 0.8083\n",
      "Epoch 20/300\n",
      "28573/28573 [==============================] - 1s 39us/step - loss: 0.6281 - acc: 0.8125 - val_loss: 0.6122 - val_acc: 0.8168\n",
      "Epoch 21/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.6084 - acc: 0.8198 - val_loss: 0.6024 - val_acc: 0.8236\n",
      "Epoch 22/300\n",
      "28573/28573 [==============================] - 1s 41us/step - loss: 0.5934 - acc: 0.8240 - val_loss: 0.5874 - val_acc: 0.8214\n",
      "Epoch 23/300\n",
      "28573/28573 [==============================] - 1s 37us/step - loss: 0.5745 - acc: 0.8259 - val_loss: 0.5687 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "28573/28573 [==============================] - 1s 37us/step - loss: 0.5591 - acc: 0.8293 - val_loss: 0.5508 - val_acc: 0.8401\n",
      "Epoch 25/300\n",
      "28573/28573 [==============================] - 1s 30us/step - loss: 0.5464 - acc: 0.8332 - val_loss: 0.5388 - val_acc: 0.8315\n",
      "Epoch 26/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.5314 - acc: 0.8385 - val_loss: 0.5266 - val_acc: 0.8472\n",
      "Epoch 27/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.5206 - acc: 0.8375 - val_loss: 0.5267 - val_acc: 0.8266\n",
      "Epoch 28/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.5087 - acc: 0.8421 - val_loss: 0.5039 - val_acc: 0.8387\n",
      "Epoch 29/300\n",
      "28573/28573 [==============================] - 1s 28us/step - loss: 0.4968 - acc: 0.8447 - val_loss: 0.4903 - val_acc: 0.8373\n",
      "Epoch 30/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.4874 - acc: 0.8463 - val_loss: 0.4847 - val_acc: 0.8422\n",
      "Epoch 31/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4779 - acc: 0.8501 - val_loss: 0.4714 - val_acc: 0.8486\n",
      "Epoch 32/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.4670 - acc: 0.8535 - val_loss: 0.4662 - val_acc: 0.8603\n",
      "Epoch 33/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.4589 - acc: 0.8531 - val_loss: 0.4585 - val_acc: 0.8471\n",
      "Epoch 34/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.4505 - acc: 0.8581 - val_loss: 0.4596 - val_acc: 0.8462\n",
      "Epoch 35/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.4441 - acc: 0.8559 - val_loss: 0.4414 - val_acc: 0.8659\n",
      "Epoch 36/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.4354 - acc: 0.8628 - val_loss: 0.4301 - val_acc: 0.8599\n",
      "Epoch 37/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.4272 - acc: 0.8628 - val_loss: 0.4253 - val_acc: 0.8648\n",
      "Epoch 38/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.4203 - acc: 0.8640 - val_loss: 0.4240 - val_acc: 0.8633\n",
      "Epoch 39/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.4123 - acc: 0.8655 - val_loss: 0.4098 - val_acc: 0.8656\n",
      "Epoch 40/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.4079 - acc: 0.8669 - val_loss: 0.4061 - val_acc: 0.8633\n",
      "Epoch 41/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.4006 - acc: 0.8676 - val_loss: 0.3997 - val_acc: 0.8695\n",
      "Epoch 42/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.3950 - acc: 0.8714 - val_loss: 0.3910 - val_acc: 0.8695\n",
      "Epoch 43/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.3888 - acc: 0.8726 - val_loss: 0.3904 - val_acc: 0.8564\n",
      "Epoch 44/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.3831 - acc: 0.8723 - val_loss: 0.3875 - val_acc: 0.8772\n",
      "Epoch 45/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3778 - acc: 0.8750 - val_loss: 0.3756 - val_acc: 0.8695\n",
      "Epoch 46/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.3720 - acc: 0.8765 - val_loss: 0.3739 - val_acc: 0.8900\n",
      "Epoch 47/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3667 - acc: 0.8789 - val_loss: 0.3713 - val_acc: 0.8701\n",
      "Epoch 48/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3623 - acc: 0.8795 - val_loss: 0.3561 - val_acc: 0.8792\n",
      "Epoch 49/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3586 - acc: 0.8797 - val_loss: 0.3515 - val_acc: 0.8807\n",
      "Epoch 50/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3533 - acc: 0.8829 - val_loss: 0.3501 - val_acc: 0.8816\n",
      "Epoch 51/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.3487 - acc: 0.8835 - val_loss: 0.3558 - val_acc: 0.8740\n",
      "Epoch 52/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3454 - acc: 0.8849 - val_loss: 0.3414 - val_acc: 0.8837\n",
      "Epoch 53/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3411 - acc: 0.8843 - val_loss: 0.3432 - val_acc: 0.8766\n",
      "Epoch 54/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.3374 - acc: 0.8876 - val_loss: 0.3340 - val_acc: 0.8916\n",
      "Epoch 55/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3320 - acc: 0.8866 - val_loss: 0.3328 - val_acc: 0.8905\n",
      "Epoch 56/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3288 - acc: 0.8902 - val_loss: 0.3280 - val_acc: 0.8904\n",
      "Epoch 57/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3249 - acc: 0.8910 - val_loss: 0.3309 - val_acc: 0.8926\n",
      "Epoch 58/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3229 - acc: 0.8918 - val_loss: 0.3190 - val_acc: 0.8878\n",
      "Epoch 59/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.3178 - acc: 0.8939 - val_loss: 0.3270 - val_acc: 0.9005\n",
      "Epoch 60/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.3153 - acc: 0.8958 - val_loss: 0.3131 - val_acc: 0.8892\n",
      "Epoch 61/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3117 - acc: 0.8977 - val_loss: 0.3103 - val_acc: 0.8881\n",
      "Epoch 62/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.3072 - acc: 0.8991 - val_loss: 0.3028 - val_acc: 0.8978\n",
      "Epoch 63/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.3044 - acc: 0.8993 - val_loss: 0.3019 - val_acc: 0.8939\n",
      "Epoch 64/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3031 - acc: 0.8980 - val_loss: 0.2999 - val_acc: 0.9052\n",
      "Epoch 65/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3002 - acc: 0.8965 - val_loss: 0.3020 - val_acc: 0.8954\n",
      "Epoch 66/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.2963 - acc: 0.9004 - val_loss: 0.2977 - val_acc: 0.8843\n",
      "Epoch 67/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.2933 - acc: 0.9009 - val_loss: 0.2935 - val_acc: 0.8967\n",
      "Epoch 68/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2912 - acc: 0.9029 - val_loss: 0.2865 - val_acc: 0.9034\n",
      "Epoch 69/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2885 - acc: 0.9018 - val_loss: 0.2858 - val_acc: 0.8996\n",
      "Epoch 70/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2851 - acc: 0.9030 - val_loss: 0.2864 - val_acc: 0.9087\n",
      "Epoch 71/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2829 - acc: 0.9042 - val_loss: 0.2857 - val_acc: 0.9108\n",
      "Epoch 72/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.2788 - acc: 0.9054 - val_loss: 0.2743 - val_acc: 0.9039\n",
      "Epoch 73/300\n",
      "28573/28573 [==============================] - ETA: 0s - loss: 0.2756 - acc: 0.906 - 1s 18us/step - loss: 0.2774 - acc: 0.9061 - val_loss: 0.2733 - val_acc: 0.9024\n",
      "Epoch 74/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.2744 - acc: 0.9080 - val_loss: 0.2748 - val_acc: 0.9186\n",
      "Epoch 75/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2725 - acc: 0.9074 - val_loss: 0.2673 - val_acc: 0.9154\n",
      "Epoch 76/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.2705 - acc: 0.9081 - val_loss: 0.2857 - val_acc: 0.9099\n",
      "Epoch 77/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.2685 - acc: 0.9082 - val_loss: 0.2665 - val_acc: 0.9196\n",
      "Epoch 78/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2673 - acc: 0.9101 - val_loss: 0.2630 - val_acc: 0.9143\n",
      "Epoch 79/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2646 - acc: 0.9108 - val_loss: 0.2640 - val_acc: 0.9200\n",
      "Epoch 80/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2627 - acc: 0.9119 - val_loss: 0.2584 - val_acc: 0.9080\n",
      "Epoch 81/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2604 - acc: 0.9116 - val_loss: 0.2589 - val_acc: 0.9136\n",
      "Epoch 82/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2580 - acc: 0.9124 - val_loss: 0.2641 - val_acc: 0.9067\n",
      "Epoch 83/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2560 - acc: 0.9128 - val_loss: 0.2536 - val_acc: 0.9178\n",
      "Epoch 84/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.2525 - acc: 0.9154 - val_loss: 0.2560 - val_acc: 0.9030\n",
      "Epoch 85/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.2512 - acc: 0.9130 - val_loss: 0.2530 - val_acc: 0.9025\n",
      "Epoch 86/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2521 - acc: 0.9109 - val_loss: 0.2523 - val_acc: 0.9090\n",
      "Epoch 87/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2482 - acc: 0.9147 - val_loss: 0.2656 - val_acc: 0.8955\n",
      "Epoch 88/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.2468 - acc: 0.9146 - val_loss: 0.2409 - val_acc: 0.9206\n",
      "Epoch 89/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2458 - acc: 0.9175 - val_loss: 0.2432 - val_acc: 0.9167\n",
      "Epoch 90/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.2424 - acc: 0.9165 - val_loss: 0.2403 - val_acc: 0.9084\n",
      "Epoch 91/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.2395 - acc: 0.9175 - val_loss: 0.2400 - val_acc: 0.9042\n",
      "Epoch 92/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.2400 - acc: 0.9169 - val_loss: 0.2325 - val_acc: 0.9271\n",
      "Epoch 93/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.2364 - acc: 0.9177 - val_loss: 0.2353 - val_acc: 0.9145\n",
      "Epoch 94/300\n",
      "28573/28573 [==============================] - 1s 36us/step - loss: 0.2366 - acc: 0.9155 - val_loss: 0.2285 - val_acc: 0.9211\n",
      "Epoch 95/300\n",
      "28573/28573 [==============================] - 1s 33us/step - loss: 0.2332 - acc: 0.9201 - val_loss: 0.2314 - val_acc: 0.9223\n",
      "Epoch 96/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.2303 - acc: 0.9210 - val_loss: 0.2319 - val_acc: 0.9204\n",
      "Epoch 97/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.2316 - acc: 0.9196 - val_loss: 0.2366 - val_acc: 0.9287\n",
      "Epoch 98/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.2284 - acc: 0.9227 - val_loss: 0.2279 - val_acc: 0.9190\n",
      "Epoch 99/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.2284 - acc: 0.9189 - val_loss: 0.2341 - val_acc: 0.9121\n",
      "Epoch 100/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.2255 - acc: 0.9206 - val_loss: 0.2259 - val_acc: 0.9212\n",
      "Epoch 101/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.2237 - acc: 0.9238 - val_loss: 0.2239 - val_acc: 0.9180\n",
      "Epoch 102/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.2232 - acc: 0.9202 - val_loss: 0.2202 - val_acc: 0.9229\n",
      "Epoch 103/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.2242 - acc: 0.9211 - val_loss: 0.2320 - val_acc: 0.9065\n",
      "Epoch 104/300\n",
      "28573/28573 [==============================] - 1s 28us/step - loss: 0.2178 - acc: 0.9248 - val_loss: 0.2213 - val_acc: 0.9300\n",
      "Epoch 105/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.2200 - acc: 0.9247 - val_loss: 0.2160 - val_acc: 0.9266\n",
      "Epoch 106/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.2169 - acc: 0.9240 - val_loss: 0.2125 - val_acc: 0.9265\n",
      "Epoch 107/300\n",
      "28573/28573 [==============================] - 1s 32us/step - loss: 0.2169 - acc: 0.9232 - val_loss: 0.2153 - val_acc: 0.9248\n",
      "Epoch 108/300\n",
      "28573/28573 [==============================] - 1s 34us/step - loss: 0.2171 - acc: 0.9214 - val_loss: 0.2256 - val_acc: 0.9021\n",
      "Epoch 109/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.2127 - acc: 0.9264 - val_loss: 0.2094 - val_acc: 0.9266\n",
      "Epoch 110/300\n",
      "28573/28573 [==============================] - 1s 43us/step - loss: 0.2126 - acc: 0.9243 - val_loss: 0.2239 - val_acc: 0.9130\n",
      "Epoch 111/300\n",
      "28573/28573 [==============================] - 1s 30us/step - loss: 0.2106 - acc: 0.9257 - val_loss: 0.2140 - val_acc: 0.9285\n",
      "Epoch 112/300\n",
      "28573/28573 [==============================] - 1s 44us/step - loss: 0.2115 - acc: 0.9259 - val_loss: 0.2058 - val_acc: 0.9327\n",
      "Epoch 113/300\n",
      "28573/28573 [==============================] - 1s 43us/step - loss: 0.2087 - acc: 0.9259 - val_loss: 0.2067 - val_acc: 0.9291\n",
      "Epoch 114/300\n",
      "28573/28573 [==============================] - 1s 42us/step - loss: 0.2055 - acc: 0.9276 - val_loss: 0.2076 - val_acc: 0.9219\n",
      "Epoch 115/300\n",
      "28573/28573 [==============================] - 1s 35us/step - loss: 0.2086 - acc: 0.9235 - val_loss: 0.2130 - val_acc: 0.9215\n",
      "Epoch 116/300\n",
      "28573/28573 [==============================] - 1s 39us/step - loss: 0.2036 - acc: 0.9283 - val_loss: 0.1965 - val_acc: 0.9392\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28573/28573 [==============================] - 1s 37us/step - loss: 0.2033 - acc: 0.9283 - val_loss: 0.1974 - val_acc: 0.9331\n",
      "Epoch 118/300\n",
      "28573/28573 [==============================] - 1s 36us/step - loss: 0.2040 - acc: 0.9240 - val_loss: 0.2127 - val_acc: 0.9179\n",
      "Epoch 119/300\n",
      "28573/28573 [==============================] - 1s 30us/step - loss: 0.2025 - acc: 0.9287 - val_loss: 0.1977 - val_acc: 0.9273\n",
      "Epoch 120/300\n",
      "28573/28573 [==============================] - 1s 31us/step - loss: 0.1987 - acc: 0.9312 - val_loss: 0.1933 - val_acc: 0.9343\n",
      "Epoch 121/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.2008 - acc: 0.9271 - val_loss: 0.1983 - val_acc: 0.9500\n",
      "Epoch 122/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1996 - acc: 0.9274 - val_loss: 0.1980 - val_acc: 0.9316\n",
      "Epoch 123/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1959 - acc: 0.9316 - val_loss: 0.1970 - val_acc: 0.9167\n",
      "Epoch 124/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1947 - acc: 0.9306 - val_loss: 0.1928 - val_acc: 0.9349\n",
      "Epoch 125/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1925 - acc: 0.9334 - val_loss: 0.1883 - val_acc: 0.9325\n",
      "Epoch 126/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1947 - acc: 0.9306 - val_loss: 0.1931 - val_acc: 0.9304\n",
      "Epoch 127/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1941 - acc: 0.9310 - val_loss: 0.1988 - val_acc: 0.9253\n",
      "Epoch 128/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.1916 - acc: 0.9325 - val_loss: 0.1831 - val_acc: 0.9345\n",
      "Epoch 129/300\n",
      "28573/28573 [==============================] - 1s 34us/step - loss: 0.1889 - acc: 0.9341 - val_loss: 0.1883 - val_acc: 0.9404\n",
      "Epoch 130/300\n",
      "28573/28573 [==============================] - 1s 33us/step - loss: 0.1897 - acc: 0.9341 - val_loss: 0.1844 - val_acc: 0.9429\n",
      "Epoch 131/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.1904 - acc: 0.9327 - val_loss: 0.1829 - val_acc: 0.9378\n",
      "Epoch 132/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1865 - acc: 0.9335 - val_loss: 0.1812 - val_acc: 0.9302\n",
      "Epoch 133/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1882 - acc: 0.9320 - val_loss: 0.1829 - val_acc: 0.9357\n",
      "Epoch 134/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1856 - acc: 0.9343 - val_loss: 0.1877 - val_acc: 0.9253\n",
      "Epoch 135/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1846 - acc: 0.9346 - val_loss: 0.1792 - val_acc: 0.9372\n",
      "Epoch 136/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1841 - acc: 0.9350 - val_loss: 0.2018 - val_acc: 0.9152\n",
      "Epoch 137/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1845 - acc: 0.9343 - val_loss: 0.1781 - val_acc: 0.9416\n",
      "Epoch 138/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1813 - acc: 0.9365 - val_loss: 0.1874 - val_acc: 0.9449\n",
      "Epoch 139/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1794 - acc: 0.9392 - val_loss: 0.1861 - val_acc: 0.9362\n",
      "Epoch 140/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1863 - acc: 0.9322 - val_loss: 0.1765 - val_acc: 0.9346\n",
      "Epoch 141/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1785 - acc: 0.9372 - val_loss: 0.1760 - val_acc: 0.9374\n",
      "Epoch 142/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1767 - acc: 0.9385 - val_loss: 0.1778 - val_acc: 0.9318\n",
      "Epoch 143/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1773 - acc: 0.9370 - val_loss: 0.1741 - val_acc: 0.9382\n",
      "Epoch 144/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1769 - acc: 0.9389 - val_loss: 0.1796 - val_acc: 0.9252\n",
      "Epoch 145/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1747 - acc: 0.9396 - val_loss: 0.1778 - val_acc: 0.9361\n",
      "Epoch 146/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1739 - acc: 0.9392 - val_loss: 0.1700 - val_acc: 0.9352\n",
      "Epoch 147/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1731 - acc: 0.9400 - val_loss: 0.1717 - val_acc: 0.9330\n",
      "Epoch 148/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1729 - acc: 0.9386 - val_loss: 0.1731 - val_acc: 0.9469\n",
      "Epoch 149/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1726 - acc: 0.9393 - val_loss: 0.1694 - val_acc: 0.9350\n",
      "Epoch 150/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1719 - acc: 0.9392 - val_loss: 0.1666 - val_acc: 0.9505\n",
      "Epoch 151/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1693 - acc: 0.9420 - val_loss: 0.1641 - val_acc: 0.9406\n",
      "Epoch 152/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1701 - acc: 0.9407 - val_loss: 0.1735 - val_acc: 0.9487\n",
      "Epoch 153/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1690 - acc: 0.9398 - val_loss: 0.1639 - val_acc: 0.9391\n",
      "Epoch 154/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1685 - acc: 0.9400 - val_loss: 0.1740 - val_acc: 0.9395\n",
      "Epoch 155/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1672 - acc: 0.9400 - val_loss: 0.1671 - val_acc: 0.9494\n",
      "Epoch 156/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1662 - acc: 0.9423 - val_loss: 0.1629 - val_acc: 0.9529\n",
      "Epoch 157/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1654 - acc: 0.9423 - val_loss: 0.1853 - val_acc: 0.9345\n",
      "Epoch 158/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1649 - acc: 0.9425 - val_loss: 0.1599 - val_acc: 0.9477\n",
      "Epoch 159/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1639 - acc: 0.9428 - val_loss: 0.1586 - val_acc: 0.9408\n",
      "Epoch 160/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1619 - acc: 0.9434 - val_loss: 0.1623 - val_acc: 0.9554\n",
      "Epoch 161/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1609 - acc: 0.9448 - val_loss: 0.1650 - val_acc: 0.9448\n",
      "Epoch 162/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1649 - acc: 0.9418 - val_loss: 0.1560 - val_acc: 0.9458\n",
      "Epoch 163/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1624 - acc: 0.9423 - val_loss: 0.1764 - val_acc: 0.9448\n",
      "Epoch 164/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1602 - acc: 0.9451 - val_loss: 0.1623 - val_acc: 0.9295\n",
      "Epoch 165/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1588 - acc: 0.9454 - val_loss: 0.1598 - val_acc: 0.9565\n",
      "Epoch 166/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1631 - acc: 0.9387 - val_loss: 0.1637 - val_acc: 0.9437\n",
      "Epoch 167/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1594 - acc: 0.9448 - val_loss: 0.1622 - val_acc: 0.9442\n",
      "Epoch 168/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1609 - acc: 0.9433 - val_loss: 0.1561 - val_acc: 0.9446\n",
      "Epoch 169/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1545 - acc: 0.9485 - val_loss: 0.1570 - val_acc: 0.9377\n",
      "Epoch 170/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1555 - acc: 0.9469 - val_loss: 0.1501 - val_acc: 0.9508\n",
      "Epoch 171/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1575 - acc: 0.9460 - val_loss: 0.1521 - val_acc: 0.9523\n",
      "Epoch 172/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1567 - acc: 0.9458 - val_loss: 0.1578 - val_acc: 0.9366\n",
      "Epoch 173/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1538 - acc: 0.9475 - val_loss: 0.1479 - val_acc: 0.9419\n",
      "Epoch 174/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1534 - acc: 0.9475 - val_loss: 0.1499 - val_acc: 0.9545\n",
      "Epoch 175/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1513 - acc: 0.9495 - val_loss: 0.1476 - val_acc: 0.9505\n",
      "Epoch 176/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1503 - acc: 0.9500 - val_loss: 0.1431 - val_acc: 0.9610\n",
      "Epoch 177/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1505 - acc: 0.9499 - val_loss: 0.1484 - val_acc: 0.9516\n",
      "Epoch 178/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1549 - acc: 0.9481 - val_loss: 0.1438 - val_acc: 0.9606\n",
      "Epoch 179/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1498 - acc: 0.9492 - val_loss: 0.1669 - val_acc: 0.9353\n",
      "Epoch 180/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1495 - acc: 0.9500 - val_loss: 0.1445 - val_acc: 0.9432\n",
      "Epoch 181/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1526 - acc: 0.9459 - val_loss: 0.1605 - val_acc: 0.9342\n",
      "Epoch 182/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1490 - acc: 0.9499 - val_loss: 0.1584 - val_acc: 0.9448\n",
      "Epoch 183/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1483 - acc: 0.9492 - val_loss: 0.1455 - val_acc: 0.9544\n",
      "Epoch 184/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1440 - acc: 0.9532 - val_loss: 0.1494 - val_acc: 0.9545\n",
      "Epoch 185/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1439 - acc: 0.9534 - val_loss: 0.1537 - val_acc: 0.9474\n",
      "Epoch 186/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1440 - acc: 0.9540 - val_loss: 0.1499 - val_acc: 0.9460\n",
      "Epoch 187/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1457 - acc: 0.9512 - val_loss: 0.1503 - val_acc: 0.9532\n",
      "Epoch 188/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1475 - acc: 0.9496 - val_loss: 0.1449 - val_acc: 0.9623\n",
      "Epoch 189/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1393 - acc: 0.9561 - val_loss: 0.1506 - val_acc: 0.9607\n",
      "Epoch 190/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1458 - acc: 0.9498 - val_loss: 0.1565 - val_acc: 0.9407\n",
      "Epoch 191/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1461 - acc: 0.9502 - val_loss: 0.1340 - val_acc: 0.9637\n",
      "Epoch 192/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1413 - acc: 0.9530 - val_loss: 0.1379 - val_acc: 0.9567\n",
      "Epoch 193/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1401 - acc: 0.9546 - val_loss: 0.1335 - val_acc: 0.9647\n",
      "Epoch 194/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1411 - acc: 0.9530 - val_loss: 0.1361 - val_acc: 0.9579\n",
      "Epoch 195/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1375 - acc: 0.9554 - val_loss: 0.1321 - val_acc: 0.9656\n",
      "Epoch 196/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1384 - acc: 0.9540 - val_loss: 0.1370 - val_acc: 0.9531\n",
      "Epoch 197/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1604 - acc: 0.9481 - val_loss: 0.1337 - val_acc: 0.9516\n",
      "Epoch 198/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1347 - acc: 0.9573 - val_loss: 0.1395 - val_acc: 0.9483\n",
      "Epoch 199/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1423 - acc: 0.9522 - val_loss: 0.1390 - val_acc: 0.9467\n",
      "Epoch 200/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1351 - acc: 0.9573 - val_loss: 0.1330 - val_acc: 0.9509\n",
      "Epoch 201/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1360 - acc: 0.9564 - val_loss: 0.1321 - val_acc: 0.9596\n",
      "Epoch 202/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1335 - acc: 0.9570 - val_loss: 0.1353 - val_acc: 0.9520\n",
      "Epoch 203/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1370 - acc: 0.9544 - val_loss: 0.1456 - val_acc: 0.9524\n",
      "Epoch 204/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1316 - acc: 0.9597 - val_loss: 0.1357 - val_acc: 0.9660\n",
      "Epoch 205/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1326 - acc: 0.9582 - val_loss: 0.1270 - val_acc: 0.9570\n",
      "Epoch 206/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1344 - acc: 0.9563 - val_loss: 0.1324 - val_acc: 0.9633\n",
      "Epoch 207/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1363 - acc: 0.9547 - val_loss: 0.1456 - val_acc: 0.9528\n",
      "Epoch 208/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1372 - acc: 0.9534 - val_loss: 0.1289 - val_acc: 0.9639\n",
      "Epoch 209/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1308 - acc: 0.9579 - val_loss: 0.1636 - val_acc: 0.9287\n",
      "Epoch 210/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1324 - acc: 0.9580 - val_loss: 0.1258 - val_acc: 0.9575\n",
      "Epoch 211/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1275 - acc: 0.9614 - val_loss: 0.1421 - val_acc: 0.9585\n",
      "Epoch 212/300\n",
      "28573/28573 [==============================] - 1s 33us/step - loss: 0.1305 - acc: 0.9592 - val_loss: 0.1324 - val_acc: 0.9561\n",
      "Epoch 213/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.1292 - acc: 0.9601 - val_loss: 0.1291 - val_acc: 0.9505\n",
      "Epoch 214/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1280 - acc: 0.9585 - val_loss: 0.1235 - val_acc: 0.9577\n",
      "Epoch 215/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1327 - acc: 0.9570 - val_loss: 0.1269 - val_acc: 0.9517\n",
      "Epoch 216/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1372 - acc: 0.9545 - val_loss: 0.1218 - val_acc: 0.9712\n",
      "Epoch 217/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1262 - acc: 0.9598 - val_loss: 0.1215 - val_acc: 0.9615\n",
      "Epoch 218/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1259 - acc: 0.9604 - val_loss: 0.1267 - val_acc: 0.9501\n",
      "Epoch 219/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1254 - acc: 0.9606 - val_loss: 0.1564 - val_acc: 0.9305\n",
      "Epoch 220/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1270 - acc: 0.9595 - val_loss: 0.1242 - val_acc: 0.9572\n",
      "Epoch 221/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1212 - acc: 0.9639 - val_loss: 0.1228 - val_acc: 0.9609\n",
      "Epoch 222/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1268 - acc: 0.9591 - val_loss: 0.1172 - val_acc: 0.9609\n",
      "Epoch 223/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1212 - acc: 0.9643 - val_loss: 0.1266 - val_acc: 0.9596\n",
      "Epoch 224/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1229 - acc: 0.9626 - val_loss: 0.1163 - val_acc: 0.9606\n",
      "Epoch 225/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1279 - acc: 0.9567 - val_loss: 0.1232 - val_acc: 0.9600\n",
      "Epoch 226/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1251 - acc: 0.9623 - val_loss: 0.1365 - val_acc: 0.9483\n",
      "Epoch 227/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1200 - acc: 0.9621 - val_loss: 0.1273 - val_acc: 0.9463\n",
      "Epoch 228/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1226 - acc: 0.9617 - val_loss: 0.1178 - val_acc: 0.9693\n",
      "Epoch 229/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1201 - acc: 0.9645 - val_loss: 0.1263 - val_acc: 0.9651\n",
      "Epoch 230/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1272 - acc: 0.9576 - val_loss: 0.1162 - val_acc: 0.9655\n",
      "Epoch 231/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1202 - acc: 0.9642 - val_loss: 0.1308 - val_acc: 0.9431\n",
      "Epoch 232/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1189 - acc: 0.9634 - val_loss: 0.1132 - val_acc: 0.9733\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1194 - acc: 0.9636 - val_loss: 0.1295 - val_acc: 0.9463\n",
      "Epoch 234/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1196 - acc: 0.9621 - val_loss: 0.1175 - val_acc: 0.9679\n",
      "Epoch 235/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1189 - acc: 0.9642 - val_loss: 0.1263 - val_acc: 0.9649\n",
      "Epoch 236/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1166 - acc: 0.9650 - val_loss: 0.1218 - val_acc: 0.9667\n",
      "Epoch 237/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1181 - acc: 0.9634 - val_loss: 0.1150 - val_acc: 0.9732\n",
      "Epoch 238/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1173 - acc: 0.9638 - val_loss: 0.1116 - val_acc: 0.9741\n",
      "Epoch 239/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1160 - acc: 0.9661 - val_loss: 0.1153 - val_acc: 0.9636\n",
      "Epoch 240/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1212 - acc: 0.9597 - val_loss: 0.1128 - val_acc: 0.9718\n",
      "Epoch 241/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1209 - acc: 0.9603 - val_loss: 0.1110 - val_acc: 0.9721\n",
      "Epoch 242/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1116 - acc: 0.9672 - val_loss: 0.1086 - val_acc: 0.9612\n",
      "Epoch 243/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1131 - acc: 0.9669 - val_loss: 0.1096 - val_acc: 0.9724\n",
      "Epoch 244/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1301 - acc: 0.9599 - val_loss: 0.1109 - val_acc: 0.9719\n",
      "Epoch 245/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1117 - acc: 0.9665 - val_loss: 0.1266 - val_acc: 0.9516\n",
      "Epoch 246/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1145 - acc: 0.9652 - val_loss: 0.1126 - val_acc: 0.9631\n",
      "Epoch 247/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1133 - acc: 0.9660 - val_loss: 0.1134 - val_acc: 0.9635\n",
      "Epoch 248/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1260 - acc: 0.9595 - val_loss: 0.1071 - val_acc: 0.9737\n",
      "Epoch 249/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1101 - acc: 0.9673 - val_loss: 0.1067 - val_acc: 0.9744\n",
      "Epoch 250/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1128 - acc: 0.9669 - val_loss: 0.1071 - val_acc: 0.9648\n",
      "Epoch 251/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1106 - acc: 0.9675 - val_loss: 0.1086 - val_acc: 0.9657\n",
      "Epoch 252/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1242 - acc: 0.9614 - val_loss: 0.1074 - val_acc: 0.9654\n",
      "Epoch 253/300\n",
      "28573/28573 [==============================] - ETA: 0s - loss: 0.1084 - acc: 0.967 - 1s 19us/step - loss: 0.1082 - acc: 0.9677 - val_loss: 0.1132 - val_acc: 0.9680\n",
      "Epoch 254/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1119 - acc: 0.9647 - val_loss: 0.1054 - val_acc: 0.9719\n",
      "Epoch 255/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1082 - acc: 0.9693 - val_loss: 0.1136 - val_acc: 0.9675\n",
      "Epoch 256/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1110 - acc: 0.9671 - val_loss: 0.1010 - val_acc: 0.9756\n",
      "Epoch 257/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1105 - acc: 0.9676 - val_loss: 0.1069 - val_acc: 0.9706\n",
      "Epoch 258/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1097 - acc: 0.9666 - val_loss: 0.1024 - val_acc: 0.9706\n",
      "Epoch 259/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1165 - acc: 0.9643 - val_loss: 0.1035 - val_acc: 0.9730\n",
      "Epoch 260/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1050 - acc: 0.9706 - val_loss: 0.1048 - val_acc: 0.9715\n",
      "Epoch 261/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1111 - acc: 0.9653 - val_loss: 0.1541 - val_acc: 0.9283\n",
      "Epoch 262/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1102 - acc: 0.9662 - val_loss: 0.1045 - val_acc: 0.9666\n",
      "Epoch 263/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1042 - acc: 0.9689 - val_loss: 0.0973 - val_acc: 0.9745\n",
      "Epoch 264/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1087 - acc: 0.9670 - val_loss: 0.1031 - val_acc: 0.9747\n",
      "Epoch 265/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1092 - acc: 0.9656 - val_loss: 0.1025 - val_acc: 0.9762\n",
      "Epoch 266/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1104 - acc: 0.9653 - val_loss: 0.1188 - val_acc: 0.9558\n",
      "Epoch 267/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1026 - acc: 0.9710 - val_loss: 0.1168 - val_acc: 0.9641\n",
      "Epoch 268/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1052 - acc: 0.9681 - val_loss: 0.1022 - val_acc: 0.9741\n",
      "Epoch 269/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1043 - acc: 0.9688 - val_loss: 0.1013 - val_acc: 0.9742\n",
      "Epoch 270/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1042 - acc: 0.9695 - val_loss: 0.1053 - val_acc: 0.9694\n",
      "Epoch 271/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1047 - acc: 0.9700 - val_loss: 0.1060 - val_acc: 0.9580\n",
      "Epoch 272/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1046 - acc: 0.9685 - val_loss: 0.0988 - val_acc: 0.9723\n",
      "Epoch 273/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1252 - acc: 0.9605 - val_loss: 0.1011 - val_acc: 0.9732\n",
      "Epoch 274/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1038 - acc: 0.9679 - val_loss: 0.1010 - val_acc: 0.9631\n",
      "Epoch 275/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0976 - acc: 0.9745 - val_loss: 0.0987 - val_acc: 0.9675\n",
      "Epoch 276/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1058 - acc: 0.9663 - val_loss: 0.0997 - val_acc: 0.9702\n",
      "Epoch 277/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1020 - acc: 0.9689 - val_loss: 0.1034 - val_acc: 0.9698\n",
      "Epoch 278/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1059 - acc: 0.9665 - val_loss: 0.1101 - val_acc: 0.9699\n",
      "Epoch 279/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1030 - acc: 0.9690 - val_loss: 0.0967 - val_acc: 0.9755\n",
      "Epoch 280/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0969 - acc: 0.9732 - val_loss: 0.0961 - val_acc: 0.9735\n",
      "Epoch 281/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1082 - acc: 0.9684 - val_loss: 0.1109 - val_acc: 0.9603\n",
      "Epoch 282/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0975 - acc: 0.9733 - val_loss: 0.1156 - val_acc: 0.9477\n",
      "Epoch 283/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1034 - acc: 0.9686 - val_loss: 0.0923 - val_acc: 0.9763\n",
      "Epoch 284/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0972 - acc: 0.9724 - val_loss: 0.0952 - val_acc: 0.9735\n",
      "Epoch 285/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0995 - acc: 0.9693 - val_loss: 0.0983 - val_acc: 0.9679\n",
      "Epoch 286/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0962 - acc: 0.9741 - val_loss: 0.0960 - val_acc: 0.9721\n",
      "Epoch 287/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1029 - acc: 0.9672 - val_loss: 0.0940 - val_acc: 0.9767\n",
      "Epoch 288/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0955 - acc: 0.9745 - val_loss: 0.0970 - val_acc: 0.9721\n",
      "Epoch 289/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0974 - acc: 0.9712 - val_loss: 0.1125 - val_acc: 0.9631\n",
      "Epoch 290/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1016 - acc: 0.9695 - val_loss: 0.1262 - val_acc: 0.9491\n",
      "Epoch 291/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1124 - acc: 0.9630 - val_loss: 0.0933 - val_acc: 0.9719\n",
      "Epoch 292/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0924 - acc: 0.9756 - val_loss: 0.0958 - val_acc: 0.9711\n",
      "Epoch 293/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0974 - acc: 0.9706 - val_loss: 0.1175 - val_acc: 0.9605\n",
      "Epoch 294/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0946 - acc: 0.9735 - val_loss: 0.0941 - val_acc: 0.9768\n",
      "Epoch 295/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1001 - acc: 0.9696 - val_loss: 0.0985 - val_acc: 0.9643\n",
      "Epoch 296/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1110 - acc: 0.9668 - val_loss: 0.0963 - val_acc: 0.9706\n",
      "Epoch 297/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.0959 - acc: 0.9727 - val_loss: 0.0969 - val_acc: 0.9694\n",
      "Epoch 298/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.0992 - acc: 0.9690 - val_loss: 0.0992 - val_acc: 0.9625\n",
      "Epoch 299/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0931 - acc: 0.9741 - val_loss: 0.1597 - val_acc: 0.9525\n",
      "Epoch 300/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0926 - acc: 0.9746 - val_loss: 0.0890 - val_acc: 0.9812\n",
      "7144/7144 [==============================] - 1s 98us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28573 samples, validate on 28573 samples\n",
      "Epoch 1/300\n",
      "28573/28573 [==============================] - 3s 108us/step - loss: 3.2004 - acc: 0.0662 - val_loss: 2.8654 - val_acc: 0.0877\n",
      "Epoch 2/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 2.5936 - acc: 0.1565 - val_loss: 2.3139 - val_acc: 0.2661\n",
      "Epoch 3/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 2.0623 - acc: 0.3341 - val_loss: 1.8358 - val_acc: 0.3707\n",
      "Epoch 4/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.6745 - acc: 0.4602 - val_loss: 1.5469 - val_acc: 0.4354\n",
      "Epoch 5/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.4907 - acc: 0.4681 - val_loss: 1.3764 - val_acc: 0.4993\n",
      "Epoch 6/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.3857 - acc: 0.4780 - val_loss: 1.3565 - val_acc: 0.4770\n",
      "Epoch 7/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.3023 - acc: 0.4923 - val_loss: 1.2308 - val_acc: 0.5211\n",
      "Epoch 8/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 1.2583 - acc: 0.4942 - val_loss: 1.1356 - val_acc: 0.5711\n",
      "Epoch 9/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 1.2155 - acc: 0.5107 - val_loss: 1.1824 - val_acc: 0.5249\n",
      "Epoch 10/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 1.1138 - acc: 0.5627 - val_loss: 1.1605 - val_acc: 0.5459\n",
      "Epoch 11/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.1055 - acc: 0.5680 - val_loss: 0.9756 - val_acc: 0.6334\n",
      "Epoch 12/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.0555 - acc: 0.5818 - val_loss: 0.9462 - val_acc: 0.6360\n",
      "Epoch 13/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 1.0036 - acc: 0.6069 - val_loss: 0.9097 - val_acc: 0.6483\n",
      "Epoch 14/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.9686 - acc: 0.6102 - val_loss: 0.8610 - val_acc: 0.6896\n",
      "Epoch 15/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.9594 - acc: 0.6133 - val_loss: 0.9206 - val_acc: 0.6145\n",
      "Epoch 16/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.9151 - acc: 0.6284 - val_loss: 0.8132 - val_acc: 0.6764\n",
      "Epoch 17/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.8737 - acc: 0.6532 - val_loss: 0.9791 - val_acc: 0.6044\n",
      "Epoch 18/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.8203 - acc: 0.6829 - val_loss: 0.8283 - val_acc: 0.6638\n",
      "Epoch 19/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.8032 - acc: 0.6903 - val_loss: 0.6996 - val_acc: 0.7666\n",
      "Epoch 20/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.6798 - acc: 0.7809 - val_loss: 0.6685 - val_acc: 0.7667\n",
      "Epoch 21/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.6534 - acc: 0.7868 - val_loss: 0.6352 - val_acc: 0.8176\n",
      "Epoch 22/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.6302 - acc: 0.7979 - val_loss: 0.6171 - val_acc: 0.8185\n",
      "Epoch 23/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.6098 - acc: 0.8067 - val_loss: 0.6003 - val_acc: 0.8331\n",
      "Epoch 24/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.5944 - acc: 0.8105 - val_loss: 0.5871 - val_acc: 0.8101\n",
      "Epoch 25/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.5768 - acc: 0.8134 - val_loss: 0.5678 - val_acc: 0.8061\n",
      "Epoch 26/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.5627 - acc: 0.8206 - val_loss: 0.5562 - val_acc: 0.8281\n",
      "Epoch 27/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.5506 - acc: 0.8245 - val_loss: 0.5497 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.5369 - acc: 0.8257 - val_loss: 0.5301 - val_acc: 0.8463\n",
      "Epoch 29/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.5270 - acc: 0.8294 - val_loss: 0.5166 - val_acc: 0.8381\n",
      "Epoch 30/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.5165 - acc: 0.8350 - val_loss: 0.5133 - val_acc: 0.8278\n",
      "Epoch 31/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.5077 - acc: 0.8360 - val_loss: 0.4982 - val_acc: 0.8466\n",
      "Epoch 32/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4974 - acc: 0.8421 - val_loss: 0.4898 - val_acc: 0.8442\n",
      "Epoch 33/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4876 - acc: 0.8435 - val_loss: 0.4868 - val_acc: 0.8435\n",
      "Epoch 34/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4800 - acc: 0.8460 - val_loss: 0.4761 - val_acc: 0.8274\n",
      "Epoch 35/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.4709 - acc: 0.8467 - val_loss: 0.4698 - val_acc: 0.8479\n",
      "Epoch 36/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.4620 - acc: 0.8481 - val_loss: 0.4602 - val_acc: 0.8568\n",
      "Epoch 37/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4552 - acc: 0.8541 - val_loss: 0.4472 - val_acc: 0.8606\n",
      "Epoch 38/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4482 - acc: 0.8540 - val_loss: 0.4444 - val_acc: 0.8555\n",
      "Epoch 39/300\n",
      "28573/28573 [==============================] - ETA: 0s - loss: 0.4421 - acc: 0.858 - 0s 17us/step - loss: 0.4407 - acc: 0.8570 - val_loss: 0.4378 - val_acc: 0.8598\n",
      "Epoch 40/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4339 - acc: 0.8600 - val_loss: 0.4287 - val_acc: 0.8727\n",
      "Epoch 41/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.4248 - acc: 0.8634 - val_loss: 0.4183 - val_acc: 0.8635\n",
      "Epoch 42/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.4209 - acc: 0.8629 - val_loss: 0.4217 - val_acc: 0.8746\n",
      "Epoch 43/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4124 - acc: 0.8689 - val_loss: 0.4146 - val_acc: 0.8634\n",
      "Epoch 44/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.4067 - acc: 0.8684 - val_loss: 0.4097 - val_acc: 0.8577\n",
      "Epoch 45/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.4010 - acc: 0.8704 - val_loss: 0.3972 - val_acc: 0.8753\n",
      "Epoch 46/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3936 - acc: 0.8741 - val_loss: 0.3906 - val_acc: 0.8765\n",
      "Epoch 47/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3885 - acc: 0.8757 - val_loss: 0.3855 - val_acc: 0.8669\n",
      "Epoch 48/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3823 - acc: 0.8777 - val_loss: 0.3767 - val_acc: 0.8780\n",
      "Epoch 49/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3774 - acc: 0.8802 - val_loss: 0.3730 - val_acc: 0.8805\n",
      "Epoch 50/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3722 - acc: 0.8817 - val_loss: 0.3732 - val_acc: 0.8796\n",
      "Epoch 51/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3661 - acc: 0.8843 - val_loss: 0.3609 - val_acc: 0.8842\n",
      "Epoch 52/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3626 - acc: 0.8836 - val_loss: 0.3589 - val_acc: 0.8945\n",
      "Epoch 53/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3565 - acc: 0.8866 - val_loss: 0.3557 - val_acc: 0.8711\n",
      "Epoch 54/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.3507 - acc: 0.8881 - val_loss: 0.3579 - val_acc: 0.8773\n",
      "Epoch 55/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.3482 - acc: 0.8872 - val_loss: 0.3447 - val_acc: 0.8801\n",
      "Epoch 56/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3434 - acc: 0.8902 - val_loss: 0.3434 - val_acc: 0.8844\n",
      "Epoch 57/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8920 - val_loss: 0.3502 - val_acc: 0.8763\n",
      "Epoch 58/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3344 - acc: 0.8924 - val_loss: 0.3350 - val_acc: 0.8824\n",
      "Epoch 59/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3302 - acc: 0.8934 - val_loss: 0.3289 - val_acc: 0.8885\n",
      "Epoch 60/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3243 - acc: 0.8972 - val_loss: 0.3204 - val_acc: 0.8992\n",
      "Epoch 61/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3217 - acc: 0.8965 - val_loss: 0.3221 - val_acc: 0.8962\n",
      "Epoch 62/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.3174 - acc: 0.8990 - val_loss: 0.3116 - val_acc: 0.8994\n",
      "Epoch 63/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3132 - acc: 0.8982 - val_loss: 0.3117 - val_acc: 0.8950\n",
      "Epoch 64/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3093 - acc: 0.9006 - val_loss: 0.3168 - val_acc: 0.8856\n",
      "Epoch 65/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3063 - acc: 0.9008 - val_loss: 0.3012 - val_acc: 0.9102\n",
      "Epoch 66/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.9016 - val_loss: 0.3020 - val_acc: 0.9141\n",
      "Epoch 67/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2979 - acc: 0.9053 - val_loss: 0.2949 - val_acc: 0.9013\n",
      "Epoch 68/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2957 - acc: 0.9039 - val_loss: 0.2970 - val_acc: 0.8889\n",
      "Epoch 69/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2913 - acc: 0.9057 - val_loss: 0.2977 - val_acc: 0.9122\n",
      "Epoch 70/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2892 - acc: 0.9062 - val_loss: 0.2835 - val_acc: 0.9189\n",
      "Epoch 71/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2868 - acc: 0.9077 - val_loss: 0.2892 - val_acc: 0.9047\n",
      "Epoch 72/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2834 - acc: 0.9077 - val_loss: 0.2789 - val_acc: 0.9200\n",
      "Epoch 73/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.2788 - acc: 0.9095 - val_loss: 0.2850 - val_acc: 0.9102\n",
      "Epoch 74/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2770 - acc: 0.9104 - val_loss: 0.2817 - val_acc: 0.9073\n",
      "Epoch 75/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2730 - acc: 0.9121 - val_loss: 0.2663 - val_acc: 0.9147\n",
      "Epoch 76/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2698 - acc: 0.9120 - val_loss: 0.2793 - val_acc: 0.9138\n",
      "Epoch 77/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2681 - acc: 0.9130 - val_loss: 0.2706 - val_acc: 0.9097\n",
      "Epoch 78/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2671 - acc: 0.9125 - val_loss: 0.2664 - val_acc: 0.8991\n",
      "Epoch 79/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2629 - acc: 0.9143 - val_loss: 0.2650 - val_acc: 0.9202\n",
      "Epoch 80/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2614 - acc: 0.9150 - val_loss: 0.2662 - val_acc: 0.9059\n",
      "Epoch 81/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2584 - acc: 0.9173 - val_loss: 0.2599 - val_acc: 0.9098\n",
      "Epoch 82/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2558 - acc: 0.9169 - val_loss: 0.2578 - val_acc: 0.9074\n",
      "Epoch 83/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2548 - acc: 0.9165 - val_loss: 0.2525 - val_acc: 0.9020\n",
      "Epoch 84/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2506 - acc: 0.9191 - val_loss: 0.2489 - val_acc: 0.9188\n",
      "Epoch 85/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2494 - acc: 0.9190 - val_loss: 0.2487 - val_acc: 0.9202\n",
      "Epoch 86/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2461 - acc: 0.9201 - val_loss: 0.2425 - val_acc: 0.9207\n",
      "Epoch 87/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2460 - acc: 0.9187 - val_loss: 0.2465 - val_acc: 0.9172\n",
      "Epoch 88/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2422 - acc: 0.9222 - val_loss: 0.2755 - val_acc: 0.9123\n",
      "Epoch 89/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2424 - acc: 0.9215 - val_loss: 0.2411 - val_acc: 0.9089\n",
      "Epoch 90/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2394 - acc: 0.9197 - val_loss: 0.2325 - val_acc: 0.9258\n",
      "Epoch 91/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.2362 - acc: 0.9242 - val_loss: 0.2368 - val_acc: 0.9304\n",
      "Epoch 92/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.2327 - acc: 0.9263 - val_loss: 0.2299 - val_acc: 0.9358\n",
      "Epoch 93/300\n",
      "28573/28573 [==============================] - 1s 27us/step - loss: 0.2312 - acc: 0.9284 - val_loss: 0.2357 - val_acc: 0.9158\n",
      "Epoch 94/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.2306 - acc: 0.9285 - val_loss: 0.2345 - val_acc: 0.9266\n",
      "Epoch 95/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.2261 - acc: 0.9277 - val_loss: 0.2327 - val_acc: 0.9288\n",
      "Epoch 96/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2276 - acc: 0.9262 - val_loss: 0.2210 - val_acc: 0.9386\n",
      "Epoch 97/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2241 - acc: 0.9295 - val_loss: 0.2201 - val_acc: 0.9330\n",
      "Epoch 98/300\n",
      "28573/28573 [==============================] - 0s 12us/step - loss: 0.2233 - acc: 0.9286 - val_loss: 0.2225 - val_acc: 0.9411\n",
      "Epoch 99/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2198 - acc: 0.9307 - val_loss: 0.2208 - val_acc: 0.9292\n",
      "Epoch 100/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2182 - acc: 0.9325 - val_loss: 0.2212 - val_acc: 0.9359\n",
      "Epoch 101/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.2168 - acc: 0.9310 - val_loss: 0.2146 - val_acc: 0.9359\n",
      "Epoch 102/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2153 - acc: 0.9339 - val_loss: 0.2109 - val_acc: 0.9303\n",
      "Epoch 103/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2130 - acc: 0.9350 - val_loss: 0.2135 - val_acc: 0.9252\n",
      "Epoch 104/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2108 - acc: 0.9345 - val_loss: 0.2056 - val_acc: 0.9445\n",
      "Epoch 105/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2094 - acc: 0.9350 - val_loss: 0.2102 - val_acc: 0.9343\n",
      "Epoch 106/300\n",
      "28573/28573 [==============================] - 0s 12us/step - loss: 0.2096 - acc: 0.9335 - val_loss: 0.2068 - val_acc: 0.9321\n",
      "Epoch 107/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.2069 - acc: 0.9337 - val_loss: 0.2019 - val_acc: 0.9464\n",
      "Epoch 108/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2050 - acc: 0.9378 - val_loss: 0.2150 - val_acc: 0.9161\n",
      "Epoch 109/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2031 - acc: 0.9368 - val_loss: 0.2063 - val_acc: 0.9340\n",
      "Epoch 110/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.2022 - acc: 0.9370 - val_loss: 0.2156 - val_acc: 0.9307\n",
      "Epoch 111/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.2026 - acc: 0.9357 - val_loss: 0.2022 - val_acc: 0.9322\n",
      "Epoch 112/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.1984 - acc: 0.9396 - val_loss: 0.1984 - val_acc: 0.9393\n",
      "Epoch 113/300\n",
      "28573/28573 [==============================] - 0s 13us/step - loss: 0.1965 - acc: 0.9401 - val_loss: 0.1929 - val_acc: 0.9476\n",
      "Epoch 114/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.1974 - acc: 0.9382 - val_loss: 0.1898 - val_acc: 0.9436\n",
      "Epoch 115/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1941 - acc: 0.9407 - val_loss: 0.1970 - val_acc: 0.9382\n",
      "Epoch 116/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1941 - acc: 0.9398 - val_loss: 0.1961 - val_acc: 0.9316\n",
      "Epoch 117/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1913 - acc: 0.9427 - val_loss: 0.1930 - val_acc: 0.9391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1895 - acc: 0.9434 - val_loss: 0.1887 - val_acc: 0.9407\n",
      "Epoch 119/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1882 - acc: 0.9433 - val_loss: 0.1856 - val_acc: 0.9463\n",
      "Epoch 120/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1904 - acc: 0.9400 - val_loss: 0.1858 - val_acc: 0.9512\n",
      "Epoch 121/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1862 - acc: 0.9436 - val_loss: 0.1830 - val_acc: 0.9418\n",
      "Epoch 122/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1862 - acc: 0.9433 - val_loss: 0.1830 - val_acc: 0.9476\n",
      "Epoch 123/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1842 - acc: 0.9454 - val_loss: 0.1835 - val_acc: 0.9461\n",
      "Epoch 124/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1830 - acc: 0.9447 - val_loss: 0.1771 - val_acc: 0.9527\n",
      "Epoch 125/300\n",
      "28573/28573 [==============================] - 0s 14us/step - loss: 0.1806 - acc: 0.9455 - val_loss: 0.1802 - val_acc: 0.9444\n",
      "Epoch 126/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1810 - acc: 0.9459 - val_loss: 0.1712 - val_acc: 0.9512\n",
      "Epoch 127/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1779 - acc: 0.9464 - val_loss: 0.1721 - val_acc: 0.9467\n",
      "Epoch 128/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1773 - acc: 0.9475 - val_loss: 0.1780 - val_acc: 0.9371\n",
      "Epoch 129/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1744 - acc: 0.9486 - val_loss: 0.1719 - val_acc: 0.9531\n",
      "Epoch 130/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1763 - acc: 0.9468 - val_loss: 0.1716 - val_acc: 0.9519\n",
      "Epoch 131/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1750 - acc: 0.9469 - val_loss: 0.1698 - val_acc: 0.9503\n",
      "Epoch 132/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1719 - acc: 0.9496 - val_loss: 0.1645 - val_acc: 0.9532\n",
      "Epoch 133/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1734 - acc: 0.9454 - val_loss: 0.1805 - val_acc: 0.9496\n",
      "Epoch 134/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1698 - acc: 0.9493 - val_loss: 0.1661 - val_acc: 0.9522\n",
      "Epoch 135/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1686 - acc: 0.9494 - val_loss: 0.1781 - val_acc: 0.9353\n",
      "Epoch 136/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1655 - acc: 0.9516 - val_loss: 0.1621 - val_acc: 0.9525\n",
      "Epoch 137/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1681 - acc: 0.9486 - val_loss: 0.1895 - val_acc: 0.9328\n",
      "Epoch 138/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1642 - acc: 0.9518 - val_loss: 0.1580 - val_acc: 0.9536\n",
      "Epoch 139/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1648 - acc: 0.9501 - val_loss: 0.1706 - val_acc: 0.9544\n",
      "Epoch 140/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1615 - acc: 0.9527 - val_loss: 0.1591 - val_acc: 0.9521\n",
      "Epoch 141/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1631 - acc: 0.9515 - val_loss: 0.1569 - val_acc: 0.9558\n",
      "Epoch 142/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1593 - acc: 0.9540 - val_loss: 0.1668 - val_acc: 0.9432\n",
      "Epoch 143/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1601 - acc: 0.9521 - val_loss: 0.1588 - val_acc: 0.9505\n",
      "Epoch 144/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1669 - acc: 0.9454 - val_loss: 0.1569 - val_acc: 0.9540\n",
      "Epoch 145/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1561 - acc: 0.9547 - val_loss: 0.1609 - val_acc: 0.9608\n",
      "Epoch 146/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1600 - acc: 0.9511 - val_loss: 0.1624 - val_acc: 0.9538\n",
      "Epoch 147/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1571 - acc: 0.9533 - val_loss: 0.1494 - val_acc: 0.9571\n",
      "Epoch 148/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1550 - acc: 0.9545 - val_loss: 0.1781 - val_acc: 0.9277\n",
      "Epoch 149/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1558 - acc: 0.9536 - val_loss: 0.1508 - val_acc: 0.9624\n",
      "Epoch 150/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1540 - acc: 0.9543 - val_loss: 0.1641 - val_acc: 0.9511\n",
      "Epoch 151/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1557 - acc: 0.9524 - val_loss: 0.1482 - val_acc: 0.9606\n",
      "Epoch 152/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1499 - acc: 0.9572 - val_loss: 0.1560 - val_acc: 0.9518\n",
      "Epoch 153/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1498 - acc: 0.9549 - val_loss: 0.1756 - val_acc: 0.9201\n",
      "Epoch 154/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1488 - acc: 0.9559 - val_loss: 0.1497 - val_acc: 0.9565\n",
      "Epoch 155/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1495 - acc: 0.9559 - val_loss: 0.1560 - val_acc: 0.9501\n",
      "Epoch 156/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1500 - acc: 0.9536 - val_loss: 0.1428 - val_acc: 0.9605\n",
      "Epoch 157/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1468 - acc: 0.9564 - val_loss: 0.1465 - val_acc: 0.9590\n",
      "Epoch 158/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1454 - acc: 0.9581 - val_loss: 0.1472 - val_acc: 0.9623\n",
      "Epoch 159/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1452 - acc: 0.9569 - val_loss: 0.1393 - val_acc: 0.9612\n",
      "Epoch 160/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1452 - acc: 0.9572 - val_loss: 0.1429 - val_acc: 0.9614\n",
      "Epoch 161/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1491 - acc: 0.9533 - val_loss: 0.1397 - val_acc: 0.9610\n",
      "Epoch 162/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1394 - acc: 0.9603 - val_loss: 0.1444 - val_acc: 0.9595\n",
      "Epoch 163/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1438 - acc: 0.9570 - val_loss: 0.1566 - val_acc: 0.9470\n",
      "Epoch 164/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1414 - acc: 0.9589 - val_loss: 0.1348 - val_acc: 0.9611\n",
      "Epoch 165/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1414 - acc: 0.9575 - val_loss: 0.1410 - val_acc: 0.9551\n",
      "Epoch 166/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1390 - acc: 0.9592 - val_loss: 0.1369 - val_acc: 0.9615\n",
      "Epoch 167/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1379 - acc: 0.9592 - val_loss: 0.1400 - val_acc: 0.9563\n",
      "Epoch 168/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1429 - acc: 0.9547 - val_loss: 0.1446 - val_acc: 0.9557\n",
      "Epoch 169/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1370 - acc: 0.9601 - val_loss: 0.1394 - val_acc: 0.9578\n",
      "Epoch 170/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1401 - acc: 0.9571 - val_loss: 0.1308 - val_acc: 0.9630\n",
      "Epoch 171/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1380 - acc: 0.9576 - val_loss: 0.1330 - val_acc: 0.9657\n",
      "Epoch 172/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1358 - acc: 0.9587 - val_loss: 0.1339 - val_acc: 0.9599\n",
      "Epoch 173/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1340 - acc: 0.9603 - val_loss: 0.1315 - val_acc: 0.9645\n",
      "Epoch 174/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1363 - acc: 0.9596 - val_loss: 0.1434 - val_acc: 0.9565\n",
      "Epoch 175/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1363 - acc: 0.9585 - val_loss: 0.1313 - val_acc: 0.9575\n",
      "Epoch 176/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1304 - acc: 0.9621 - val_loss: 0.1272 - val_acc: 0.9640\n",
      "Epoch 177/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1318 - acc: 0.9617 - val_loss: 0.1392 - val_acc: 0.9536\n",
      "Epoch 178/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1371 - acc: 0.9579 - val_loss: 0.1383 - val_acc: 0.9626\n",
      "Epoch 179/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1268 - acc: 0.9630 - val_loss: 0.1266 - val_acc: 0.9677\n",
      "Epoch 180/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1284 - acc: 0.9626 - val_loss: 0.1283 - val_acc: 0.9623\n",
      "Epoch 181/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1298 - acc: 0.9610 - val_loss: 0.1255 - val_acc: 0.9622\n",
      "Epoch 182/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1280 - acc: 0.9621 - val_loss: 0.1561 - val_acc: 0.9420\n",
      "Epoch 183/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1300 - acc: 0.9603 - val_loss: 0.1386 - val_acc: 0.9563\n",
      "Epoch 184/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1284 - acc: 0.9612 - val_loss: 0.1345 - val_acc: 0.9592\n",
      "Epoch 185/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1280 - acc: 0.9610 - val_loss: 0.1225 - val_acc: 0.9656\n",
      "Epoch 186/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1265 - acc: 0.9622 - val_loss: 0.1266 - val_acc: 0.9567\n",
      "Epoch 187/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.1233 - acc: 0.9632 - val_loss: 0.1259 - val_acc: 0.9617\n",
      "Epoch 188/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1219 - acc: 0.9654 - val_loss: 0.1190 - val_acc: 0.9674\n",
      "Epoch 189/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1238 - acc: 0.9636 - val_loss: 0.1178 - val_acc: 0.9634\n",
      "Epoch 190/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1218 - acc: 0.9639 - val_loss: 0.1245 - val_acc: 0.9639\n",
      "Epoch 191/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1197 - acc: 0.9655 - val_loss: 0.1228 - val_acc: 0.9648\n",
      "Epoch 192/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1225 - acc: 0.9628 - val_loss: 0.1148 - val_acc: 0.9692\n",
      "Epoch 193/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1228 - acc: 0.9630 - val_loss: 0.1261 - val_acc: 0.9643\n",
      "Epoch 194/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1213 - acc: 0.9633 - val_loss: 0.1165 - val_acc: 0.9656\n",
      "Epoch 195/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1216 - acc: 0.9624 - val_loss: 0.1275 - val_acc: 0.9637\n",
      "Epoch 196/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1188 - acc: 0.9648 - val_loss: 0.1114 - val_acc: 0.9695\n",
      "Epoch 197/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1166 - acc: 0.9654 - val_loss: 0.1193 - val_acc: 0.9611\n",
      "Epoch 198/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1297 - acc: 0.9569 - val_loss: 0.1856 - val_acc: 0.9099\n",
      "Epoch 199/300\n",
      "28573/28573 [==============================] - 1s 28us/step - loss: 0.1205 - acc: 0.9631 - val_loss: 0.1159 - val_acc: 0.9626\n",
      "Epoch 200/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1141 - acc: 0.9663 - val_loss: 0.1143 - val_acc: 0.9628\n",
      "Epoch 201/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1177 - acc: 0.9641 - val_loss: 0.1127 - val_acc: 0.9661\n",
      "Epoch 202/300\n",
      "28573/28573 [==============================] - 1s 35us/step - loss: 0.1157 - acc: 0.9666 - val_loss: 0.1221 - val_acc: 0.9646\n",
      "Epoch 203/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1171 - acc: 0.9638 - val_loss: 0.1189 - val_acc: 0.9580\n",
      "Epoch 204/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.1128 - acc: 0.9663 - val_loss: 0.1239 - val_acc: 0.9580\n",
      "Epoch 205/300\n",
      "28573/28573 [==============================] - 0s 15us/step - loss: 0.1155 - acc: 0.9655 - val_loss: 0.1150 - val_acc: 0.9622\n",
      "Epoch 206/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1156 - acc: 0.9647 - val_loss: 0.1105 - val_acc: 0.9697\n",
      "Epoch 207/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1128 - acc: 0.9662 - val_loss: 0.1139 - val_acc: 0.9629\n",
      "Epoch 208/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1109 - acc: 0.9667 - val_loss: 0.1136 - val_acc: 0.9602\n",
      "Epoch 209/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1144 - acc: 0.9651 - val_loss: 0.1078 - val_acc: 0.9687\n",
      "Epoch 210/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.1097 - acc: 0.9672 - val_loss: 0.1063 - val_acc: 0.9686\n",
      "Epoch 211/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.1103 - acc: 0.9680 - val_loss: 0.1137 - val_acc: 0.9674\n",
      "Epoch 212/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1114 - acc: 0.9650 - val_loss: 0.1064 - val_acc: 0.9674\n",
      "Epoch 213/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1089 - acc: 0.9679 - val_loss: 0.1151 - val_acc: 0.9653\n",
      "Epoch 214/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1112 - acc: 0.9661 - val_loss: 0.1038 - val_acc: 0.9706\n",
      "Epoch 215/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1103 - acc: 0.9668 - val_loss: 0.1024 - val_acc: 0.9697\n",
      "Epoch 216/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1069 - acc: 0.9684 - val_loss: 0.1063 - val_acc: 0.9656\n",
      "Epoch 217/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1064 - acc: 0.9685 - val_loss: 0.1035 - val_acc: 0.9735\n",
      "Epoch 218/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1110 - acc: 0.9669 - val_loss: 0.1017 - val_acc: 0.9688\n",
      "Epoch 219/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1048 - acc: 0.9691 - val_loss: 0.1014 - val_acc: 0.9686\n",
      "Epoch 220/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1061 - acc: 0.9673 - val_loss: 0.1079 - val_acc: 0.9644\n",
      "Epoch 221/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1049 - acc: 0.9686 - val_loss: 0.1030 - val_acc: 0.9693\n",
      "Epoch 222/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1079 - acc: 0.9663 - val_loss: 0.1001 - val_acc: 0.9725\n",
      "Epoch 223/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1051 - acc: 0.9673 - val_loss: 0.1031 - val_acc: 0.9694\n",
      "Epoch 224/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.1024 - acc: 0.9698 - val_loss: 0.1018 - val_acc: 0.9670\n",
      "Epoch 225/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1035 - acc: 0.9700 - val_loss: 0.1497 - val_acc: 0.9335\n",
      "Epoch 226/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1025 - acc: 0.9692 - val_loss: 0.0963 - val_acc: 0.9736\n",
      "Epoch 227/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1024 - acc: 0.9703 - val_loss: 0.1074 - val_acc: 0.9630\n",
      "Epoch 228/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1013 - acc: 0.9708 - val_loss: 0.0985 - val_acc: 0.9742\n",
      "Epoch 229/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.1024 - acc: 0.9690 - val_loss: 0.0961 - val_acc: 0.9724\n",
      "Epoch 230/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0997 - acc: 0.9715 - val_loss: 0.0972 - val_acc: 0.9710\n",
      "Epoch 231/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1060 - acc: 0.9674 - val_loss: 0.0959 - val_acc: 0.9733\n",
      "Epoch 232/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1000 - acc: 0.9698 - val_loss: 0.0953 - val_acc: 0.9742\n",
      "Epoch 233/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.1054 - acc: 0.9682 - val_loss: 0.1016 - val_acc: 0.9701\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0990 - acc: 0.9696 - val_loss: 0.0999 - val_acc: 0.9708\n",
      "Epoch 235/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0967 - acc: 0.9717 - val_loss: 0.1012 - val_acc: 0.9657\n",
      "Epoch 236/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.1179 - acc: 0.9603 - val_loss: 0.0962 - val_acc: 0.9727\n",
      "Epoch 237/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0967 - acc: 0.9706 - val_loss: 0.1028 - val_acc: 0.9693\n",
      "Epoch 238/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0985 - acc: 0.9708 - val_loss: 0.1017 - val_acc: 0.9682\n",
      "Epoch 239/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0984 - acc: 0.9707 - val_loss: 0.0978 - val_acc: 0.9651\n",
      "Epoch 240/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0967 - acc: 0.9712 - val_loss: 0.1016 - val_acc: 0.9680\n",
      "Epoch 241/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0965 - acc: 0.9714 - val_loss: 0.0908 - val_acc: 0.9738\n",
      "Epoch 242/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1026 - acc: 0.9672 - val_loss: 0.0913 - val_acc: 0.9734\n",
      "Epoch 243/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0924 - acc: 0.9729 - val_loss: 0.1011 - val_acc: 0.9644\n",
      "Epoch 244/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0951 - acc: 0.9728 - val_loss: 0.0943 - val_acc: 0.9722\n",
      "Epoch 245/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0952 - acc: 0.9725 - val_loss: 0.1010 - val_acc: 0.9675\n",
      "Epoch 246/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0949 - acc: 0.9715 - val_loss: 0.1021 - val_acc: 0.9693\n",
      "Epoch 247/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.1149 - acc: 0.9617 - val_loss: 0.1232 - val_acc: 0.9519\n",
      "Epoch 248/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0958 - acc: 0.9702 - val_loss: 0.0998 - val_acc: 0.9711\n",
      "Epoch 249/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0918 - acc: 0.9732 - val_loss: 0.0902 - val_acc: 0.9716\n",
      "Epoch 250/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0917 - acc: 0.9724 - val_loss: 0.0953 - val_acc: 0.9704\n",
      "Epoch 251/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0948 - acc: 0.9709 - val_loss: 0.1618 - val_acc: 0.9331\n",
      "Epoch 252/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0929 - acc: 0.9719 - val_loss: 0.0925 - val_acc: 0.9709\n",
      "Epoch 253/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0918 - acc: 0.9721 - val_loss: 0.0926 - val_acc: 0.9749\n",
      "Epoch 254/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0936 - acc: 0.9719 - val_loss: 0.0858 - val_acc: 0.9745\n",
      "Epoch 255/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0899 - acc: 0.9729 - val_loss: 0.0897 - val_acc: 0.9703\n",
      "Epoch 256/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0934 - acc: 0.9705 - val_loss: 0.0904 - val_acc: 0.9743\n",
      "Epoch 257/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0895 - acc: 0.9739 - val_loss: 0.0953 - val_acc: 0.9719\n",
      "Epoch 258/300\n",
      "28573/28573 [==============================] - 1s 24us/step - loss: 0.0926 - acc: 0.9712 - val_loss: 0.0881 - val_acc: 0.9772\n",
      "Epoch 259/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0885 - acc: 0.9736 - val_loss: 0.0851 - val_acc: 0.9761\n",
      "Epoch 260/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0941 - acc: 0.9715 - val_loss: 0.0849 - val_acc: 0.9725\n",
      "Epoch 261/300\n",
      "28573/28573 [==============================] - 1s 23us/step - loss: 0.0884 - acc: 0.9726 - val_loss: 0.0905 - val_acc: 0.9752\n",
      "Epoch 262/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0887 - acc: 0.9733 - val_loss: 0.0989 - val_acc: 0.9694\n",
      "Epoch 263/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0986 - acc: 0.9685 - val_loss: 0.0876 - val_acc: 0.9685\n",
      "Epoch 264/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0872 - acc: 0.9739 - val_loss: 0.0876 - val_acc: 0.9767\n",
      "Epoch 265/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0876 - acc: 0.9742 - val_loss: 0.0825 - val_acc: 0.9757\n",
      "Epoch 266/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0917 - acc: 0.9698 - val_loss: 0.0862 - val_acc: 0.9738\n",
      "Epoch 267/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.0858 - acc: 0.9749 - val_loss: 0.0821 - val_acc: 0.9765\n",
      "Epoch 268/300\n",
      "28573/28573 [==============================] - 1s 31us/step - loss: 0.0894 - acc: 0.9704 - val_loss: 0.0815 - val_acc: 0.9762\n",
      "Epoch 269/300\n",
      "28573/28573 [==============================] - 1s 29us/step - loss: 0.0835 - acc: 0.9752 - val_loss: 0.0854 - val_acc: 0.9739\n",
      "Epoch 270/300\n",
      "28573/28573 [==============================] - 1s 25us/step - loss: 0.0862 - acc: 0.9729 - val_loss: 0.0996 - val_acc: 0.9713\n",
      "Epoch 271/300\n",
      "28573/28573 [==============================] - 1s 26us/step - loss: 0.0900 - acc: 0.9720 - val_loss: 0.0858 - val_acc: 0.9723\n",
      "Epoch 272/300\n",
      "28573/28573 [==============================] - 1s 32us/step - loss: 0.0833 - acc: 0.9747 - val_loss: 0.0990 - val_acc: 0.9673\n",
      "Epoch 273/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0903 - acc: 0.9712 - val_loss: 0.1420 - val_acc: 0.9509\n",
      "Epoch 274/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0966 - acc: 0.9695 - val_loss: 0.0854 - val_acc: 0.9754\n",
      "Epoch 275/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0820 - acc: 0.9753 - val_loss: 0.0918 - val_acc: 0.9689\n",
      "Epoch 276/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0861 - acc: 0.9735 - val_loss: 0.0781 - val_acc: 0.9782\n",
      "Epoch 277/300\n",
      "28573/28573 [==============================] - 1s 19us/step - loss: 0.0824 - acc: 0.9749 - val_loss: 0.0783 - val_acc: 0.9752\n",
      "Epoch 278/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0838 - acc: 0.9750 - val_loss: 0.0874 - val_acc: 0.9694\n",
      "Epoch 279/300\n",
      "28573/28573 [==============================] - 1s 22us/step - loss: 0.0983 - acc: 0.9671 - val_loss: 0.0829 - val_acc: 0.9727\n",
      "Epoch 280/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0796 - acc: 0.9756 - val_loss: 0.0805 - val_acc: 0.9741\n",
      "Epoch 281/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0809 - acc: 0.9750 - val_loss: 0.0977 - val_acc: 0.9655\n",
      "Epoch 282/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0819 - acc: 0.9748 - val_loss: 0.0812 - val_acc: 0.9774\n",
      "Epoch 283/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.1010 - acc: 0.9650 - val_loss: 0.0882 - val_acc: 0.9675\n",
      "Epoch 284/300\n",
      "28573/28573 [==============================] - 1s 20us/step - loss: 0.0815 - acc: 0.9745 - val_loss: 0.0789 - val_acc: 0.9754\n",
      "Epoch 285/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0786 - acc: 0.9764 - val_loss: 0.0821 - val_acc: 0.9719\n",
      "Epoch 286/300\n",
      "28573/28573 [==============================] - 1s 18us/step - loss: 0.0854 - acc: 0.9726 - val_loss: 0.0789 - val_acc: 0.9774\n",
      "Epoch 287/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0798 - acc: 0.9755 - val_loss: 0.0853 - val_acc: 0.9750\n",
      "Epoch 288/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0828 - acc: 0.9735 - val_loss: 0.0892 - val_acc: 0.9689\n",
      "Epoch 289/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0833 - acc: 0.9744 - val_loss: 0.0765 - val_acc: 0.9774\n",
      "Epoch 290/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0788 - acc: 0.9753 - val_loss: 0.0779 - val_acc: 0.9777\n",
      "Epoch 291/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0797 - acc: 0.9754 - val_loss: 0.0750 - val_acc: 0.9784\n",
      "Epoch 292/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0823 - acc: 0.9747 - val_loss: 0.0744 - val_acc: 0.9774\n",
      "Epoch 293/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0777 - acc: 0.9763 - val_loss: 0.0768 - val_acc: 0.9768\n",
      "Epoch 294/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0806 - acc: 0.9744 - val_loss: 0.0799 - val_acc: 0.9718\n",
      "Epoch 295/300\n",
      "28573/28573 [==============================] - 1s 21us/step - loss: 0.0788 - acc: 0.9757 - val_loss: 0.0763 - val_acc: 0.9756\n",
      "Epoch 296/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0781 - acc: 0.9760 - val_loss: 0.0892 - val_acc: 0.9707\n",
      "Epoch 297/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0808 - acc: 0.9753 - val_loss: 0.0884 - val_acc: 0.9759\n",
      "Epoch 298/300\n",
      "28573/28573 [==============================] - 0s 17us/step - loss: 0.0827 - acc: 0.9742 - val_loss: 0.0741 - val_acc: 0.9768\n",
      "Epoch 299/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0756 - acc: 0.9772 - val_loss: 0.0909 - val_acc: 0.9660\n",
      "Epoch 300/300\n",
      "28573/28573 [==============================] - 0s 16us/step - loss: 0.0799 - acc: 0.9756 - val_loss: 0.0732 - val_acc: 0.9781\n",
      "7144/7144 [==============================] - 1s 72us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17858 samples, validate on 17858 samples\n",
      "Epoch 1/300\n",
      "17858/17858 [==============================] - 3s 186us/step - loss: 3.3300 - acc: 0.0871 - val_loss: 3.1458 - val_acc: 0.1192\n",
      "Epoch 2/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 2.9443 - acc: 0.1337 - val_loss: 2.7452 - val_acc: 0.1794\n",
      "Epoch 3/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 2.5869 - acc: 0.2156 - val_loss: 2.4181 - val_acc: 0.2553\n",
      "Epoch 4/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 2.2509 - acc: 0.2964 - val_loss: 2.0728 - val_acc: 0.3644\n",
      "Epoch 5/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.9279 - acc: 0.4114 - val_loss: 1.7840 - val_acc: 0.4456\n",
      "Epoch 6/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.6885 - acc: 0.4739 - val_loss: 1.5962 - val_acc: 0.5249\n",
      "Epoch 7/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 1.5224 - acc: 0.5140 - val_loss: 1.4884 - val_acc: 0.4403\n",
      "Epoch 8/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.4355 - acc: 0.4905 - val_loss: 1.3946 - val_acc: 0.4863\n",
      "Epoch 9/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.3985 - acc: 0.4761 - val_loss: 1.3712 - val_acc: 0.4737\n",
      "Epoch 10/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 1.3376 - acc: 0.4934 - val_loss: 1.2922 - val_acc: 0.4796\n",
      "Epoch 11/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.2756 - acc: 0.5063 - val_loss: 1.2202 - val_acc: 0.5062\n",
      "Epoch 12/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 1.2573 - acc: 0.5089 - val_loss: 1.2599 - val_acc: 0.5246\n",
      "Epoch 13/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.2106 - acc: 0.5233 - val_loss: 1.0952 - val_acc: 0.5595\n",
      "Epoch 14/300\n",
      "17858/17858 [==============================] - 0s 22us/step - loss: 1.1658 - acc: 0.5418 - val_loss: 1.2766 - val_acc: 0.4591\n",
      "Epoch 15/300\n",
      "17858/17858 [==============================] - 1s 28us/step - loss: 1.1660 - acc: 0.5337 - val_loss: 1.2486 - val_acc: 0.5241\n",
      "Epoch 16/300\n",
      "17858/17858 [==============================] - 0s 24us/step - loss: 1.1008 - acc: 0.5711 - val_loss: 1.1520 - val_acc: 0.5096\n",
      "Epoch 17/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 1.0748 - acc: 0.5732 - val_loss: 0.9334 - val_acc: 0.6790\n",
      "Epoch 18/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 1.0285 - acc: 0.5864 - val_loss: 1.1396 - val_acc: 0.5100\n",
      "Epoch 19/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.9996 - acc: 0.6064 - val_loss: 1.0162 - val_acc: 0.5905\n",
      "Epoch 20/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 1.0174 - acc: 0.5913 - val_loss: 0.8879 - val_acc: 0.6958\n",
      "Epoch 21/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.9678 - acc: 0.6104 - val_loss: 0.9820 - val_acc: 0.6079\n",
      "Epoch 22/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.9718 - acc: 0.6082 - val_loss: 0.9656 - val_acc: 0.6348\n",
      "Epoch 23/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.9340 - acc: 0.6144 - val_loss: 1.0368 - val_acc: 0.5613\n",
      "Epoch 24/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.9027 - acc: 0.6439 - val_loss: 0.8174 - val_acc: 0.7069\n",
      "Epoch 25/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.9325 - acc: 0.6087 - val_loss: 0.8496 - val_acc: 0.6971\n",
      "Epoch 26/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.8584 - acc: 0.6588 - val_loss: 0.8648 - val_acc: 0.6191\n",
      "Epoch 27/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.8553 - acc: 0.6560 - val_loss: 0.8211 - val_acc: 0.6917\n",
      "Epoch 28/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.8544 - acc: 0.6485 - val_loss: 0.7192 - val_acc: 0.7592\n",
      "Epoch 29/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.8001 - acc: 0.6931 - val_loss: 0.9327 - val_acc: 0.5944\n",
      "Epoch 30/300\n",
      "17858/17858 [==============================] - 0s 22us/step - loss: 0.7182 - acc: 0.7501 - val_loss: 0.7360 - val_acc: 0.7476\n",
      "Epoch 31/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.7743 - acc: 0.7031 - val_loss: 0.6853 - val_acc: 0.7577\n",
      "Epoch 32/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.6563 - acc: 0.7886 - val_loss: 0.6740 - val_acc: 0.7891\n",
      "Epoch 33/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.6652 - acc: 0.7771 - val_loss: 0.6303 - val_acc: 0.7806\n",
      "Epoch 34/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.6588 - acc: 0.7785 - val_loss: 0.6145 - val_acc: 0.8254\n",
      "Epoch 35/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.6311 - acc: 0.7901 - val_loss: 0.6294 - val_acc: 0.7817\n",
      "Epoch 36/300\n",
      "17858/17858 [==============================] - 0s 22us/step - loss: 0.6050 - acc: 0.8099 - val_loss: 0.5908 - val_acc: 0.8326\n",
      "Epoch 37/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5811 - acc: 0.8180 - val_loss: 0.5761 - val_acc: 0.8299\n",
      "Epoch 38/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.5807 - acc: 0.8144 - val_loss: 0.5683 - val_acc: 0.8308\n",
      "Epoch 39/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5690 - acc: 0.8172 - val_loss: 0.5586 - val_acc: 0.8380\n",
      "Epoch 40/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.5561 - acc: 0.8285 - val_loss: 0.5529 - val_acc: 0.8295\n",
      "Epoch 41/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5466 - acc: 0.8336 - val_loss: 0.5361 - val_acc: 0.8339\n",
      "Epoch 42/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.5382 - acc: 0.8349 - val_loss: 0.5398 - val_acc: 0.8373\n",
      "Epoch 43/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5311 - acc: 0.8364 - val_loss: 0.5272 - val_acc: 0.8120\n",
      "Epoch 44/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.5229 - acc: 0.8371 - val_loss: 0.5215 - val_acc: 0.8439\n",
      "Epoch 45/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5137 - acc: 0.8416 - val_loss: 0.5078 - val_acc: 0.8434\n",
      "Epoch 46/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5083 - acc: 0.8382 - val_loss: 0.5149 - val_acc: 0.8038\n",
      "Epoch 47/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5005 - acc: 0.8419 - val_loss: 0.4911 - val_acc: 0.8550\n",
      "Epoch 48/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4976 - acc: 0.8405 - val_loss: 0.4921 - val_acc: 0.8411\n",
      "Epoch 49/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4878 - acc: 0.8467 - val_loss: 0.4905 - val_acc: 0.8437\n",
      "Epoch 50/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4828 - acc: 0.8491 - val_loss: 0.4799 - val_acc: 0.8568\n",
      "Epoch 51/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4754 - acc: 0.8515 - val_loss: 0.4754 - val_acc: 0.8453\n",
      "Epoch 52/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4681 - acc: 0.8550 - val_loss: 0.4676 - val_acc: 0.8592\n",
      "Epoch 53/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4620 - acc: 0.8566 - val_loss: 0.4658 - val_acc: 0.8495\n",
      "Epoch 54/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4568 - acc: 0.8570 - val_loss: 0.4497 - val_acc: 0.8616\n",
      "Epoch 55/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.4529 - acc: 0.8565 - val_loss: 0.4514 - val_acc: 0.8571\n",
      "Epoch 56/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.4482 - acc: 0.8573 - val_loss: 0.4462 - val_acc: 0.8642\n",
      "Epoch 57/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4417 - acc: 0.8595 - val_loss: 0.4401 - val_acc: 0.8598\n",
      "Epoch 58/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4354 - acc: 0.8614 - val_loss: 0.4418 - val_acc: 0.8586\n",
      "Epoch 59/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4322 - acc: 0.8610 - val_loss: 0.4292 - val_acc: 0.8609\n",
      "Epoch 60/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4273 - acc: 0.8606 - val_loss: 0.4261 - val_acc: 0.8650\n",
      "Epoch 61/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4230 - acc: 0.8642 - val_loss: 0.4195 - val_acc: 0.8670\n",
      "Epoch 62/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4191 - acc: 0.8630 - val_loss: 0.4250 - val_acc: 0.8570\n",
      "Epoch 63/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4148 - acc: 0.8641 - val_loss: 0.4055 - val_acc: 0.8694\n",
      "Epoch 64/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4085 - acc: 0.8680 - val_loss: 0.4108 - val_acc: 0.8644\n",
      "Epoch 65/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4067 - acc: 0.8673 - val_loss: 0.4078 - val_acc: 0.8582\n",
      "Epoch 66/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4024 - acc: 0.8672 - val_loss: 0.3989 - val_acc: 0.8688\n",
      "Epoch 67/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3981 - acc: 0.8677 - val_loss: 0.3951 - val_acc: 0.8695\n",
      "Epoch 68/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3938 - acc: 0.8680 - val_loss: 0.3869 - val_acc: 0.8722\n",
      "Epoch 69/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3920 - acc: 0.8712 - val_loss: 0.3833 - val_acc: 0.8677\n",
      "Epoch 70/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3849 - acc: 0.8713 - val_loss: 0.3819 - val_acc: 0.8685\n",
      "Epoch 71/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3830 - acc: 0.8723 - val_loss: 0.3784 - val_acc: 0.8715\n",
      "Epoch 72/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3823 - acc: 0.8692 - val_loss: 0.3716 - val_acc: 0.8751\n",
      "Epoch 73/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3755 - acc: 0.8732 - val_loss: 0.3805 - val_acc: 0.8760\n",
      "Epoch 74/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3732 - acc: 0.8731 - val_loss: 0.3642 - val_acc: 0.8729\n",
      "Epoch 75/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3695 - acc: 0.8719 - val_loss: 0.3676 - val_acc: 0.8783\n",
      "Epoch 76/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3694 - acc: 0.8738 - val_loss: 0.3620 - val_acc: 0.8783\n",
      "Epoch 77/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3631 - acc: 0.8775 - val_loss: 0.3566 - val_acc: 0.8752\n",
      "Epoch 78/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3594 - acc: 0.8765 - val_loss: 0.3557 - val_acc: 0.8823\n",
      "Epoch 79/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3566 - acc: 0.8789 - val_loss: 0.3642 - val_acc: 0.8821\n",
      "Epoch 80/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3539 - acc: 0.8771 - val_loss: 0.3480 - val_acc: 0.8769\n",
      "Epoch 81/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3529 - acc: 0.8774 - val_loss: 0.3566 - val_acc: 0.8748\n",
      "Epoch 82/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3495 - acc: 0.8784 - val_loss: 0.3514 - val_acc: 0.8802\n",
      "Epoch 83/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3461 - acc: 0.8792 - val_loss: 0.3440 - val_acc: 0.8775\n",
      "Epoch 84/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3442 - acc: 0.8781 - val_loss: 0.3436 - val_acc: 0.8805\n",
      "Epoch 85/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3417 - acc: 0.8816 - val_loss: 0.3415 - val_acc: 0.8807\n",
      "Epoch 86/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3372 - acc: 0.8818 - val_loss: 0.3343 - val_acc: 0.8827\n",
      "Epoch 87/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3365 - acc: 0.8808 - val_loss: 0.3397 - val_acc: 0.8802\n",
      "Epoch 88/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3325 - acc: 0.8832 - val_loss: 0.3301 - val_acc: 0.8821\n",
      "Epoch 89/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3310 - acc: 0.8847 - val_loss: 0.3260 - val_acc: 0.8844\n",
      "Epoch 90/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3266 - acc: 0.8849 - val_loss: 0.3231 - val_acc: 0.8803\n",
      "Epoch 91/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.3268 - acc: 0.8813 - val_loss: 0.3280 - val_acc: 0.8826\n",
      "Epoch 92/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3264 - acc: 0.8827 - val_loss: 0.3244 - val_acc: 0.8898\n",
      "Epoch 93/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3224 - acc: 0.8845 - val_loss: 0.3267 - val_acc: 0.8873\n",
      "Epoch 94/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3236 - acc: 0.8848 - val_loss: 0.3140 - val_acc: 0.8859\n",
      "Epoch 95/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3157 - acc: 0.8851 - val_loss: 0.3098 - val_acc: 0.8895\n",
      "Epoch 96/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3154 - acc: 0.8872 - val_loss: 0.3098 - val_acc: 0.8902\n",
      "Epoch 97/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3126 - acc: 0.8881 - val_loss: 0.3115 - val_acc: 0.8944\n",
      "Epoch 98/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3116 - acc: 0.8887 - val_loss: 0.3089 - val_acc: 0.8872\n",
      "Epoch 99/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3091 - acc: 0.8878 - val_loss: 0.3154 - val_acc: 0.8893\n",
      "Epoch 100/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3090 - acc: 0.8882 - val_loss: 0.3045 - val_acc: 0.8911\n",
      "Epoch 101/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3044 - acc: 0.8891 - val_loss: 0.3023 - val_acc: 0.8907\n",
      "Epoch 102/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3021 - acc: 0.8913 - val_loss: 0.2996 - val_acc: 0.8924\n",
      "Epoch 103/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3019 - acc: 0.8905 - val_loss: 0.3069 - val_acc: 0.8846\n",
      "Epoch 104/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3005 - acc: 0.8913 - val_loss: 0.3029 - val_acc: 0.8848\n",
      "Epoch 105/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3025 - acc: 0.8899 - val_loss: 0.2934 - val_acc: 0.8932\n",
      "Epoch 106/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2967 - acc: 0.8927 - val_loss: 0.2916 - val_acc: 0.8928\n",
      "Epoch 107/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2957 - acc: 0.8924 - val_loss: 0.2928 - val_acc: 0.9007\n",
      "Epoch 108/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8907 - val_loss: 0.2965 - val_acc: 0.8935\n",
      "Epoch 109/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2904 - acc: 0.8960 - val_loss: 0.2901 - val_acc: 0.8874\n",
      "Epoch 110/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2895 - acc: 0.8934 - val_loss: 0.2904 - val_acc: 0.8904\n",
      "Epoch 111/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2883 - acc: 0.8933 - val_loss: 0.2849 - val_acc: 0.9044\n",
      "Epoch 112/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2874 - acc: 0.8960 - val_loss: 0.2892 - val_acc: 0.8953\n",
      "Epoch 113/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2852 - acc: 0.8949 - val_loss: 0.2837 - val_acc: 0.8962\n",
      "Epoch 114/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2844 - acc: 0.8971 - val_loss: 0.2777 - val_acc: 0.8946\n",
      "Epoch 115/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2816 - acc: 0.8977 - val_loss: 0.2851 - val_acc: 0.8934\n",
      "Epoch 116/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2786 - acc: 0.8975 - val_loss: 0.2764 - val_acc: 0.8952\n",
      "Epoch 117/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2791 - acc: 0.8960 - val_loss: 0.2764 - val_acc: 0.9013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2796 - acc: 0.8949 - val_loss: 0.2749 - val_acc: 0.8972\n",
      "Epoch 119/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2773 - acc: 0.8975 - val_loss: 0.2706 - val_acc: 0.9063\n",
      "Epoch 120/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2755 - acc: 0.8983 - val_loss: 0.2813 - val_acc: 0.8884\n",
      "Epoch 121/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2735 - acc: 0.8995 - val_loss: 0.2737 - val_acc: 0.8948\n",
      "Epoch 122/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2718 - acc: 0.8979 - val_loss: 0.2670 - val_acc: 0.9039\n",
      "Epoch 123/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2723 - acc: 0.8991 - val_loss: 0.2657 - val_acc: 0.9078\n",
      "Epoch 124/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2714 - acc: 0.9008 - val_loss: 0.2705 - val_acc: 0.9011\n",
      "Epoch 125/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2692 - acc: 0.8993 - val_loss: 0.2686 - val_acc: 0.8952\n",
      "Epoch 126/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.2680 - acc: 0.8989 - val_loss: 0.2638 - val_acc: 0.9063\n",
      "Epoch 127/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2649 - acc: 0.9016 - val_loss: 0.2659 - val_acc: 0.8935\n",
      "Epoch 128/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2643 - acc: 0.9008 - val_loss: 0.2645 - val_acc: 0.9036\n",
      "Epoch 129/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.2641 - acc: 0.9026 - val_loss: 0.2650 - val_acc: 0.8953\n",
      "Epoch 130/300\n",
      "17858/17858 [==============================] - 0s 24us/step - loss: 0.2632 - acc: 0.9013 - val_loss: 0.2625 - val_acc: 0.8951\n",
      "Epoch 131/300\n",
      "17858/17858 [==============================] - ETA: 0s - loss: 0.2613 - acc: 0.899 - 0s 18us/step - loss: 0.2609 - acc: 0.9003 - val_loss: 0.2568 - val_acc: 0.9003\n",
      "Epoch 132/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2598 - acc: 0.9036 - val_loss: 0.2545 - val_acc: 0.9140\n",
      "Epoch 133/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2580 - acc: 0.9016 - val_loss: 0.2637 - val_acc: 0.9017\n",
      "Epoch 134/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2596 - acc: 0.9012 - val_loss: 0.2538 - val_acc: 0.8984\n",
      "Epoch 135/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.2547 - acc: 0.9026 - val_loss: 0.2630 - val_acc: 0.9087\n",
      "Epoch 136/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.2547 - acc: 0.9066 - val_loss: 0.2564 - val_acc: 0.9007\n",
      "Epoch 137/300\n",
      "17858/17858 [==============================] - 0s 26us/step - loss: 0.2524 - acc: 0.9042 - val_loss: 0.2484 - val_acc: 0.9055\n",
      "Epoch 138/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.2548 - acc: 0.9047 - val_loss: 0.2552 - val_acc: 0.9025\n",
      "Epoch 139/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2525 - acc: 0.9050 - val_loss: 0.2494 - val_acc: 0.9027\n",
      "Epoch 140/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2509 - acc: 0.9065 - val_loss: 0.2525 - val_acc: 0.9024\n",
      "Epoch 141/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2502 - acc: 0.9060 - val_loss: 0.2474 - val_acc: 0.8964\n",
      "Epoch 142/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2502 - acc: 0.9059 - val_loss: 0.2432 - val_acc: 0.9175\n",
      "Epoch 143/300\n",
      "17858/17858 [==============================] - 0s 25us/step - loss: 0.2488 - acc: 0.9054 - val_loss: 0.2449 - val_acc: 0.9035\n",
      "Epoch 144/300\n",
      "17858/17858 [==============================] - 0s 22us/step - loss: 0.2482 - acc: 0.9060 - val_loss: 0.2490 - val_acc: 0.9096\n",
      "Epoch 145/300\n",
      "17858/17858 [==============================] - 0s 24us/step - loss: 0.2465 - acc: 0.9064 - val_loss: 0.2458 - val_acc: 0.9088\n",
      "Epoch 146/300\n",
      "17858/17858 [==============================] - 0s 22us/step - loss: 0.2467 - acc: 0.9050 - val_loss: 0.2406 - val_acc: 0.9014\n",
      "Epoch 147/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.2416 - acc: 0.9075 - val_loss: 0.2449 - val_acc: 0.9143\n",
      "Epoch 148/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2424 - acc: 0.9059 - val_loss: 0.2403 - val_acc: 0.9091\n",
      "Epoch 149/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2416 - acc: 0.9056 - val_loss: 0.2429 - val_acc: 0.9058\n",
      "Epoch 150/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2404 - acc: 0.9073 - val_loss: 0.2407 - val_acc: 0.9061\n",
      "Epoch 151/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2398 - acc: 0.9091 - val_loss: 0.2394 - val_acc: 0.8995\n",
      "Epoch 152/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2397 - acc: 0.9087 - val_loss: 0.2384 - val_acc: 0.9059\n",
      "Epoch 153/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2380 - acc: 0.9128 - val_loss: 0.2400 - val_acc: 0.9166\n",
      "Epoch 154/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2415 - acc: 0.9058 - val_loss: 0.2445 - val_acc: 0.9016\n",
      "Epoch 155/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2361 - acc: 0.9125 - val_loss: 0.2465 - val_acc: 0.9168\n",
      "Epoch 156/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2351 - acc: 0.9088 - val_loss: 0.2393 - val_acc: 0.9063\n",
      "Epoch 157/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2346 - acc: 0.9105 - val_loss: 0.2380 - val_acc: 0.9113\n",
      "Epoch 158/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2318 - acc: 0.9120 - val_loss: 0.2306 - val_acc: 0.9162\n",
      "Epoch 159/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.2320 - acc: 0.9105 - val_loss: 0.2344 - val_acc: 0.9050\n",
      "Epoch 160/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2323 - acc: 0.9109 - val_loss: 0.2395 - val_acc: 0.9069\n",
      "Epoch 161/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2324 - acc: 0.9120 - val_loss: 0.2273 - val_acc: 0.9162\n",
      "Epoch 162/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2293 - acc: 0.9135 - val_loss: 0.2256 - val_acc: 0.9112\n",
      "Epoch 163/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2278 - acc: 0.9138 - val_loss: 0.2245 - val_acc: 0.9186\n",
      "Epoch 164/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2286 - acc: 0.9131 - val_loss: 0.2264 - val_acc: 0.9163\n",
      "Epoch 165/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2269 - acc: 0.9132 - val_loss: 0.2277 - val_acc: 0.9115\n",
      "Epoch 166/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2302 - acc: 0.9115 - val_loss: 0.2234 - val_acc: 0.9186\n",
      "Epoch 167/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.2254 - acc: 0.9151 - val_loss: 0.2346 - val_acc: 0.9217\n",
      "Epoch 168/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2262 - acc: 0.9146 - val_loss: 0.2325 - val_acc: 0.9230\n",
      "Epoch 169/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2269 - acc: 0.9121 - val_loss: 0.2368 - val_acc: 0.8989\n",
      "Epoch 170/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2277 - acc: 0.9117 - val_loss: 0.2174 - val_acc: 0.9124\n",
      "Epoch 171/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2242 - acc: 0.9149 - val_loss: 0.2293 - val_acc: 0.9005\n",
      "Epoch 172/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2214 - acc: 0.9159 - val_loss: 0.2240 - val_acc: 0.9047\n",
      "Epoch 173/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2206 - acc: 0.9167 - val_loss: 0.2219 - val_acc: 0.9134\n",
      "Epoch 174/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2205 - acc: 0.9158 - val_loss: 0.2377 - val_acc: 0.8991\n",
      "Epoch 175/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2219 - acc: 0.9129 - val_loss: 0.2227 - val_acc: 0.9129\n",
      "Epoch 176/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2206 - acc: 0.9156 - val_loss: 0.2177 - val_acc: 0.9205\n",
      "Epoch 177/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2178 - acc: 0.9167 - val_loss: 0.2156 - val_acc: 0.9213\n",
      "Epoch 178/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2190 - acc: 0.9164 - val_loss: 0.2138 - val_acc: 0.9176\n",
      "Epoch 179/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.2153 - acc: 0.9159 - val_loss: 0.2121 - val_acc: 0.9244\n",
      "Epoch 180/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2181 - acc: 0.9177 - val_loss: 0.2191 - val_acc: 0.9124\n",
      "Epoch 181/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2165 - acc: 0.9171 - val_loss: 0.2266 - val_acc: 0.9109\n",
      "Epoch 182/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2147 - acc: 0.9183 - val_loss: 0.2148 - val_acc: 0.9280\n",
      "Epoch 183/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2149 - acc: 0.9198 - val_loss: 0.2104 - val_acc: 0.9233\n",
      "Epoch 184/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2140 - acc: 0.9185 - val_loss: 0.2127 - val_acc: 0.9186\n",
      "Epoch 185/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2143 - acc: 0.9170 - val_loss: 0.2185 - val_acc: 0.9047\n",
      "Epoch 186/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2127 - acc: 0.9181 - val_loss: 0.2075 - val_acc: 0.9178\n",
      "Epoch 187/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2141 - acc: 0.9192 - val_loss: 0.2150 - val_acc: 0.9221\n",
      "Epoch 188/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2108 - acc: 0.9192 - val_loss: 0.2112 - val_acc: 0.9212\n",
      "Epoch 189/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2100 - acc: 0.9220 - val_loss: 0.2051 - val_acc: 0.9143\n",
      "Epoch 190/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2081 - acc: 0.9224 - val_loss: 0.2126 - val_acc: 0.9142\n",
      "Epoch 191/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2100 - acc: 0.9236 - val_loss: 0.2072 - val_acc: 0.9140\n",
      "Epoch 192/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2119 - acc: 0.9186 - val_loss: 0.2076 - val_acc: 0.9323\n",
      "Epoch 193/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2094 - acc: 0.9222 - val_loss: 0.2079 - val_acc: 0.9120\n",
      "Epoch 194/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2060 - acc: 0.9211 - val_loss: 0.2071 - val_acc: 0.9288\n",
      "Epoch 195/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2078 - acc: 0.9210 - val_loss: 0.2018 - val_acc: 0.9185\n",
      "Epoch 196/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2055 - acc: 0.9241 - val_loss: 0.2039 - val_acc: 0.9231\n",
      "Epoch 197/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2140 - acc: 0.9168 - val_loss: 0.2024 - val_acc: 0.9197\n",
      "Epoch 198/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2075 - acc: 0.9210 - val_loss: 0.1990 - val_acc: 0.9200\n",
      "Epoch 199/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2026 - acc: 0.9254 - val_loss: 0.2097 - val_acc: 0.9125\n",
      "Epoch 200/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2041 - acc: 0.9216 - val_loss: 0.2019 - val_acc: 0.9267\n",
      "Epoch 201/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2017 - acc: 0.9261 - val_loss: 0.2051 - val_acc: 0.9223\n",
      "Epoch 202/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2054 - acc: 0.9213 - val_loss: 0.2025 - val_acc: 0.9276\n",
      "Epoch 203/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2028 - acc: 0.9227 - val_loss: 0.1973 - val_acc: 0.9222\n",
      "Epoch 204/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1997 - acc: 0.9260 - val_loss: 0.2036 - val_acc: 0.9247\n",
      "Epoch 205/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1999 - acc: 0.9255 - val_loss: 0.2032 - val_acc: 0.9175\n",
      "Epoch 206/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1995 - acc: 0.9249 - val_loss: 0.1987 - val_acc: 0.9174\n",
      "Epoch 207/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1979 - acc: 0.9282 - val_loss: 0.1949 - val_acc: 0.9248\n",
      "Epoch 208/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2003 - acc: 0.9266 - val_loss: 0.1942 - val_acc: 0.9331\n",
      "Epoch 209/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1956 - acc: 0.9286 - val_loss: 0.1964 - val_acc: 0.9392\n",
      "Epoch 210/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.1970 - acc: 0.9256 - val_loss: 0.1946 - val_acc: 0.9287\n",
      "Epoch 211/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.1975 - acc: 0.9264 - val_loss: 0.1939 - val_acc: 0.9335\n",
      "Epoch 212/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.1952 - acc: 0.9283 - val_loss: 0.1941 - val_acc: 0.9288\n",
      "Epoch 213/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1943 - acc: 0.9303 - val_loss: 0.2087 - val_acc: 0.9248\n",
      "Epoch 214/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1979 - acc: 0.9277 - val_loss: 0.1959 - val_acc: 0.9243\n",
      "Epoch 215/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1950 - acc: 0.9292 - val_loss: 0.1894 - val_acc: 0.9328\n",
      "Epoch 216/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1945 - acc: 0.9292 - val_loss: 0.1991 - val_acc: 0.9256\n",
      "Epoch 217/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1926 - acc: 0.9280 - val_loss: 0.2091 - val_acc: 0.9245\n",
      "Epoch 218/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1917 - acc: 0.9311 - val_loss: 0.1991 - val_acc: 0.9190\n",
      "Epoch 219/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1939 - acc: 0.9298 - val_loss: 0.1902 - val_acc: 0.9287\n",
      "Epoch 220/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1935 - acc: 0.9284 - val_loss: 0.1933 - val_acc: 0.9203\n",
      "Epoch 221/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1941 - acc: 0.9271 - val_loss: 0.1948 - val_acc: 0.9257\n",
      "Epoch 222/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1892 - acc: 0.9320 - val_loss: 0.1869 - val_acc: 0.9434\n",
      "Epoch 223/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1892 - acc: 0.9311 - val_loss: 0.1917 - val_acc: 0.9344\n",
      "Epoch 224/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1936 - acc: 0.9276 - val_loss: 0.1910 - val_acc: 0.9295\n",
      "Epoch 225/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1914 - acc: 0.9309 - val_loss: 0.1882 - val_acc: 0.9298\n",
      "Epoch 226/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1886 - acc: 0.9316 - val_loss: 0.1892 - val_acc: 0.9214\n",
      "Epoch 227/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1861 - acc: 0.9336 - val_loss: 0.1828 - val_acc: 0.9447\n",
      "Epoch 228/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1868 - acc: 0.9305 - val_loss: 0.1887 - val_acc: 0.9321\n",
      "Epoch 229/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1861 - acc: 0.9333 - val_loss: 0.1880 - val_acc: 0.9464\n",
      "Epoch 230/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1887 - acc: 0.9330 - val_loss: 0.1817 - val_acc: 0.9313\n",
      "Epoch 231/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1870 - acc: 0.9301 - val_loss: 0.1811 - val_acc: 0.9361\n",
      "Epoch 232/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1833 - acc: 0.9361 - val_loss: 0.1826 - val_acc: 0.9316\n",
      "Epoch 233/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1842 - acc: 0.9341 - val_loss: 0.1821 - val_acc: 0.9391\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1857 - acc: 0.9328 - val_loss: 0.1805 - val_acc: 0.9453\n",
      "Epoch 235/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1826 - acc: 0.9371 - val_loss: 0.1889 - val_acc: 0.9280\n",
      "Epoch 236/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1848 - acc: 0.9316 - val_loss: 0.1993 - val_acc: 0.9115\n",
      "Epoch 237/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1845 - acc: 0.9356 - val_loss: 0.1776 - val_acc: 0.9484\n",
      "Epoch 238/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1839 - acc: 0.9343 - val_loss: 0.1848 - val_acc: 0.9409\n",
      "Epoch 239/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1861 - acc: 0.9311 - val_loss: 0.1841 - val_acc: 0.9336\n",
      "Epoch 240/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1798 - acc: 0.9368 - val_loss: 0.1862 - val_acc: 0.9164\n",
      "Epoch 241/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1793 - acc: 0.9371 - val_loss: 0.1800 - val_acc: 0.9482\n",
      "Epoch 242/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1804 - acc: 0.9358 - val_loss: 0.1778 - val_acc: 0.9485\n",
      "Epoch 243/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1845 - acc: 0.9329 - val_loss: 0.1764 - val_acc: 0.9396\n",
      "Epoch 244/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1786 - acc: 0.9383 - val_loss: 0.1755 - val_acc: 0.9437\n",
      "Epoch 245/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1773 - acc: 0.9396 - val_loss: 0.1781 - val_acc: 0.9337\n",
      "Epoch 246/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.1774 - acc: 0.9389 - val_loss: 0.1788 - val_acc: 0.9330\n",
      "Epoch 247/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1777 - acc: 0.9385 - val_loss: 0.1713 - val_acc: 0.9497\n",
      "Epoch 248/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1808 - acc: 0.9385 - val_loss: 0.1736 - val_acc: 0.9308\n",
      "Epoch 249/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1731 - acc: 0.9423 - val_loss: 0.1744 - val_acc: 0.9401\n",
      "Epoch 250/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1732 - acc: 0.9428 - val_loss: 0.1672 - val_acc: 0.9414\n",
      "Epoch 251/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1749 - acc: 0.9411 - val_loss: 0.1745 - val_acc: 0.9317\n",
      "Epoch 252/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1748 - acc: 0.9389 - val_loss: 0.1681 - val_acc: 0.9477\n",
      "Epoch 253/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1757 - acc: 0.9390 - val_loss: 0.1845 - val_acc: 0.9363\n",
      "Epoch 254/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1759 - acc: 0.9397 - val_loss: 0.1763 - val_acc: 0.9396\n",
      "Epoch 255/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1706 - acc: 0.9447 - val_loss: 0.1680 - val_acc: 0.9586\n",
      "Epoch 256/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1726 - acc: 0.9442 - val_loss: 0.1752 - val_acc: 0.9452\n",
      "Epoch 257/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1681 - acc: 0.9475 - val_loss: 0.1693 - val_acc: 0.9566\n",
      "Epoch 258/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1708 - acc: 0.9429 - val_loss: 0.1673 - val_acc: 0.9406\n",
      "Epoch 259/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1738 - acc: 0.9406 - val_loss: 0.1790 - val_acc: 0.9377\n",
      "Epoch 260/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1755 - acc: 0.9364 - val_loss: 0.2105 - val_acc: 0.8960\n",
      "Epoch 261/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1724 - acc: 0.9377 - val_loss: 0.1845 - val_acc: 0.9291\n",
      "Epoch 262/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1706 - acc: 0.9410 - val_loss: 0.1703 - val_acc: 0.9433\n",
      "Epoch 263/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1686 - acc: 0.9461 - val_loss: 0.1674 - val_acc: 0.9341\n",
      "Epoch 264/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1691 - acc: 0.9431 - val_loss: 0.1707 - val_acc: 0.9364\n",
      "Epoch 265/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1675 - acc: 0.9442 - val_loss: 0.1723 - val_acc: 0.9327\n",
      "Epoch 266/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1720 - acc: 0.9402 - val_loss: 0.1634 - val_acc: 0.9354\n",
      "Epoch 267/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1648 - acc: 0.9455 - val_loss: 0.1719 - val_acc: 0.9434\n",
      "Epoch 268/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1682 - acc: 0.9414 - val_loss: 0.1636 - val_acc: 0.9476\n",
      "Epoch 269/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1641 - acc: 0.9472 - val_loss: 0.1586 - val_acc: 0.9493\n",
      "Epoch 270/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1639 - acc: 0.9475 - val_loss: 0.1694 - val_acc: 0.9497\n",
      "Epoch 271/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1622 - acc: 0.9488 - val_loss: 0.1802 - val_acc: 0.9407\n",
      "Epoch 272/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1628 - acc: 0.9484 - val_loss: 0.1584 - val_acc: 0.9611\n",
      "Epoch 273/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1624 - acc: 0.9481 - val_loss: 0.1637 - val_acc: 0.9489\n",
      "Epoch 274/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1679 - acc: 0.9422 - val_loss: 0.1878 - val_acc: 0.9237\n",
      "Epoch 275/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1634 - acc: 0.9486 - val_loss: 0.1676 - val_acc: 0.9478\n",
      "Epoch 276/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1628 - acc: 0.9467 - val_loss: 0.1642 - val_acc: 0.9563\n",
      "Epoch 277/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1614 - acc: 0.9498 - val_loss: 0.1553 - val_acc: 0.9591\n",
      "Epoch 278/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1618 - acc: 0.9462 - val_loss: 0.1578 - val_acc: 0.9576\n",
      "Epoch 279/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1645 - acc: 0.9450 - val_loss: 0.1605 - val_acc: 0.9580\n",
      "Epoch 280/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1619 - acc: 0.9475 - val_loss: 0.1674 - val_acc: 0.9367\n",
      "Epoch 281/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1606 - acc: 0.9473 - val_loss: 0.1615 - val_acc: 0.9375\n",
      "Epoch 282/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1681 - acc: 0.9396 - val_loss: 0.1672 - val_acc: 0.9374\n",
      "Epoch 283/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1599 - acc: 0.9474 - val_loss: 0.1529 - val_acc: 0.9612\n",
      "Epoch 284/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1571 - acc: 0.9499 - val_loss: 0.1546 - val_acc: 0.9565\n",
      "Epoch 285/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1549 - acc: 0.9534 - val_loss: 0.1518 - val_acc: 0.9594\n",
      "Epoch 286/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1594 - acc: 0.9461 - val_loss: 0.1529 - val_acc: 0.9536\n",
      "Epoch 287/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1585 - acc: 0.9456 - val_loss: 0.1571 - val_acc: 0.9523\n",
      "Epoch 288/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1558 - acc: 0.9485 - val_loss: 0.1581 - val_acc: 0.9465\n",
      "Epoch 289/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1565 - acc: 0.9494 - val_loss: 0.1554 - val_acc: 0.9453\n",
      "Epoch 290/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1579 - acc: 0.9501 - val_loss: 0.1625 - val_acc: 0.9437\n",
      "Epoch 291/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1623 - acc: 0.9438 - val_loss: 0.1705 - val_acc: 0.9336\n",
      "Epoch 292/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1550 - acc: 0.9516 - val_loss: 0.1537 - val_acc: 0.9581\n",
      "Epoch 293/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1553 - acc: 0.9502 - val_loss: 0.1542 - val_acc: 0.9525\n",
      "Epoch 294/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1526 - acc: 0.9531 - val_loss: 0.1548 - val_acc: 0.9493\n",
      "Epoch 295/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1546 - acc: 0.9517 - val_loss: 0.1464 - val_acc: 0.9632\n",
      "Epoch 296/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1539 - acc: 0.9526 - val_loss: 0.1534 - val_acc: 0.9535\n",
      "Epoch 297/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1518 - acc: 0.9543 - val_loss: 0.1450 - val_acc: 0.9575\n",
      "Epoch 298/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1501 - acc: 0.9535 - val_loss: 0.1598 - val_acc: 0.9512\n",
      "Epoch 299/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1506 - acc: 0.9546 - val_loss: 0.1454 - val_acc: 0.9623\n",
      "Epoch 300/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1538 - acc: 0.9537 - val_loss: 0.1472 - val_acc: 0.9410\n",
      "17859/17859 [==============================] - 1s 75us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17858 samples, validate on 17858 samples\n",
      "Epoch 1/300\n",
      "17858/17858 [==============================] - 3s 158us/step - loss: 3.3068 - acc: 0.1221 - val_loss: 3.1248 - val_acc: 0.0995\n",
      "Epoch 2/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 2.8798 - acc: 0.1188 - val_loss: 2.6751 - val_acc: 0.1098\n",
      "Epoch 3/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 2.5194 - acc: 0.1969 - val_loss: 2.3363 - val_acc: 0.2721\n",
      "Epoch 4/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 2.1620 - acc: 0.3199 - val_loss: 1.9912 - val_acc: 0.3518\n",
      "Epoch 5/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.8548 - acc: 0.4042 - val_loss: 1.7394 - val_acc: 0.4655\n",
      "Epoch 6/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.6335 - acc: 0.4872 - val_loss: 1.5482 - val_acc: 0.5413\n",
      "Epoch 7/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.4930 - acc: 0.5067 - val_loss: 1.4159 - val_acc: 0.5173\n",
      "Epoch 8/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.3983 - acc: 0.5238 - val_loss: 1.3560 - val_acc: 0.5665\n",
      "Epoch 9/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.3437 - acc: 0.5143 - val_loss: 1.3061 - val_acc: 0.5226\n",
      "Epoch 10/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.3083 - acc: 0.5139 - val_loss: 1.3213 - val_acc: 0.5025\n",
      "Epoch 11/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.2570 - acc: 0.5340 - val_loss: 1.2075 - val_acc: 0.5460\n",
      "Epoch 12/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.2526 - acc: 0.5109 - val_loss: 1.1273 - val_acc: 0.6168\n",
      "Epoch 13/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.1707 - acc: 0.5533 - val_loss: 1.1907 - val_acc: 0.4992\n",
      "Epoch 14/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.0890 - acc: 0.6077 - val_loss: 1.0435 - val_acc: 0.6335\n",
      "Epoch 15/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.0977 - acc: 0.5730 - val_loss: 1.1039 - val_acc: 0.5723\n",
      "Epoch 16/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.0875 - acc: 0.5818 - val_loss: 1.0682 - val_acc: 0.5721\n",
      "Epoch 17/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.0805 - acc: 0.5668 - val_loss: 1.0834 - val_acc: 0.5620\n",
      "Epoch 18/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.0265 - acc: 0.6029 - val_loss: 0.9665 - val_acc: 0.6165\n",
      "Epoch 19/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.0366 - acc: 0.5864 - val_loss: 1.0332 - val_acc: 0.6091\n",
      "Epoch 20/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 1.0047 - acc: 0.5884 - val_loss: 1.0008 - val_acc: 0.6257\n",
      "Epoch 21/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.9848 - acc: 0.5938 - val_loss: 0.8992 - val_acc: 0.6710\n",
      "Epoch 22/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.9231 - acc: 0.6427 - val_loss: 0.8969 - val_acc: 0.6568\n",
      "Epoch 23/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.9093 - acc: 0.6423 - val_loss: 0.9165 - val_acc: 0.6420\n",
      "Epoch 24/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.9045 - acc: 0.6340 - val_loss: 0.9237 - val_acc: 0.5951\n",
      "Epoch 25/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.9285 - acc: 0.6106 - val_loss: 0.9582 - val_acc: 0.5678\n",
      "Epoch 26/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8654 - acc: 0.6504 - val_loss: 0.9987 - val_acc: 0.6044\n",
      "Epoch 27/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8295 - acc: 0.6744 - val_loss: 0.8795 - val_acc: 0.6081\n",
      "Epoch 28/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8347 - acc: 0.6616 - val_loss: 0.7716 - val_acc: 0.7396\n",
      "Epoch 29/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8047 - acc: 0.6864 - val_loss: 0.8006 - val_acc: 0.6822\n",
      "Epoch 30/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.7829 - acc: 0.6991 - val_loss: 0.7663 - val_acc: 0.6949\n",
      "Epoch 31/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.7713 - acc: 0.7042 - val_loss: 0.6561 - val_acc: 0.8155\n",
      "Epoch 32/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.6761 - acc: 0.7676 - val_loss: 0.7156 - val_acc: 0.7221\n",
      "Epoch 33/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.6801 - acc: 0.7629 - val_loss: 0.6416 - val_acc: 0.7984\n",
      "Epoch 34/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.6275 - acc: 0.8030 - val_loss: 0.6477 - val_acc: 0.7953\n",
      "Epoch 35/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.6248 - acc: 0.8000 - val_loss: 0.6346 - val_acc: 0.8108\n",
      "Epoch 36/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5962 - acc: 0.8174 - val_loss: 0.5780 - val_acc: 0.8294\n",
      "Epoch 37/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5841 - acc: 0.8207 - val_loss: 0.5827 - val_acc: 0.8352\n",
      "Epoch 38/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5682 - acc: 0.8306 - val_loss: 0.5704 - val_acc: 0.8186\n",
      "Epoch 39/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5577 - acc: 0.8294 - val_loss: 0.5521 - val_acc: 0.8313\n",
      "Epoch 40/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5500 - acc: 0.8326 - val_loss: 0.5452 - val_acc: 0.8197\n",
      "Epoch 41/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5382 - acc: 0.8351 - val_loss: 0.5415 - val_acc: 0.8165\n",
      "Epoch 42/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5296 - acc: 0.8386 - val_loss: 0.5183 - val_acc: 0.8470\n",
      "Epoch 43/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5203 - acc: 0.8392 - val_loss: 0.5104 - val_acc: 0.8466\n",
      "Epoch 44/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5088 - acc: 0.8456 - val_loss: 0.5083 - val_acc: 0.8328\n",
      "Epoch 45/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4993 - acc: 0.8463 - val_loss: 0.4959 - val_acc: 0.8366\n",
      "Epoch 46/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4949 - acc: 0.8474 - val_loss: 0.4968 - val_acc: 0.8467\n",
      "Epoch 47/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4868 - acc: 0.8526 - val_loss: 0.4809 - val_acc: 0.8578\n",
      "Epoch 48/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4798 - acc: 0.8510 - val_loss: 0.4755 - val_acc: 0.8510\n",
      "Epoch 49/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4726 - acc: 0.8544 - val_loss: 0.4639 - val_acc: 0.8580\n",
      "Epoch 50/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4674 - acc: 0.8509 - val_loss: 0.4649 - val_acc: 0.8508\n",
      "Epoch 51/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4597 - acc: 0.8554 - val_loss: 0.4648 - val_acc: 0.8346\n",
      "Epoch 52/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4539 - acc: 0.8582 - val_loss: 0.4509 - val_acc: 0.8701\n",
      "Epoch 53/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4482 - acc: 0.8598 - val_loss: 0.4399 - val_acc: 0.8603\n",
      "Epoch 54/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4425 - acc: 0.8611 - val_loss: 0.4413 - val_acc: 0.8632\n",
      "Epoch 55/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.4375 - acc: 0.8601 - val_loss: 0.4303 - val_acc: 0.8650\n",
      "Epoch 56/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4314 - acc: 0.8607 - val_loss: 0.4357 - val_acc: 0.8640\n",
      "Epoch 57/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4282 - acc: 0.8599 - val_loss: 0.4263 - val_acc: 0.8620\n",
      "Epoch 58/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4231 - acc: 0.8650 - val_loss: 0.4167 - val_acc: 0.8654\n",
      "Epoch 59/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.4166 - acc: 0.8649 - val_loss: 0.4176 - val_acc: 0.8489\n",
      "Epoch 60/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4129 - acc: 0.8657 - val_loss: 0.4138 - val_acc: 0.8579\n",
      "Epoch 61/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8660 - val_loss: 0.4064 - val_acc: 0.8673\n",
      "Epoch 62/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.4036 - acc: 0.8675 - val_loss: 0.3977 - val_acc: 0.8687\n",
      "Epoch 63/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.3999 - acc: 0.8681 - val_loss: 0.4063 - val_acc: 0.8679\n",
      "Epoch 64/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3965 - acc: 0.8691 - val_loss: 0.3924 - val_acc: 0.8691\n",
      "Epoch 65/300\n",
      "17858/17858 [==============================] - ETA: 0s - loss: 0.3914 - acc: 0.871 - 0s 18us/step - loss: 0.3918 - acc: 0.8702 - val_loss: 0.3923 - val_acc: 0.8668\n",
      "Epoch 66/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3914 - acc: 0.8696 - val_loss: 0.3832 - val_acc: 0.8707\n",
      "Epoch 67/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3864 - acc: 0.8691 - val_loss: 0.3859 - val_acc: 0.8659\n",
      "Epoch 68/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3817 - acc: 0.8715 - val_loss: 0.3835 - val_acc: 0.8644\n",
      "Epoch 69/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3794 - acc: 0.8701 - val_loss: 0.3786 - val_acc: 0.8783\n",
      "Epoch 70/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3734 - acc: 0.8734 - val_loss: 0.3708 - val_acc: 0.8733\n",
      "Epoch 71/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3712 - acc: 0.8748 - val_loss: 0.3652 - val_acc: 0.8778\n",
      "Epoch 72/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3679 - acc: 0.8736 - val_loss: 0.3605 - val_acc: 0.8785\n",
      "Epoch 73/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3651 - acc: 0.8765 - val_loss: 0.3626 - val_acc: 0.8901\n",
      "Epoch 74/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3613 - acc: 0.8760 - val_loss: 0.3538 - val_acc: 0.8770\n",
      "Epoch 75/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3570 - acc: 0.8798 - val_loss: 0.3528 - val_acc: 0.8902\n",
      "Epoch 76/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3541 - acc: 0.8795 - val_loss: 0.3546 - val_acc: 0.8770\n",
      "Epoch 77/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3517 - acc: 0.8793 - val_loss: 0.3485 - val_acc: 0.8739\n",
      "Epoch 78/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3489 - acc: 0.8806 - val_loss: 0.3519 - val_acc: 0.9030\n",
      "Epoch 79/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.3464 - acc: 0.8839 - val_loss: 0.3427 - val_acc: 0.8772\n",
      "Epoch 80/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3440 - acc: 0.8818 - val_loss: 0.3392 - val_acc: 0.8904\n",
      "Epoch 81/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3413 - acc: 0.8818 - val_loss: 0.3395 - val_acc: 0.9036\n",
      "Epoch 82/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3370 - acc: 0.8880 - val_loss: 0.3360 - val_acc: 0.8755\n",
      "Epoch 83/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3350 - acc: 0.8834 - val_loss: 0.3367 - val_acc: 0.8858\n",
      "Epoch 84/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3324 - acc: 0.8857 - val_loss: 0.3279 - val_acc: 0.8844\n",
      "Epoch 85/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3302 - acc: 0.8868 - val_loss: 0.3296 - val_acc: 0.8839\n",
      "Epoch 86/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3285 - acc: 0.8893 - val_loss: 0.3254 - val_acc: 0.9033\n",
      "Epoch 87/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3266 - acc: 0.8869 - val_loss: 0.3195 - val_acc: 0.8856\n",
      "Epoch 88/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3235 - acc: 0.8891 - val_loss: 0.3189 - val_acc: 0.9035\n",
      "Epoch 89/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3213 - acc: 0.8918 - val_loss: 0.3185 - val_acc: 0.9043\n",
      "Epoch 90/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3213 - acc: 0.8903 - val_loss: 0.3200 - val_acc: 0.8948\n",
      "Epoch 91/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3177 - acc: 0.8919 - val_loss: 0.3125 - val_acc: 0.8821\n",
      "Epoch 92/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3132 - acc: 0.8917 - val_loss: 0.3104 - val_acc: 0.9002\n",
      "Epoch 93/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3127 - acc: 0.8925 - val_loss: 0.3139 - val_acc: 0.9042\n",
      "Epoch 94/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3116 - acc: 0.8941 - val_loss: 0.3210 - val_acc: 0.8848\n",
      "Epoch 95/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3076 - acc: 0.8946 - val_loss: 0.3069 - val_acc: 0.9145\n",
      "Epoch 96/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3073 - acc: 0.8938 - val_loss: 0.3065 - val_acc: 0.9061\n",
      "Epoch 97/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3057 - acc: 0.8945 - val_loss: 0.3003 - val_acc: 0.9011\n",
      "Epoch 98/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.3025 - acc: 0.8973 - val_loss: 0.3068 - val_acc: 0.8790\n",
      "Epoch 99/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2998 - acc: 0.8944 - val_loss: 0.3031 - val_acc: 0.9086\n",
      "Epoch 100/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3005 - acc: 0.8972 - val_loss: 0.2936 - val_acc: 0.9100\n",
      "Epoch 101/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.3000 - acc: 0.8991 - val_loss: 0.3021 - val_acc: 0.8939\n",
      "Epoch 102/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2926 - acc: 0.9010 - val_loss: 0.2915 - val_acc: 0.9056\n",
      "Epoch 103/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2923 - acc: 0.9011 - val_loss: 0.2905 - val_acc: 0.9103\n",
      "Epoch 104/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2920 - acc: 0.8985 - val_loss: 0.2876 - val_acc: 0.9098\n",
      "Epoch 105/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2897 - acc: 0.8997 - val_loss: 0.2832 - val_acc: 0.8916\n",
      "Epoch 106/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2881 - acc: 0.8987 - val_loss: 0.2810 - val_acc: 0.9046\n",
      "Epoch 107/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.2868 - acc: 0.9012 - val_loss: 0.2861 - val_acc: 0.9121\n",
      "Epoch 108/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2859 - acc: 0.9008 - val_loss: 0.2809 - val_acc: 0.9013\n",
      "Epoch 109/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2833 - acc: 0.9045 - val_loss: 0.2772 - val_acc: 0.9097\n",
      "Epoch 110/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.2829 - acc: 0.9041 - val_loss: 0.2760 - val_acc: 0.9093\n",
      "Epoch 111/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2828 - acc: 0.9025 - val_loss: 0.2791 - val_acc: 0.9109\n",
      "Epoch 112/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2765 - acc: 0.9055 - val_loss: 0.2715 - val_acc: 0.9123\n",
      "Epoch 113/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2776 - acc: 0.9058 - val_loss: 0.2801 - val_acc: 0.9002\n",
      "Epoch 114/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2749 - acc: 0.9070 - val_loss: 0.2776 - val_acc: 0.9111\n",
      "Epoch 115/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2733 - acc: 0.9058 - val_loss: 0.2720 - val_acc: 0.9092\n",
      "Epoch 116/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2728 - acc: 0.9062 - val_loss: 0.2747 - val_acc: 0.9147\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2723 - acc: 0.9075 - val_loss: 0.2705 - val_acc: 0.9139\n",
      "Epoch 118/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2706 - acc: 0.9061 - val_loss: 0.2712 - val_acc: 0.9075\n",
      "Epoch 119/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2692 - acc: 0.9066 - val_loss: 0.2615 - val_acc: 0.9163\n",
      "Epoch 120/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2669 - acc: 0.9070 - val_loss: 0.2709 - val_acc: 0.8904\n",
      "Epoch 121/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2644 - acc: 0.9096 - val_loss: 0.2687 - val_acc: 0.9186\n",
      "Epoch 122/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2646 - acc: 0.9112 - val_loss: 0.2571 - val_acc: 0.9184\n",
      "Epoch 123/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2614 - acc: 0.9124 - val_loss: 0.2580 - val_acc: 0.9175\n",
      "Epoch 124/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2621 - acc: 0.9081 - val_loss: 0.2594 - val_acc: 0.9215\n",
      "Epoch 125/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2605 - acc: 0.9151 - val_loss: 0.2590 - val_acc: 0.9123\n",
      "Epoch 126/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2576 - acc: 0.9120 - val_loss: 0.2593 - val_acc: 0.8961\n",
      "Epoch 127/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2590 - acc: 0.9123 - val_loss: 0.2526 - val_acc: 0.9197\n",
      "Epoch 128/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2553 - acc: 0.9124 - val_loss: 0.2572 - val_acc: 0.9283\n",
      "Epoch 129/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2571 - acc: 0.9113 - val_loss: 0.2543 - val_acc: 0.9142\n",
      "Epoch 130/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2530 - acc: 0.9141 - val_loss: 0.2553 - val_acc: 0.9307\n",
      "Epoch 131/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2525 - acc: 0.9146 - val_loss: 0.2540 - val_acc: 0.9014\n",
      "Epoch 132/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2510 - acc: 0.9139 - val_loss: 0.2500 - val_acc: 0.9091\n",
      "Epoch 133/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2488 - acc: 0.9143 - val_loss: 0.2543 - val_acc: 0.9008\n",
      "Epoch 134/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2495 - acc: 0.9171 - val_loss: 0.2645 - val_acc: 0.9035\n",
      "Epoch 135/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2460 - acc: 0.9163 - val_loss: 0.2471 - val_acc: 0.9149\n",
      "Epoch 136/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.2468 - acc: 0.9159 - val_loss: 0.2476 - val_acc: 0.9195\n",
      "Epoch 137/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2472 - acc: 0.9154 - val_loss: 0.2431 - val_acc: 0.9196\n",
      "Epoch 138/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2444 - acc: 0.9158 - val_loss: 0.2403 - val_acc: 0.9254\n",
      "Epoch 139/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2421 - acc: 0.9179 - val_loss: 0.2410 - val_acc: 0.9209\n",
      "Epoch 140/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2413 - acc: 0.9167 - val_loss: 0.2414 - val_acc: 0.9154\n",
      "Epoch 141/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2427 - acc: 0.9182 - val_loss: 0.2410 - val_acc: 0.9199\n",
      "Epoch 142/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2396 - acc: 0.9181 - val_loss: 0.2477 - val_acc: 0.9077\n",
      "Epoch 143/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2392 - acc: 0.9171 - val_loss: 0.2365 - val_acc: 0.9179\n",
      "Epoch 144/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2364 - acc: 0.9186 - val_loss: 0.2465 - val_acc: 0.9213\n",
      "Epoch 145/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2358 - acc: 0.9196 - val_loss: 0.2358 - val_acc: 0.9083\n",
      "Epoch 146/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2380 - acc: 0.9158 - val_loss: 0.2327 - val_acc: 0.9193\n",
      "Epoch 147/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2334 - acc: 0.9218 - val_loss: 0.2283 - val_acc: 0.9303\n",
      "Epoch 148/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2343 - acc: 0.9193 - val_loss: 0.2411 - val_acc: 0.9064\n",
      "Epoch 149/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2324 - acc: 0.9214 - val_loss: 0.2255 - val_acc: 0.9242\n",
      "Epoch 150/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2336 - acc: 0.9191 - val_loss: 0.2411 - val_acc: 0.9061\n",
      "Epoch 151/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2334 - acc: 0.9169 - val_loss: 0.2361 - val_acc: 0.9243\n",
      "Epoch 152/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2297 - acc: 0.9233 - val_loss: 0.2259 - val_acc: 0.9280\n",
      "Epoch 153/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2281 - acc: 0.9232 - val_loss: 0.2251 - val_acc: 0.9192\n",
      "Epoch 154/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2270 - acc: 0.9224 - val_loss: 0.2298 - val_acc: 0.9177\n",
      "Epoch 155/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2256 - acc: 0.9241 - val_loss: 0.2281 - val_acc: 0.9005\n",
      "Epoch 156/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2248 - acc: 0.9250 - val_loss: 0.2284 - val_acc: 0.9242\n",
      "Epoch 157/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2239 - acc: 0.9265 - val_loss: 0.2193 - val_acc: 0.9321\n",
      "Epoch 158/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2212 - acc: 0.9289 - val_loss: 0.2296 - val_acc: 0.9181\n",
      "Epoch 159/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2248 - acc: 0.9199 - val_loss: 0.2168 - val_acc: 0.9347\n",
      "Epoch 160/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2201 - acc: 0.9260 - val_loss: 0.2168 - val_acc: 0.9259\n",
      "Epoch 161/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2209 - acc: 0.9250 - val_loss: 0.2168 - val_acc: 0.9250\n",
      "Epoch 162/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2209 - acc: 0.9252 - val_loss: 0.2359 - val_acc: 0.9206\n",
      "Epoch 163/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2194 - acc: 0.9257 - val_loss: 0.2192 - val_acc: 0.9197\n",
      "Epoch 164/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2212 - acc: 0.9242 - val_loss: 0.2119 - val_acc: 0.9305\n",
      "Epoch 165/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2174 - acc: 0.9289 - val_loss: 0.2366 - val_acc: 0.9191\n",
      "Epoch 166/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2194 - acc: 0.9231 - val_loss: 0.2191 - val_acc: 0.9293\n",
      "Epoch 167/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2147 - acc: 0.9267 - val_loss: 0.2112 - val_acc: 0.9335\n",
      "Epoch 168/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.2130 - acc: 0.9304 - val_loss: 0.2099 - val_acc: 0.9207\n",
      "Epoch 169/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2180 - acc: 0.9234 - val_loss: 0.2054 - val_acc: 0.9372\n",
      "Epoch 170/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2101 - acc: 0.9296 - val_loss: 0.2083 - val_acc: 0.9324\n",
      "Epoch 171/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2106 - acc: 0.9305 - val_loss: 0.2111 - val_acc: 0.9320\n",
      "Epoch 172/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2097 - acc: 0.9330 - val_loss: 0.2064 - val_acc: 0.9276\n",
      "Epoch 173/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2106 - acc: 0.9314 - val_loss: 0.2066 - val_acc: 0.9348\n",
      "Epoch 174/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2094 - acc: 0.9317 - val_loss: 0.2142 - val_acc: 0.9277\n",
      "Epoch 175/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2107 - acc: 0.9285 - val_loss: 0.2079 - val_acc: 0.9270\n",
      "Epoch 176/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2100 - acc: 0.9276 - val_loss: 0.2030 - val_acc: 0.9392\n",
      "Epoch 177/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.2077 - acc: 0.9297 - val_loss: 0.2067 - val_acc: 0.9210\n",
      "Epoch 178/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.2041 - acc: 0.9339 - val_loss: 0.2034 - val_acc: 0.9375\n",
      "Epoch 179/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2041 - acc: 0.9322 - val_loss: 0.2035 - val_acc: 0.9284\n",
      "Epoch 180/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2045 - acc: 0.9324 - val_loss: 0.2062 - val_acc: 0.9246\n",
      "Epoch 181/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2047 - acc: 0.9310 - val_loss: 0.2040 - val_acc: 0.9394\n",
      "Epoch 182/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2088 - acc: 0.9290 - val_loss: 0.2016 - val_acc: 0.9444\n",
      "Epoch 183/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2017 - acc: 0.9339 - val_loss: 0.1989 - val_acc: 0.9446\n",
      "Epoch 184/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2041 - acc: 0.9326 - val_loss: 0.2026 - val_acc: 0.9466\n",
      "Epoch 185/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2009 - acc: 0.9344 - val_loss: 0.1970 - val_acc: 0.9348\n",
      "Epoch 186/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2031 - acc: 0.9295 - val_loss: 0.1999 - val_acc: 0.9335\n",
      "Epoch 187/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2027 - acc: 0.9299 - val_loss: 0.1955 - val_acc: 0.9309\n",
      "Epoch 188/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1996 - acc: 0.9309 - val_loss: 0.1947 - val_acc: 0.9287\n",
      "Epoch 189/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1959 - acc: 0.9372 - val_loss: 0.2075 - val_acc: 0.9448\n",
      "Epoch 190/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2003 - acc: 0.9335 - val_loss: 0.2072 - val_acc: 0.9236\n",
      "Epoch 191/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1971 - acc: 0.9340 - val_loss: 0.1999 - val_acc: 0.9221\n",
      "Epoch 192/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1940 - acc: 0.9368 - val_loss: 0.1985 - val_acc: 0.9326\n",
      "Epoch 193/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1971 - acc: 0.9346 - val_loss: 0.1944 - val_acc: 0.9378\n",
      "Epoch 194/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1978 - acc: 0.9356 - val_loss: 0.2094 - val_acc: 0.9149\n",
      "Epoch 195/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1969 - acc: 0.9333 - val_loss: 0.2102 - val_acc: 0.9194\n",
      "Epoch 196/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.1977 - acc: 0.9313 - val_loss: 0.1870 - val_acc: 0.9401\n",
      "Epoch 197/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1902 - acc: 0.9405 - val_loss: 0.1885 - val_acc: 0.9513\n",
      "Epoch 198/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1919 - acc: 0.9355 - val_loss: 0.1877 - val_acc: 0.9504\n",
      "Epoch 199/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1910 - acc: 0.9371 - val_loss: 0.1845 - val_acc: 0.9430\n",
      "Epoch 200/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1911 - acc: 0.9381 - val_loss: 0.1943 - val_acc: 0.9275\n",
      "Epoch 201/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1897 - acc: 0.9374 - val_loss: 0.1839 - val_acc: 0.9422\n",
      "Epoch 202/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1893 - acc: 0.9394 - val_loss: 0.2026 - val_acc: 0.9229\n",
      "Epoch 203/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1876 - acc: 0.9377 - val_loss: 0.1864 - val_acc: 0.9433\n",
      "Epoch 204/300\n",
      "17858/17858 [==============================] - 0s 14us/step - loss: 0.1879 - acc: 0.9385 - val_loss: 0.1914 - val_acc: 0.9390\n",
      "Epoch 205/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1885 - acc: 0.9353 - val_loss: 0.1846 - val_acc: 0.9485\n",
      "Epoch 206/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1849 - acc: 0.9401 - val_loss: 0.1896 - val_acc: 0.9378\n",
      "Epoch 207/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1871 - acc: 0.9382 - val_loss: 0.1815 - val_acc: 0.9407\n",
      "Epoch 208/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1848 - acc: 0.9402 - val_loss: 0.1786 - val_acc: 0.9436\n",
      "Epoch 209/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1820 - acc: 0.9422 - val_loss: 0.1904 - val_acc: 0.9409\n",
      "Epoch 210/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1840 - acc: 0.9401 - val_loss: 0.1778 - val_acc: 0.9462\n",
      "Epoch 211/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1819 - acc: 0.9415 - val_loss: 0.1802 - val_acc: 0.9455\n",
      "Epoch 212/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1830 - acc: 0.9413 - val_loss: 0.1803 - val_acc: 0.9428\n",
      "Epoch 213/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1815 - acc: 0.9419 - val_loss: 0.1906 - val_acc: 0.9427\n",
      "Epoch 214/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1841 - acc: 0.9390 - val_loss: 0.1756 - val_acc: 0.9447\n",
      "Epoch 215/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1782 - acc: 0.9439 - val_loss: 0.1767 - val_acc: 0.9462\n",
      "Epoch 216/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1783 - acc: 0.9433 - val_loss: 0.1770 - val_acc: 0.9391\n",
      "Epoch 217/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1800 - acc: 0.9425 - val_loss: 0.1755 - val_acc: 0.9482\n",
      "Epoch 218/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1793 - acc: 0.9445 - val_loss: 0.1799 - val_acc: 0.9413\n",
      "Epoch 219/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1793 - acc: 0.9420 - val_loss: 0.1772 - val_acc: 0.9396\n",
      "Epoch 220/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1783 - acc: 0.9433 - val_loss: 0.1746 - val_acc: 0.9415\n",
      "Epoch 221/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1772 - acc: 0.9413 - val_loss: 0.1857 - val_acc: 0.9373\n",
      "Epoch 222/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1771 - acc: 0.9439 - val_loss: 0.1771 - val_acc: 0.9553\n",
      "Epoch 223/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1774 - acc: 0.9428 - val_loss: 0.1783 - val_acc: 0.9400\n",
      "Epoch 224/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1772 - acc: 0.9422 - val_loss: 0.1717 - val_acc: 0.9442\n",
      "Epoch 225/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1796 - acc: 0.9407 - val_loss: 0.1664 - val_acc: 0.9476\n",
      "Epoch 226/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1712 - acc: 0.9473 - val_loss: 0.1704 - val_acc: 0.9424\n",
      "Epoch 227/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.1751 - acc: 0.9434 - val_loss: 0.1717 - val_acc: 0.9406\n",
      "Epoch 228/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1716 - acc: 0.9461 - val_loss: 0.1902 - val_acc: 0.9440\n",
      "Epoch 229/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1725 - acc: 0.9467 - val_loss: 0.1768 - val_acc: 0.9292\n",
      "Epoch 230/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1749 - acc: 0.9424 - val_loss: 0.1726 - val_acc: 0.9540\n",
      "Epoch 231/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1731 - acc: 0.9441 - val_loss: 0.1754 - val_acc: 0.9427\n",
      "Epoch 232/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1718 - acc: 0.9478 - val_loss: 0.1749 - val_acc: 0.9354\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1743 - acc: 0.9442 - val_loss: 0.1643 - val_acc: 0.9455\n",
      "Epoch 234/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1677 - acc: 0.9492 - val_loss: 0.1639 - val_acc: 0.9477\n",
      "Epoch 235/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1671 - acc: 0.9501 - val_loss: 0.1675 - val_acc: 0.9414\n",
      "Epoch 236/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1726 - acc: 0.9437 - val_loss: 0.1683 - val_acc: 0.9615\n",
      "Epoch 237/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1660 - acc: 0.9493 - val_loss: 0.1608 - val_acc: 0.9518\n",
      "Epoch 238/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1682 - acc: 0.9467 - val_loss: 0.1639 - val_acc: 0.9478\n",
      "Epoch 239/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1703 - acc: 0.9455 - val_loss: 0.1791 - val_acc: 0.9371\n",
      "Epoch 240/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1689 - acc: 0.9466 - val_loss: 0.1697 - val_acc: 0.9411\n",
      "Epoch 241/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1642 - acc: 0.9511 - val_loss: 0.1662 - val_acc: 0.9363\n",
      "Epoch 242/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1643 - acc: 0.9484 - val_loss: 0.1644 - val_acc: 0.9471\n",
      "Epoch 243/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1640 - acc: 0.9490 - val_loss: 0.1644 - val_acc: 0.9509\n",
      "Epoch 244/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1652 - acc: 0.9479 - val_loss: 0.1645 - val_acc: 0.9536\n",
      "Epoch 245/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1614 - acc: 0.9516 - val_loss: 0.1650 - val_acc: 0.9522\n",
      "Epoch 246/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1649 - acc: 0.9492 - val_loss: 0.1793 - val_acc: 0.9266\n",
      "Epoch 247/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1715 - acc: 0.9423 - val_loss: 0.1608 - val_acc: 0.9604\n",
      "Epoch 248/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1594 - acc: 0.9527 - val_loss: 0.1560 - val_acc: 0.9520\n",
      "Epoch 249/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1630 - acc: 0.9484 - val_loss: 0.1601 - val_acc: 0.9446\n",
      "Epoch 250/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1666 - acc: 0.9481 - val_loss: 0.1570 - val_acc: 0.9534\n",
      "Epoch 251/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1603 - acc: 0.9529 - val_loss: 0.1556 - val_acc: 0.9540\n",
      "Epoch 252/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1660 - acc: 0.9454 - val_loss: 0.1751 - val_acc: 0.9588\n",
      "Epoch 253/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1610 - acc: 0.9504 - val_loss: 0.1636 - val_acc: 0.9624\n",
      "Epoch 254/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1588 - acc: 0.9498 - val_loss: 0.1598 - val_acc: 0.9552\n",
      "Epoch 255/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1608 - acc: 0.9501 - val_loss: 0.1526 - val_acc: 0.9597\n",
      "Epoch 256/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1557 - acc: 0.9543 - val_loss: 0.1560 - val_acc: 0.9484\n",
      "Epoch 257/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1571 - acc: 0.9517 - val_loss: 0.1515 - val_acc: 0.9560\n",
      "Epoch 258/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1571 - acc: 0.9527 - val_loss: 0.1657 - val_acc: 0.9455\n",
      "Epoch 259/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1606 - acc: 0.9494 - val_loss: 0.1579 - val_acc: 0.9507\n",
      "Epoch 260/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1524 - acc: 0.9553 - val_loss: 0.1534 - val_acc: 0.9527\n",
      "Epoch 261/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1577 - acc: 0.9504 - val_loss: 0.1559 - val_acc: 0.9605\n",
      "Epoch 262/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1537 - acc: 0.9535 - val_loss: 0.1510 - val_acc: 0.9537\n",
      "Epoch 263/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.1520 - acc: 0.9547 - val_loss: 0.1565 - val_acc: 0.9598\n",
      "Epoch 264/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1573 - acc: 0.9508 - val_loss: 0.1599 - val_acc: 0.9517\n",
      "Epoch 265/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1536 - acc: 0.9531 - val_loss: 0.1504 - val_acc: 0.9450\n",
      "Epoch 266/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1553 - acc: 0.9520 - val_loss: 0.1525 - val_acc: 0.9609\n",
      "Epoch 267/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1566 - acc: 0.9501 - val_loss: 0.1458 - val_acc: 0.9535\n",
      "Epoch 268/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1510 - acc: 0.9542 - val_loss: 0.1653 - val_acc: 0.9518\n",
      "Epoch 269/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1535 - acc: 0.9523 - val_loss: 0.1536 - val_acc: 0.9568\n",
      "Epoch 270/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1533 - acc: 0.9537 - val_loss: 0.1500 - val_acc: 0.9519\n",
      "Epoch 271/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1568 - acc: 0.9512 - val_loss: 0.1579 - val_acc: 0.9603\n",
      "Epoch 272/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1506 - acc: 0.9538 - val_loss: 0.1539 - val_acc: 0.9633\n",
      "Epoch 273/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1566 - acc: 0.9507 - val_loss: 0.1482 - val_acc: 0.9656\n",
      "Epoch 274/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1471 - acc: 0.9579 - val_loss: 0.1560 - val_acc: 0.9418\n",
      "Epoch 275/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1524 - acc: 0.9536 - val_loss: 0.1440 - val_acc: 0.9647\n",
      "Epoch 276/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1489 - acc: 0.9557 - val_loss: 0.1474 - val_acc: 0.9491\n",
      "Epoch 277/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1443 - acc: 0.9584 - val_loss: 0.1451 - val_acc: 0.9633\n",
      "Epoch 278/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1485 - acc: 0.9561 - val_loss: 0.1413 - val_acc: 0.9643\n",
      "Epoch 279/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1467 - acc: 0.9577 - val_loss: 0.1436 - val_acc: 0.9668\n",
      "Epoch 280/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1487 - acc: 0.9532 - val_loss: 0.1399 - val_acc: 0.9666\n",
      "Epoch 281/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1448 - acc: 0.9578 - val_loss: 0.1497 - val_acc: 0.9639\n",
      "Epoch 282/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1506 - acc: 0.9530 - val_loss: 0.1435 - val_acc: 0.9569\n",
      "Epoch 283/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1466 - acc: 0.9572 - val_loss: 0.1424 - val_acc: 0.9534\n",
      "Epoch 284/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1465 - acc: 0.9559 - val_loss: 0.1409 - val_acc: 0.9647\n",
      "Epoch 285/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1483 - acc: 0.9543 - val_loss: 0.1683 - val_acc: 0.9377\n",
      "Epoch 286/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1448 - acc: 0.9572 - val_loss: 0.1401 - val_acc: 0.9584\n",
      "Epoch 287/300\n",
      "17858/17858 [==============================] - ETA: 0s - loss: 0.1448 - acc: 0.956 - 0s 18us/step - loss: 0.1438 - acc: 0.9568 - val_loss: 0.1483 - val_acc: 0.9631\n",
      "Epoch 288/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1432 - acc: 0.9598 - val_loss: 0.1399 - val_acc: 0.9558\n",
      "Epoch 289/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1470 - acc: 0.9543 - val_loss: 0.1381 - val_acc: 0.9598\n",
      "Epoch 290/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1402 - acc: 0.9607 - val_loss: 0.1351 - val_acc: 0.9648\n",
      "Epoch 291/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1429 - acc: 0.9584 - val_loss: 0.1352 - val_acc: 0.9618\n",
      "Epoch 292/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1499 - acc: 0.9538 - val_loss: 0.1629 - val_acc: 0.9350\n",
      "Epoch 293/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1420 - acc: 0.9590 - val_loss: 0.1395 - val_acc: 0.9531\n",
      "Epoch 294/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1402 - acc: 0.9582 - val_loss: 0.1376 - val_acc: 0.9557\n",
      "Epoch 295/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1379 - acc: 0.9612 - val_loss: 0.1495 - val_acc: 0.9565\n",
      "Epoch 296/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.1410 - acc: 0.9574 - val_loss: 0.1374 - val_acc: 0.9677\n",
      "Epoch 297/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1422 - acc: 0.9579 - val_loss: 0.1392 - val_acc: 0.9615\n",
      "Epoch 298/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1436 - acc: 0.9557 - val_loss: 0.1442 - val_acc: 0.9636\n",
      "Epoch 299/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1440 - acc: 0.9564 - val_loss: 0.1323 - val_acc: 0.9618\n",
      "Epoch 300/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1439 - acc: 0.9571 - val_loss: 0.1436 - val_acc: 0.9668\n",
      "17859/17859 [==============================] - 1s 72us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17858 samples, validate on 17858 samples\n",
      "Epoch 1/300\n",
      "17858/17858 [==============================] - 3s 159us/step - loss: 3.2781 - acc: 0.0980 - val_loss: 3.0719 - val_acc: 0.1193\n",
      "Epoch 2/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 2.8796 - acc: 0.1517 - val_loss: 2.6610 - val_acc: 0.1845\n",
      "Epoch 3/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 2.4416 - acc: 0.2476 - val_loss: 2.2200 - val_acc: 0.3431\n",
      "Epoch 4/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 2.0586 - acc: 0.3612 - val_loss: 1.8982 - val_acc: 0.4121\n",
      "Epoch 5/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.7981 - acc: 0.4216 - val_loss: 1.7173 - val_acc: 0.4208\n",
      "Epoch 6/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.6104 - acc: 0.4762 - val_loss: 1.5296 - val_acc: 0.5274\n",
      "Epoch 7/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.4808 - acc: 0.5123 - val_loss: 1.4473 - val_acc: 0.4914\n",
      "Epoch 8/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.3801 - acc: 0.5308 - val_loss: 1.3682 - val_acc: 0.4900\n",
      "Epoch 9/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 1.3269 - acc: 0.5227 - val_loss: 1.3235 - val_acc: 0.4866\n",
      "Epoch 10/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 1.2689 - acc: 0.5278 - val_loss: 1.2945 - val_acc: 0.5217\n",
      "Epoch 11/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.2383 - acc: 0.5344 - val_loss: 1.1673 - val_acc: 0.6048\n",
      "Epoch 12/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.1844 - acc: 0.5580 - val_loss: 1.2578 - val_acc: 0.5001\n",
      "Epoch 13/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 1.1739 - acc: 0.5448 - val_loss: 1.1372 - val_acc: 0.5849\n",
      "Epoch 14/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.1493 - acc: 0.5380 - val_loss: 1.1288 - val_acc: 0.5151\n",
      "Epoch 15/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 1.1035 - acc: 0.5611 - val_loss: 1.0111 - val_acc: 0.6000\n",
      "Epoch 16/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 1.0595 - acc: 0.5942 - val_loss: 1.0163 - val_acc: 0.6084\n",
      "Epoch 17/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 1.0575 - acc: 0.5751 - val_loss: 1.1012 - val_acc: 0.5732\n",
      "Epoch 18/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 1.0472 - acc: 0.5753 - val_loss: 1.0470 - val_acc: 0.5553\n",
      "Epoch 19/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.9920 - acc: 0.6023 - val_loss: 1.0287 - val_acc: 0.5738\n",
      "Epoch 20/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 1.0243 - acc: 0.5757 - val_loss: 0.8783 - val_acc: 0.6649\n",
      "Epoch 21/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8869 - acc: 0.6703 - val_loss: 0.8411 - val_acc: 0.6938\n",
      "Epoch 22/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.9077 - acc: 0.6417 - val_loss: 1.0139 - val_acc: 0.5435\n",
      "Epoch 23/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8778 - acc: 0.6542 - val_loss: 0.8714 - val_acc: 0.6426\n",
      "Epoch 24/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8390 - acc: 0.6739 - val_loss: 0.9098 - val_acc: 0.5751\n",
      "Epoch 25/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.8250 - acc: 0.6861 - val_loss: 0.7593 - val_acc: 0.7550\n",
      "Epoch 26/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.8573 - acc: 0.6631 - val_loss: 0.8861 - val_acc: 0.6514\n",
      "Epoch 27/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.8056 - acc: 0.6946 - val_loss: 0.6902 - val_acc: 0.7927\n",
      "Epoch 28/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.6963 - acc: 0.7673 - val_loss: 0.7325 - val_acc: 0.7636\n",
      "Epoch 29/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.7127 - acc: 0.7458 - val_loss: 0.6586 - val_acc: 0.7828\n",
      "Epoch 30/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.6681 - acc: 0.7730 - val_loss: 0.6450 - val_acc: 0.8202\n",
      "Epoch 31/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.6467 - acc: 0.7882 - val_loss: 0.6311 - val_acc: 0.8030\n",
      "Epoch 32/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.6299 - acc: 0.7951 - val_loss: 0.6249 - val_acc: 0.7810\n",
      "Epoch 33/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.6126 - acc: 0.8019 - val_loss: 0.6022 - val_acc: 0.8098\n",
      "Epoch 34/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.6035 - acc: 0.8035 - val_loss: 0.5909 - val_acc: 0.8085\n",
      "Epoch 35/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5905 - acc: 0.8084 - val_loss: 0.5914 - val_acc: 0.7914\n",
      "Epoch 36/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5798 - acc: 0.8159 - val_loss: 0.5769 - val_acc: 0.8145\n",
      "Epoch 37/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5695 - acc: 0.8191 - val_loss: 0.5604 - val_acc: 0.8306\n",
      "Epoch 38/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5569 - acc: 0.8252 - val_loss: 0.5598 - val_acc: 0.8245\n",
      "Epoch 39/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5458 - acc: 0.8297 - val_loss: 0.5354 - val_acc: 0.8314\n",
      "Epoch 40/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5386 - acc: 0.8345 - val_loss: 0.5331 - val_acc: 0.8345\n",
      "Epoch 41/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.5278 - acc: 0.8376 - val_loss: 0.5272 - val_acc: 0.8308\n",
      "Epoch 42/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5190 - acc: 0.8381 - val_loss: 0.5223 - val_acc: 0.8266\n",
      "Epoch 43/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5121 - acc: 0.8404 - val_loss: 0.5078 - val_acc: 0.8433\n",
      "Epoch 44/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.5032 - acc: 0.8457 - val_loss: 0.4979 - val_acc: 0.8375\n",
      "Epoch 45/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4953 - acc: 0.8447 - val_loss: 0.4892 - val_acc: 0.8395\n",
      "Epoch 46/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4872 - acc: 0.8495 - val_loss: 0.4832 - val_acc: 0.8503\n",
      "Epoch 47/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4803 - acc: 0.8495 - val_loss: 0.4817 - val_acc: 0.8527\n",
      "Epoch 48/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4742 - acc: 0.8527 - val_loss: 0.4718 - val_acc: 0.8522\n",
      "Epoch 49/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4661 - acc: 0.8541 - val_loss: 0.4643 - val_acc: 0.8514\n",
      "Epoch 50/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.4599 - acc: 0.8559 - val_loss: 0.4545 - val_acc: 0.8482\n",
      "Epoch 51/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4521 - acc: 0.8586 - val_loss: 0.4487 - val_acc: 0.8633\n",
      "Epoch 52/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.4472 - acc: 0.8596 - val_loss: 0.4428 - val_acc: 0.8593\n",
      "Epoch 53/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.4415 - acc: 0.8608 - val_loss: 0.4389 - val_acc: 0.8599\n",
      "Epoch 54/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4362 - acc: 0.8605 - val_loss: 0.4280 - val_acc: 0.8684\n",
      "Epoch 55/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4279 - acc: 0.8645 - val_loss: 0.4270 - val_acc: 0.8642\n",
      "Epoch 56/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4231 - acc: 0.8652 - val_loss: 0.4293 - val_acc: 0.8622\n",
      "Epoch 57/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4173 - acc: 0.8641 - val_loss: 0.4165 - val_acc: 0.8633\n",
      "Epoch 58/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4135 - acc: 0.8667 - val_loss: 0.4193 - val_acc: 0.8598\n",
      "Epoch 59/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.4075 - acc: 0.8691 - val_loss: 0.4022 - val_acc: 0.8675\n",
      "Epoch 60/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.4017 - acc: 0.8719 - val_loss: 0.4062 - val_acc: 0.8798\n",
      "Epoch 61/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3977 - acc: 0.8703 - val_loss: 0.3946 - val_acc: 0.8732\n",
      "Epoch 62/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3931 - acc: 0.8731 - val_loss: 0.3898 - val_acc: 0.8690\n",
      "Epoch 63/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3884 - acc: 0.8733 - val_loss: 0.3856 - val_acc: 0.8683\n",
      "Epoch 64/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3838 - acc: 0.8749 - val_loss: 0.3819 - val_acc: 0.8784\n",
      "Epoch 65/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3790 - acc: 0.8756 - val_loss: 0.3757 - val_acc: 0.8756\n",
      "Epoch 66/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3744 - acc: 0.8772 - val_loss: 0.3690 - val_acc: 0.8757\n",
      "Epoch 67/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.3706 - acc: 0.8783 - val_loss: 0.3680 - val_acc: 0.8779\n",
      "Epoch 68/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3663 - acc: 0.8798 - val_loss: 0.3650 - val_acc: 0.8712\n",
      "Epoch 69/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3622 - acc: 0.8798 - val_loss: 0.3590 - val_acc: 0.8767\n",
      "Epoch 70/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3577 - acc: 0.8809 - val_loss: 0.3530 - val_acc: 0.8846\n",
      "Epoch 71/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3545 - acc: 0.8797 - val_loss: 0.3502 - val_acc: 0.8831\n",
      "Epoch 72/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3537 - acc: 0.8828 - val_loss: 0.3492 - val_acc: 0.8812\n",
      "Epoch 73/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3450 - acc: 0.8823 - val_loss: 0.3448 - val_acc: 0.8913\n",
      "Epoch 74/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3421 - acc: 0.8873 - val_loss: 0.3376 - val_acc: 0.8823\n",
      "Epoch 75/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3391 - acc: 0.8856 - val_loss: 0.3343 - val_acc: 0.8910\n",
      "Epoch 76/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3357 - acc: 0.8860 - val_loss: 0.3357 - val_acc: 0.8850\n",
      "Epoch 77/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3344 - acc: 0.8867 - val_loss: 0.3291 - val_acc: 0.8841\n",
      "Epoch 78/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3313 - acc: 0.8873 - val_loss: 0.3260 - val_acc: 0.8889\n",
      "Epoch 79/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3254 - acc: 0.8898 - val_loss: 0.3265 - val_acc: 0.8954\n",
      "Epoch 80/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3231 - acc: 0.8886 - val_loss: 0.3226 - val_acc: 0.8817\n",
      "Epoch 81/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3206 - acc: 0.8910 - val_loss: 0.3151 - val_acc: 0.8957\n",
      "Epoch 82/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3179 - acc: 0.8918 - val_loss: 0.3096 - val_acc: 0.8906\n",
      "Epoch 83/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.3137 - acc: 0.8904 - val_loss: 0.3105 - val_acc: 0.8912\n",
      "Epoch 84/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3133 - acc: 0.8924 - val_loss: 0.3073 - val_acc: 0.8941\n",
      "Epoch 85/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3083 - acc: 0.8937 - val_loss: 0.3117 - val_acc: 0.8939\n",
      "Epoch 86/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3064 - acc: 0.8957 - val_loss: 0.3072 - val_acc: 0.8870\n",
      "Epoch 87/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.3053 - acc: 0.8946 - val_loss: 0.2973 - val_acc: 0.8943\n",
      "Epoch 88/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.3030 - acc: 0.8949 - val_loss: 0.3011 - val_acc: 0.9070\n",
      "Epoch 89/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2993 - acc: 0.8969 - val_loss: 0.3002 - val_acc: 0.8851\n",
      "Epoch 90/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2972 - acc: 0.8974 - val_loss: 0.2927 - val_acc: 0.9027\n",
      "Epoch 91/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2925 - acc: 0.9002 - val_loss: 0.2916 - val_acc: 0.9032\n",
      "Epoch 92/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2935 - acc: 0.9014 - val_loss: 0.2937 - val_acc: 0.8858\n",
      "Epoch 93/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2902 - acc: 0.8987 - val_loss: 0.2883 - val_acc: 0.9099\n",
      "Epoch 94/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2857 - acc: 0.9032 - val_loss: 0.2847 - val_acc: 0.8991\n",
      "Epoch 95/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2830 - acc: 0.9032 - val_loss: 0.2920 - val_acc: 0.8994\n",
      "Epoch 96/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2826 - acc: 0.9033 - val_loss: 0.2796 - val_acc: 0.8923\n",
      "Epoch 97/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2800 - acc: 0.9048 - val_loss: 0.2749 - val_acc: 0.9037\n",
      "Epoch 98/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2781 - acc: 0.9047 - val_loss: 0.2705 - val_acc: 0.9085\n",
      "Epoch 99/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2758 - acc: 0.9037 - val_loss: 0.2793 - val_acc: 0.8910\n",
      "Epoch 100/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2796 - acc: 0.9004 - val_loss: 0.2692 - val_acc: 0.9000\n",
      "Epoch 101/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2715 - acc: 0.9051 - val_loss: 0.2670 - val_acc: 0.9017\n",
      "Epoch 102/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2718 - acc: 0.9048 - val_loss: 0.2785 - val_acc: 0.8916\n",
      "Epoch 103/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2671 - acc: 0.9067 - val_loss: 0.2650 - val_acc: 0.9135\n",
      "Epoch 104/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2676 - acc: 0.9059 - val_loss: 0.2644 - val_acc: 0.9079\n",
      "Epoch 105/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2643 - acc: 0.9103 - val_loss: 0.2656 - val_acc: 0.9005\n",
      "Epoch 106/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2640 - acc: 0.9091 - val_loss: 0.2702 - val_acc: 0.8972\n",
      "Epoch 107/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2602 - acc: 0.9095 - val_loss: 0.2738 - val_acc: 0.9167\n",
      "Epoch 108/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2597 - acc: 0.9105 - val_loss: 0.2668 - val_acc: 0.8935\n",
      "Epoch 109/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2563 - acc: 0.9105 - val_loss: 0.2534 - val_acc: 0.9028\n",
      "Epoch 110/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2557 - acc: 0.9105 - val_loss: 0.2640 - val_acc: 0.9237\n",
      "Epoch 111/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2537 - acc: 0.9118 - val_loss: 0.2576 - val_acc: 0.9016\n",
      "Epoch 112/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2524 - acc: 0.9087 - val_loss: 0.2524 - val_acc: 0.9016\n",
      "Epoch 113/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2498 - acc: 0.9117 - val_loss: 0.2477 - val_acc: 0.9119\n",
      "Epoch 114/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2506 - acc: 0.9141 - val_loss: 0.2468 - val_acc: 0.9142\n",
      "Epoch 115/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2465 - acc: 0.9161 - val_loss: 0.2544 - val_acc: 0.9064\n",
      "Epoch 116/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2459 - acc: 0.9146 - val_loss: 0.2480 - val_acc: 0.9063\n",
      "Epoch 117/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2434 - acc: 0.9139 - val_loss: 0.2479 - val_acc: 0.9187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2447 - acc: 0.9152 - val_loss: 0.2422 - val_acc: 0.9057\n",
      "Epoch 119/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.2404 - acc: 0.9154 - val_loss: 0.2397 - val_acc: 0.9129\n",
      "Epoch 120/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2372 - acc: 0.9196 - val_loss: 0.2372 - val_acc: 0.9284\n",
      "Epoch 121/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2398 - acc: 0.9165 - val_loss: 0.2428 - val_acc: 0.9010\n",
      "Epoch 122/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2351 - acc: 0.9201 - val_loss: 0.2301 - val_acc: 0.9160\n",
      "Epoch 123/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.2349 - acc: 0.9186 - val_loss: 0.2357 - val_acc: 0.9106\n",
      "Epoch 124/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2345 - acc: 0.9193 - val_loss: 0.2330 - val_acc: 0.9163\n",
      "Epoch 125/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2340 - acc: 0.9190 - val_loss: 0.2288 - val_acc: 0.9176\n",
      "Epoch 126/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2284 - acc: 0.9253 - val_loss: 0.2347 - val_acc: 0.9217\n",
      "Epoch 127/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2279 - acc: 0.9234 - val_loss: 0.2228 - val_acc: 0.9261\n",
      "Epoch 128/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2298 - acc: 0.9214 - val_loss: 0.2294 - val_acc: 0.9251\n",
      "Epoch 129/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2264 - acc: 0.9231 - val_loss: 0.2292 - val_acc: 0.9223\n",
      "Epoch 130/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2252 - acc: 0.9245 - val_loss: 0.2190 - val_acc: 0.9272\n",
      "Epoch 131/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.2279 - acc: 0.9194 - val_loss: 0.2257 - val_acc: 0.9131\n",
      "Epoch 132/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2236 - acc: 0.9246 - val_loss: 0.2163 - val_acc: 0.9249\n",
      "Epoch 133/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2200 - acc: 0.9273 - val_loss: 0.2191 - val_acc: 0.9182\n",
      "Epoch 134/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2199 - acc: 0.9275 - val_loss: 0.2117 - val_acc: 0.9316\n",
      "Epoch 135/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2171 - acc: 0.9263 - val_loss: 0.2112 - val_acc: 0.9361\n",
      "Epoch 136/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2158 - acc: 0.9294 - val_loss: 0.2217 - val_acc: 0.9237\n",
      "Epoch 137/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2140 - acc: 0.9296 - val_loss: 0.2154 - val_acc: 0.9332\n",
      "Epoch 138/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.2139 - acc: 0.9304 - val_loss: 0.2119 - val_acc: 0.9336\n",
      "Epoch 139/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2170 - acc: 0.9251 - val_loss: 0.2158 - val_acc: 0.9310\n",
      "Epoch 140/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2134 - acc: 0.9289 - val_loss: 0.2150 - val_acc: 0.9200\n",
      "Epoch 141/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2150 - acc: 0.9265 - val_loss: 0.2048 - val_acc: 0.9403\n",
      "Epoch 142/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2121 - acc: 0.9297 - val_loss: 0.2094 - val_acc: 0.9319\n",
      "Epoch 143/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2101 - acc: 0.9309 - val_loss: 0.2213 - val_acc: 0.9407\n",
      "Epoch 144/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2103 - acc: 0.9322 - val_loss: 0.2073 - val_acc: 0.9279\n",
      "Epoch 145/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2047 - acc: 0.9343 - val_loss: 0.2116 - val_acc: 0.9308\n",
      "Epoch 146/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2083 - acc: 0.9291 - val_loss: 0.2021 - val_acc: 0.9293\n",
      "Epoch 147/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2029 - acc: 0.9359 - val_loss: 0.2019 - val_acc: 0.9409\n",
      "Epoch 148/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.2059 - acc: 0.9281 - val_loss: 0.1992 - val_acc: 0.9448\n",
      "Epoch 149/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.2022 - acc: 0.9362 - val_loss: 0.2012 - val_acc: 0.9384\n",
      "Epoch 150/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.2021 - acc: 0.9343 - val_loss: 0.2069 - val_acc: 0.9264\n",
      "Epoch 151/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.2002 - acc: 0.9356 - val_loss: 0.1992 - val_acc: 0.9307\n",
      "Epoch 152/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1982 - acc: 0.9319 - val_loss: 0.2017 - val_acc: 0.9336\n",
      "Epoch 153/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1971 - acc: 0.9373 - val_loss: 0.1998 - val_acc: 0.9470\n",
      "Epoch 154/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1998 - acc: 0.9342 - val_loss: 0.1947 - val_acc: 0.9456\n",
      "Epoch 155/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1947 - acc: 0.9365 - val_loss: 0.1965 - val_acc: 0.9225\n",
      "Epoch 156/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1944 - acc: 0.9385 - val_loss: 0.2030 - val_acc: 0.9255\n",
      "Epoch 157/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1945 - acc: 0.9360 - val_loss: 0.1960 - val_acc: 0.9193\n",
      "Epoch 158/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1932 - acc: 0.9387 - val_loss: 0.1947 - val_acc: 0.9448\n",
      "Epoch 159/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1925 - acc: 0.9371 - val_loss: 0.2040 - val_acc: 0.9255\n",
      "Epoch 160/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1901 - acc: 0.9394 - val_loss: 0.1899 - val_acc: 0.9465\n",
      "Epoch 161/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1902 - acc: 0.9409 - val_loss: 0.1931 - val_acc: 0.9410\n",
      "Epoch 162/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1908 - acc: 0.9417 - val_loss: 0.1874 - val_acc: 0.9288\n",
      "Epoch 163/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1875 - acc: 0.9401 - val_loss: 0.1913 - val_acc: 0.9381\n",
      "Epoch 164/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1874 - acc: 0.9429 - val_loss: 0.1979 - val_acc: 0.9298\n",
      "Epoch 165/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1875 - acc: 0.9401 - val_loss: 0.1836 - val_acc: 0.9483\n",
      "Epoch 166/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1867 - acc: 0.9429 - val_loss: 0.1950 - val_acc: 0.9242\n",
      "Epoch 167/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1865 - acc: 0.9385 - val_loss: 0.1801 - val_acc: 0.9509\n",
      "Epoch 168/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1872 - acc: 0.9375 - val_loss: 0.1778 - val_acc: 0.9497\n",
      "Epoch 169/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1843 - acc: 0.9423 - val_loss: 0.1827 - val_acc: 0.9333\n",
      "Epoch 170/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1814 - acc: 0.9427 - val_loss: 0.1771 - val_acc: 0.9551\n",
      "Epoch 171/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1852 - acc: 0.9385 - val_loss: 0.1860 - val_acc: 0.9293\n",
      "Epoch 172/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1823 - acc: 0.9425 - val_loss: 0.2000 - val_acc: 0.9055\n",
      "Epoch 173/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1799 - acc: 0.9433 - val_loss: 0.1957 - val_acc: 0.9232\n",
      "Epoch 174/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1763 - acc: 0.9473 - val_loss: 0.1915 - val_acc: 0.9433\n",
      "Epoch 175/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1770 - acc: 0.9441 - val_loss: 0.1715 - val_acc: 0.9504\n",
      "Epoch 176/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1760 - acc: 0.9449 - val_loss: 0.1771 - val_acc: 0.9451\n",
      "Epoch 177/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1759 - acc: 0.9466 - val_loss: 0.1766 - val_acc: 0.9465\n",
      "Epoch 178/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1775 - acc: 0.9435 - val_loss: 0.1735 - val_acc: 0.9453\n",
      "Epoch 179/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1750 - acc: 0.9452 - val_loss: 0.1892 - val_acc: 0.9359\n",
      "Epoch 180/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1776 - acc: 0.9431 - val_loss: 0.1699 - val_acc: 0.9501\n",
      "Epoch 181/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1768 - acc: 0.9447 - val_loss: 0.1835 - val_acc: 0.9239\n",
      "Epoch 182/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1728 - acc: 0.9457 - val_loss: 0.1687 - val_acc: 0.9512\n",
      "Epoch 183/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.1735 - acc: 0.9437 - val_loss: 0.1650 - val_acc: 0.9579\n",
      "Epoch 184/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1735 - acc: 0.9452 - val_loss: 0.1676 - val_acc: 0.9520\n",
      "Epoch 185/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1700 - acc: 0.9487 - val_loss: 0.1758 - val_acc: 0.9295\n",
      "Epoch 186/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1709 - acc: 0.9478 - val_loss: 0.1849 - val_acc: 0.9278\n",
      "Epoch 187/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1695 - acc: 0.9439 - val_loss: 0.1866 - val_acc: 0.9206\n",
      "Epoch 188/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1682 - acc: 0.9495 - val_loss: 0.1635 - val_acc: 0.9551\n",
      "Epoch 189/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1682 - acc: 0.9494 - val_loss: 0.1586 - val_acc: 0.9554\n",
      "Epoch 190/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1676 - acc: 0.9469 - val_loss: 0.1605 - val_acc: 0.9517\n",
      "Epoch 191/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1621 - acc: 0.9523 - val_loss: 0.1617 - val_acc: 0.9471\n",
      "Epoch 192/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1640 - acc: 0.9509 - val_loss: 0.1616 - val_acc: 0.9462\n",
      "Epoch 193/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1641 - acc: 0.9489 - val_loss: 0.1743 - val_acc: 0.9317\n",
      "Epoch 194/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1654 - acc: 0.9479 - val_loss: 0.1569 - val_acc: 0.9553\n",
      "Epoch 195/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1658 - acc: 0.9478 - val_loss: 0.1620 - val_acc: 0.9576\n",
      "Epoch 196/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1619 - acc: 0.9520 - val_loss: 0.1602 - val_acc: 0.9568\n",
      "Epoch 197/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1583 - acc: 0.9529 - val_loss: 0.1602 - val_acc: 0.9519\n",
      "Epoch 198/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1592 - acc: 0.9540 - val_loss: 0.1639 - val_acc: 0.9506\n",
      "Epoch 199/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1643 - acc: 0.9472 - val_loss: 0.1812 - val_acc: 0.9158\n",
      "Epoch 200/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1626 - acc: 0.9478 - val_loss: 0.1578 - val_acc: 0.9475\n",
      "Epoch 201/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1617 - acc: 0.9493 - val_loss: 0.1659 - val_acc: 0.9582\n",
      "Epoch 202/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1696 - acc: 0.9396 - val_loss: 0.1558 - val_acc: 0.9554\n",
      "Epoch 203/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1549 - acc: 0.9547 - val_loss: 0.1612 - val_acc: 0.9459\n",
      "Epoch 204/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1594 - acc: 0.9501 - val_loss: 0.1584 - val_acc: 0.9408\n",
      "Epoch 205/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1574 - acc: 0.9508 - val_loss: 0.1606 - val_acc: 0.9460\n",
      "Epoch 206/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1555 - acc: 0.9548 - val_loss: 0.1496 - val_acc: 0.9546\n",
      "Epoch 207/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1525 - acc: 0.9550 - val_loss: 0.1712 - val_acc: 0.9406\n",
      "Epoch 208/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1567 - acc: 0.9501 - val_loss: 0.1525 - val_acc: 0.9594\n",
      "Epoch 209/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1534 - acc: 0.9549 - val_loss: 0.1488 - val_acc: 0.9556\n",
      "Epoch 210/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1512 - acc: 0.9551 - val_loss: 0.1471 - val_acc: 0.9584\n",
      "Epoch 211/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1556 - acc: 0.9526 - val_loss: 0.1636 - val_acc: 0.9433\n",
      "Epoch 212/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1513 - acc: 0.9554 - val_loss: 0.1607 - val_acc: 0.9418\n",
      "Epoch 213/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1497 - acc: 0.9574 - val_loss: 0.1438 - val_acc: 0.9592\n",
      "Epoch 214/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1483 - acc: 0.9569 - val_loss: 0.1459 - val_acc: 0.9564\n",
      "Epoch 215/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1567 - acc: 0.9502 - val_loss: 0.1513 - val_acc: 0.9488\n",
      "Epoch 216/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1500 - acc: 0.9554 - val_loss: 0.1471 - val_acc: 0.9517\n",
      "Epoch 217/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1481 - acc: 0.9568 - val_loss: 0.1640 - val_acc: 0.9472\n",
      "Epoch 218/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1525 - acc: 0.9502 - val_loss: 0.1434 - val_acc: 0.9595\n",
      "Epoch 219/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1474 - acc: 0.9560 - val_loss: 0.1416 - val_acc: 0.9643\n",
      "Epoch 220/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1461 - acc: 0.9563 - val_loss: 0.1442 - val_acc: 0.9595\n",
      "Epoch 221/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1505 - acc: 0.9534 - val_loss: 0.1447 - val_acc: 0.9610\n",
      "Epoch 222/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1475 - acc: 0.9538 - val_loss: 0.1542 - val_acc: 0.9545\n",
      "Epoch 223/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1454 - acc: 0.9577 - val_loss: 0.1524 - val_acc: 0.9413\n",
      "Epoch 224/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1453 - acc: 0.9550 - val_loss: 0.1408 - val_acc: 0.9598\n",
      "Epoch 225/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1408 - acc: 0.9600 - val_loss: 0.1396 - val_acc: 0.9602\n",
      "Epoch 226/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1414 - acc: 0.9583 - val_loss: 0.1366 - val_acc: 0.9618\n",
      "Epoch 227/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1426 - acc: 0.9573 - val_loss: 0.1663 - val_acc: 0.9471\n",
      "Epoch 228/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1427 - acc: 0.9578 - val_loss: 0.1376 - val_acc: 0.9625\n",
      "Epoch 229/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1391 - acc: 0.9596 - val_loss: 0.1485 - val_acc: 0.9443\n",
      "Epoch 230/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1434 - acc: 0.9555 - val_loss: 0.1369 - val_acc: 0.9607\n",
      "Epoch 231/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1380 - acc: 0.9598 - val_loss: 0.1430 - val_acc: 0.9547\n",
      "Epoch 232/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1404 - acc: 0.9577 - val_loss: 0.1386 - val_acc: 0.9611\n",
      "Epoch 233/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1421 - acc: 0.9554 - val_loss: 0.1425 - val_acc: 0.9627\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1428 - acc: 0.9569 - val_loss: 0.1349 - val_acc: 0.9659\n",
      "Epoch 235/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1370 - acc: 0.9600 - val_loss: 0.1376 - val_acc: 0.9609\n",
      "Epoch 236/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1380 - acc: 0.9592 - val_loss: 0.1480 - val_acc: 0.9518\n",
      "Epoch 237/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1389 - acc: 0.9593 - val_loss: 0.1335 - val_acc: 0.9648\n",
      "Epoch 238/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1360 - acc: 0.9590 - val_loss: 0.1296 - val_acc: 0.9679\n",
      "Epoch 239/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1362 - acc: 0.9593 - val_loss: 0.1457 - val_acc: 0.9576\n",
      "Epoch 240/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1438 - acc: 0.9537 - val_loss: 0.1437 - val_acc: 0.9549\n",
      "Epoch 241/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1342 - acc: 0.9607 - val_loss: 0.1398 - val_acc: 0.9563\n",
      "Epoch 242/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1339 - acc: 0.9595 - val_loss: 0.1288 - val_acc: 0.9667\n",
      "Epoch 243/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1318 - acc: 0.9620 - val_loss: 0.1336 - val_acc: 0.9642\n",
      "Epoch 244/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1379 - acc: 0.9582 - val_loss: 0.1461 - val_acc: 0.9439\n",
      "Epoch 245/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1372 - acc: 0.9574 - val_loss: 0.1276 - val_acc: 0.9645\n",
      "Epoch 246/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1311 - acc: 0.9624 - val_loss: 0.1257 - val_acc: 0.9634\n",
      "Epoch 247/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1310 - acc: 0.9612 - val_loss: 0.1294 - val_acc: 0.9586\n",
      "Epoch 248/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1291 - acc: 0.9634 - val_loss: 0.1296 - val_acc: 0.9634\n",
      "Epoch 249/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1347 - acc: 0.9597 - val_loss: 0.1289 - val_acc: 0.9648\n",
      "Epoch 250/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1318 - acc: 0.9607 - val_loss: 0.1252 - val_acc: 0.9655\n",
      "Epoch 251/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1307 - acc: 0.9587 - val_loss: 0.1436 - val_acc: 0.9507\n",
      "Epoch 252/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1296 - acc: 0.9632 - val_loss: 0.1388 - val_acc: 0.9473\n",
      "Epoch 253/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1354 - acc: 0.9548 - val_loss: 0.1473 - val_acc: 0.9454\n",
      "Epoch 254/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1268 - acc: 0.9623 - val_loss: 0.1305 - val_acc: 0.9646\n",
      "Epoch 255/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1265 - acc: 0.9638 - val_loss: 0.1310 - val_acc: 0.9495\n",
      "Epoch 256/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1309 - acc: 0.9602 - val_loss: 0.1277 - val_acc: 0.9577\n",
      "Epoch 257/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1341 - acc: 0.9560 - val_loss: 0.1273 - val_acc: 0.9631\n",
      "Epoch 258/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1325 - acc: 0.9585 - val_loss: 0.1345 - val_acc: 0.9553\n",
      "Epoch 259/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1256 - acc: 0.9633 - val_loss: 0.1384 - val_acc: 0.9494\n",
      "Epoch 260/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1274 - acc: 0.9622 - val_loss: 0.1195 - val_acc: 0.9693\n",
      "Epoch 261/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1258 - acc: 0.9635 - val_loss: 0.1424 - val_acc: 0.9516\n",
      "Epoch 262/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1342 - acc: 0.9569 - val_loss: 0.1251 - val_acc: 0.9582\n",
      "Epoch 263/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1226 - acc: 0.9640 - val_loss: 0.1248 - val_acc: 0.9604\n",
      "Epoch 264/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1229 - acc: 0.9631 - val_loss: 0.1261 - val_acc: 0.9522\n",
      "Epoch 265/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1279 - acc: 0.9586 - val_loss: 0.1280 - val_acc: 0.9635\n",
      "Epoch 266/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1237 - acc: 0.9637 - val_loss: 0.1258 - val_acc: 0.9673\n",
      "Epoch 267/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1243 - acc: 0.9640 - val_loss: 0.1200 - val_acc: 0.9686\n",
      "Epoch 268/300\n",
      "17858/17858 [==============================] - 0s 20us/step - loss: 0.1274 - acc: 0.9606 - val_loss: 0.1241 - val_acc: 0.9599\n",
      "Epoch 269/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1224 - acc: 0.9637 - val_loss: 0.1189 - val_acc: 0.9688\n",
      "Epoch 270/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1243 - acc: 0.9617 - val_loss: 0.1204 - val_acc: 0.9675\n",
      "Epoch 271/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1213 - acc: 0.9638 - val_loss: 0.1356 - val_acc: 0.9419\n",
      "Epoch 272/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1213 - acc: 0.9629 - val_loss: 0.1267 - val_acc: 0.9582\n",
      "Epoch 273/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1258 - acc: 0.9607 - val_loss: 0.1426 - val_acc: 0.9360\n",
      "Epoch 274/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1215 - acc: 0.9629 - val_loss: 0.1311 - val_acc: 0.9530\n",
      "Epoch 275/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1274 - acc: 0.9591 - val_loss: 0.1431 - val_acc: 0.9382\n",
      "Epoch 276/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1265 - acc: 0.9587 - val_loss: 0.1192 - val_acc: 0.9643\n",
      "Epoch 277/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1188 - acc: 0.9665 - val_loss: 0.1155 - val_acc: 0.9650\n",
      "Epoch 278/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1196 - acc: 0.9641 - val_loss: 0.1176 - val_acc: 0.9693\n",
      "Epoch 279/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1181 - acc: 0.9657 - val_loss: 0.1126 - val_acc: 0.9687\n",
      "Epoch 280/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1154 - acc: 0.9674 - val_loss: 0.1247 - val_acc: 0.9531\n",
      "Epoch 281/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1215 - acc: 0.9624 - val_loss: 0.1172 - val_acc: 0.9629\n",
      "Epoch 282/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1199 - acc: 0.9643 - val_loss: 0.1142 - val_acc: 0.9689\n",
      "Epoch 283/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1202 - acc: 0.9620 - val_loss: 0.1418 - val_acc: 0.9418\n",
      "Epoch 284/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1226 - acc: 0.9600 - val_loss: 0.1206 - val_acc: 0.9617\n",
      "Epoch 285/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1179 - acc: 0.9651 - val_loss: 0.1111 - val_acc: 0.9701\n",
      "Epoch 286/300\n",
      "17858/17858 [==============================] - 0s 21us/step - loss: 0.1137 - acc: 0.9678 - val_loss: 0.1103 - val_acc: 0.9702\n",
      "Epoch 287/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1181 - acc: 0.9639 - val_loss: 0.1234 - val_acc: 0.9662\n",
      "Epoch 288/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1143 - acc: 0.9658 - val_loss: 0.1089 - val_acc: 0.9695\n",
      "Epoch 289/300\n",
      "17858/17858 [==============================] - 0s 18us/step - loss: 0.1139 - acc: 0.9669 - val_loss: 0.1294 - val_acc: 0.9644\n",
      "Epoch 290/300\n",
      "17858/17858 [==============================] - 0s 17us/step - loss: 0.1169 - acc: 0.9634 - val_loss: 0.1208 - val_acc: 0.9587\n",
      "Epoch 291/300\n",
      "17858/17858 [==============================] - 0s 23us/step - loss: 0.1139 - acc: 0.9665 - val_loss: 0.1101 - val_acc: 0.9718\n",
      "Epoch 292/300\n",
      "17858/17858 [==============================] - 0s 26us/step - loss: 0.1158 - acc: 0.9653 - val_loss: 0.1078 - val_acc: 0.9681\n",
      "Epoch 293/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1155 - acc: 0.9649 - val_loss: 0.1202 - val_acc: 0.9651\n",
      "Epoch 294/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1162 - acc: 0.9636 - val_loss: 0.1138 - val_acc: 0.9632\n",
      "Epoch 295/300\n",
      "17858/17858 [==============================] - 0s 19us/step - loss: 0.1192 - acc: 0.9616 - val_loss: 0.1406 - val_acc: 0.9288\n",
      "Epoch 296/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1160 - acc: 0.9646 - val_loss: 0.1054 - val_acc: 0.9710\n",
      "Epoch 297/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1087 - acc: 0.9699 - val_loss: 0.1147 - val_acc: 0.9620\n",
      "Epoch 298/300\n",
      "17858/17858 [==============================] - 0s 16us/step - loss: 0.1132 - acc: 0.9668 - val_loss: 0.1128 - val_acc: 0.9701\n",
      "Epoch 299/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1151 - acc: 0.9638 - val_loss: 0.1052 - val_acc: 0.9721\n",
      "Epoch 300/300\n",
      "17858/17858 [==============================] - 0s 15us/step - loss: 0.1159 - acc: 0.9638 - val_loss: 0.1069 - val_acc: 0.9688\n",
      "17859/17859 [==============================] - 1s 74us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7143 samples, validate on 7143 samples\n",
      "Epoch 1/300\n",
      "7143/7143 [==============================] - 3s 378us/step - loss: 3.4057 - acc: 0.0353 - val_loss: 3.3603 - val_acc: 0.0584\n",
      "Epoch 2/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 3.3135 - acc: 0.0830 - val_loss: 3.2469 - val_acc: 0.0875\n",
      "Epoch 3/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 3.1680 - acc: 0.0794 - val_loss: 3.0624 - val_acc: 0.0788\n",
      "Epoch 4/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 2.9680 - acc: 0.0806 - val_loss: 2.8624 - val_acc: 0.0956\n",
      "Epoch 5/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 2.7926 - acc: 0.0942 - val_loss: 2.7151 - val_acc: 0.0988\n",
      "Epoch 6/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 2.6622 - acc: 0.1142 - val_loss: 2.5978 - val_acc: 0.1378\n",
      "Epoch 7/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 2.5488 - acc: 0.1957 - val_loss: 2.4848 - val_acc: 0.2621\n",
      "Epoch 8/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 2.4334 - acc: 0.2544 - val_loss: 2.3651 - val_acc: 0.2607\n",
      "Epoch 9/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 2.3105 - acc: 0.2779 - val_loss: 2.2400 - val_acc: 0.3058\n",
      "Epoch 10/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 2.1810 - acc: 0.3157 - val_loss: 2.1078 - val_acc: 0.3619\n",
      "Epoch 11/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 2.0519 - acc: 0.3623 - val_loss: 1.9790 - val_acc: 0.4074\n",
      "Epoch 12/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.9312 - acc: 0.3877 - val_loss: 1.8773 - val_acc: 0.3984\n",
      "Epoch 13/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.8287 - acc: 0.4131 - val_loss: 1.7796 - val_acc: 0.4203\n",
      "Epoch 14/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.7439 - acc: 0.4428 - val_loss: 1.6925 - val_acc: 0.5017\n",
      "Epoch 15/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.6666 - acc: 0.4651 - val_loss: 1.6167 - val_acc: 0.5247\n",
      "Epoch 16/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.5931 - acc: 0.4764 - val_loss: 1.5705 - val_acc: 0.4743\n",
      "Epoch 17/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.5382 - acc: 0.4934 - val_loss: 1.4893 - val_acc: 0.5197\n",
      "Epoch 18/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.4869 - acc: 0.5118 - val_loss: 1.4589 - val_acc: 0.5552\n",
      "Epoch 19/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.4556 - acc: 0.5202 - val_loss: 1.3894 - val_acc: 0.5362\n",
      "Epoch 20/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.3978 - acc: 0.5422 - val_loss: 1.3513 - val_acc: 0.5775\n",
      "Epoch 21/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.3690 - acc: 0.5566 - val_loss: 1.3484 - val_acc: 0.5597\n",
      "Epoch 22/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.3459 - acc: 0.5429 - val_loss: 1.3309 - val_acc: 0.5520\n",
      "Epoch 23/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.3530 - acc: 0.5206 - val_loss: 1.2617 - val_acc: 0.5850\n",
      "Epoch 24/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.2947 - acc: 0.5503 - val_loss: 1.2684 - val_acc: 0.5267\n",
      "Epoch 25/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.3179 - acc: 0.5180 - val_loss: 1.2939 - val_acc: 0.5216\n",
      "Epoch 26/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.2811 - acc: 0.5223 - val_loss: 1.2887 - val_acc: 0.5449\n",
      "Epoch 27/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.2815 - acc: 0.5202 - val_loss: 1.3643 - val_acc: 0.4704\n",
      "Epoch 28/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.2636 - acc: 0.5092 - val_loss: 1.2116 - val_acc: 0.5253\n",
      "Epoch 29/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.2348 - acc: 0.5393 - val_loss: 1.3350 - val_acc: 0.4848\n",
      "Epoch 30/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 1.2334 - acc: 0.5400 - val_loss: 1.1560 - val_acc: 0.5579\n",
      "Epoch 31/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 1.2070 - acc: 0.5509 - val_loss: 1.2030 - val_acc: 0.5044\n",
      "Epoch 32/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.2467 - acc: 0.5419 - val_loss: 1.1422 - val_acc: 0.5663\n",
      "Epoch 33/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.1841 - acc: 0.5580 - val_loss: 1.2698 - val_acc: 0.4894\n",
      "Epoch 34/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.2061 - acc: 0.5379 - val_loss: 1.2339 - val_acc: 0.5446\n",
      "Epoch 35/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.2432 - acc: 0.5124 - val_loss: 1.1240 - val_acc: 0.6038\n",
      "Epoch 36/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.1488 - acc: 0.5622 - val_loss: 1.1414 - val_acc: 0.5629\n",
      "Epoch 37/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.0941 - acc: 0.5930 - val_loss: 1.0975 - val_acc: 0.6065\n",
      "Epoch 38/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.1158 - acc: 0.5667 - val_loss: 1.0809 - val_acc: 0.6402\n",
      "Epoch 39/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.1435 - acc: 0.5575 - val_loss: 1.1503 - val_acc: 0.5478\n",
      "Epoch 40/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.1286 - acc: 0.5573 - val_loss: 1.0810 - val_acc: 0.5603\n",
      "Epoch 41/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 1.0849 - acc: 0.5912 - val_loss: 1.1980 - val_acc: 0.5276\n",
      "Epoch 42/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.1096 - acc: 0.5695 - val_loss: 1.0417 - val_acc: 0.5877\n",
      "Epoch 43/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.0409 - acc: 0.5874 - val_loss: 0.9906 - val_acc: 0.6461\n",
      "Epoch 44/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 1.0619 - acc: 0.5927 - val_loss: 0.9886 - val_acc: 0.6147\n",
      "Epoch 45/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.0241 - acc: 0.6286 - val_loss: 1.0730 - val_acc: 0.5607\n",
      "Epoch 46/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0367 - acc: 0.5916 - val_loss: 1.0909 - val_acc: 0.5703\n",
      "Epoch 47/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0406 - acc: 0.5883 - val_loss: 1.0190 - val_acc: 0.5869\n",
      "Epoch 48/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.0224 - acc: 0.5859 - val_loss: 0.9680 - val_acc: 0.6263\n",
      "Epoch 49/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.0069 - acc: 0.5923 - val_loss: 0.9281 - val_acc: 0.6021\n",
      "Epoch 50/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.9906 - acc: 0.6086 - val_loss: 1.0119 - val_acc: 0.5815\n",
      "Epoch 51/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.0225 - acc: 0.5957 - val_loss: 0.9644 - val_acc: 0.6265\n",
      "Epoch 52/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.0096 - acc: 0.6016 - val_loss: 0.9559 - val_acc: 0.6322\n",
      "Epoch 53/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9887 - acc: 0.6049 - val_loss: 0.9347 - val_acc: 0.6283\n",
      "Epoch 54/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9240 - acc: 0.6455 - val_loss: 1.0306 - val_acc: 0.5997\n",
      "Epoch 55/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9624 - acc: 0.6287 - val_loss: 0.8924 - val_acc: 0.6734\n",
      "Epoch 56/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9084 - acc: 0.6528 - val_loss: 0.9334 - val_acc: 0.6368\n",
      "Epoch 57/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9513 - acc: 0.6315 - val_loss: 0.9047 - val_acc: 0.6580\n",
      "Epoch 58/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9734 - acc: 0.6077 - val_loss: 0.9138 - val_acc: 0.6430\n",
      "Epoch 59/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9089 - acc: 0.6436 - val_loss: 0.8527 - val_acc: 0.6975\n",
      "Epoch 60/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9228 - acc: 0.6272 - val_loss: 1.0092 - val_acc: 0.6097\n",
      "Epoch 61/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 0.8967 - acc: 0.6676 - val_loss: 0.8188 - val_acc: 0.6878\n",
      "Epoch 62/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8704 - acc: 0.6735 - val_loss: 0.9534 - val_acc: 0.5944\n",
      "Epoch 63/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9232 - acc: 0.6188 - val_loss: 0.9481 - val_acc: 0.6326\n",
      "Epoch 64/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.6377 - val_loss: 0.8842 - val_acc: 0.7110\n",
      "Epoch 65/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9119 - acc: 0.6462 - val_loss: 0.8051 - val_acc: 0.6837\n",
      "Epoch 66/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8152 - acc: 0.7054 - val_loss: 0.8098 - val_acc: 0.6808\n",
      "Epoch 67/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8153 - acc: 0.6881 - val_loss: 0.8102 - val_acc: 0.6606\n",
      "Epoch 68/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.6702 - val_loss: 0.8620 - val_acc: 0.6462\n",
      "Epoch 69/300\n",
      "7143/7143 [==============================] - 0s 14us/step - loss: 0.8885 - acc: 0.6380 - val_loss: 0.7676 - val_acc: 0.7452\n",
      "Epoch 70/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.7570 - acc: 0.7539 - val_loss: 0.7335 - val_acc: 0.7358\n",
      "Epoch 71/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.7352 - acc: 0.7599 - val_loss: 0.7043 - val_acc: 0.7754\n",
      "Epoch 72/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.7416 - acc: 0.7467 - val_loss: 0.8233 - val_acc: 0.6493\n",
      "Epoch 73/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8106 - acc: 0.6835 - val_loss: 0.7409 - val_acc: 0.7371\n",
      "Epoch 74/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.7398 - acc: 0.7521 - val_loss: 0.7740 - val_acc: 0.7332\n",
      "Epoch 75/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.7188 - acc: 0.7596 - val_loss: 0.6837 - val_acc: 0.7969\n",
      "Epoch 76/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6930 - acc: 0.7771 - val_loss: 0.6738 - val_acc: 0.7722\n",
      "Epoch 77/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6821 - acc: 0.7889 - val_loss: 0.7108 - val_acc: 0.7780\n",
      "Epoch 78/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6932 - acc: 0.7719 - val_loss: 0.6724 - val_acc: 0.7896\n",
      "Epoch 79/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6853 - acc: 0.7752 - val_loss: 0.6772 - val_acc: 0.7668\n",
      "Epoch 80/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 0.6691 - acc: 0.7792 - val_loss: 0.6773 - val_acc: 0.7780\n",
      "Epoch 81/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.6688 - acc: 0.7901 - val_loss: 0.6467 - val_acc: 0.7980\n",
      "Epoch 82/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6459 - acc: 0.8001 - val_loss: 0.6560 - val_acc: 0.7879\n",
      "Epoch 83/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.6530 - acc: 0.7984 - val_loss: 0.6575 - val_acc: 0.7967\n",
      "Epoch 84/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.6489 - acc: 0.7984 - val_loss: 0.6355 - val_acc: 0.7894\n",
      "Epoch 85/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6354 - acc: 0.8008 - val_loss: 0.6526 - val_acc: 0.7565\n",
      "Epoch 86/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6273 - acc: 0.8061 - val_loss: 0.6368 - val_acc: 0.7851\n",
      "Epoch 87/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.6400 - acc: 0.7882 - val_loss: 0.6331 - val_acc: 0.7977\n",
      "Epoch 88/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 0.6194 - acc: 0.8057 - val_loss: 0.6204 - val_acc: 0.7887\n",
      "Epoch 89/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6108 - acc: 0.8138 - val_loss: 0.6120 - val_acc: 0.8197\n",
      "Epoch 90/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6079 - acc: 0.8179 - val_loss: 0.6169 - val_acc: 0.7830\n",
      "Epoch 91/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.5986 - acc: 0.8264 - val_loss: 0.6032 - val_acc: 0.8078\n",
      "Epoch 92/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.6039 - acc: 0.8124 - val_loss: 0.6077 - val_acc: 0.8162\n",
      "Epoch 93/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5953 - acc: 0.8221 - val_loss: 0.5917 - val_acc: 0.8103\n",
      "Epoch 94/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.5863 - acc: 0.8264 - val_loss: 0.5932 - val_acc: 0.8162\n",
      "Epoch 95/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5829 - acc: 0.8251 - val_loss: 0.5785 - val_acc: 0.8279\n",
      "Epoch 96/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5902 - acc: 0.8190 - val_loss: 0.5813 - val_acc: 0.8117\n",
      "Epoch 97/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5760 - acc: 0.8268 - val_loss: 0.5656 - val_acc: 0.8428\n",
      "Epoch 98/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5680 - acc: 0.8286 - val_loss: 0.5653 - val_acc: 0.8361\n",
      "Epoch 99/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5700 - acc: 0.8285 - val_loss: 0.5571 - val_acc: 0.8415\n",
      "Epoch 100/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5640 - acc: 0.8298 - val_loss: 0.5672 - val_acc: 0.8208\n",
      "Epoch 101/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5646 - acc: 0.8278 - val_loss: 0.5514 - val_acc: 0.8394\n",
      "Epoch 102/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5559 - acc: 0.8393 - val_loss: 0.5502 - val_acc: 0.8463\n",
      "Epoch 103/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5502 - acc: 0.8373 - val_loss: 0.5423 - val_acc: 0.8481\n",
      "Epoch 104/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5537 - acc: 0.8390 - val_loss: 0.5429 - val_acc: 0.8498\n",
      "Epoch 105/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5436 - acc: 0.8379 - val_loss: 0.5392 - val_acc: 0.8415\n",
      "Epoch 106/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5393 - acc: 0.8433 - val_loss: 0.5373 - val_acc: 0.8397\n",
      "Epoch 107/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5370 - acc: 0.8428 - val_loss: 0.5322 - val_acc: 0.8492\n",
      "Epoch 108/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5339 - acc: 0.8438 - val_loss: 0.5313 - val_acc: 0.8221\n",
      "Epoch 109/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5317 - acc: 0.8403 - val_loss: 0.5287 - val_acc: 0.8286\n",
      "Epoch 110/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5259 - acc: 0.8438 - val_loss: 0.5209 - val_acc: 0.8516\n",
      "Epoch 111/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5270 - acc: 0.8428 - val_loss: 0.5175 - val_acc: 0.8538\n",
      "Epoch 112/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5206 - acc: 0.8484 - val_loss: 0.5230 - val_acc: 0.8468\n",
      "Epoch 113/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5208 - acc: 0.8429 - val_loss: 0.5125 - val_acc: 0.8552\n",
      "Epoch 114/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5152 - acc: 0.8470 - val_loss: 0.5095 - val_acc: 0.8547\n",
      "Epoch 115/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5141 - acc: 0.8477 - val_loss: 0.5110 - val_acc: 0.8501\n",
      "Epoch 116/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5104 - acc: 0.8492 - val_loss: 0.5052 - val_acc: 0.8557\n",
      "Epoch 117/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5079 - acc: 0.8478 - val_loss: 0.5045 - val_acc: 0.8544\n",
      "Epoch 118/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5042 - acc: 0.8498 - val_loss: 0.4989 - val_acc: 0.8492\n",
      "Epoch 119/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5040 - acc: 0.8491 - val_loss: 0.4987 - val_acc: 0.8526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4974 - acc: 0.8533 - val_loss: 0.4945 - val_acc: 0.8494\n",
      "Epoch 121/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4955 - acc: 0.8517 - val_loss: 0.4906 - val_acc: 0.8554\n",
      "Epoch 122/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4925 - acc: 0.8488 - val_loss: 0.4852 - val_acc: 0.8545\n",
      "Epoch 123/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4911 - acc: 0.8530 - val_loss: 0.4854 - val_acc: 0.8531\n",
      "Epoch 124/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4875 - acc: 0.8498 - val_loss: 0.4882 - val_acc: 0.8540\n",
      "Epoch 125/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4858 - acc: 0.8552 - val_loss: 0.4818 - val_acc: 0.8635\n",
      "Epoch 126/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4820 - acc: 0.8538 - val_loss: 0.4788 - val_acc: 0.8554\n",
      "Epoch 127/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4812 - acc: 0.8571 - val_loss: 0.4737 - val_acc: 0.8569\n",
      "Epoch 128/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4774 - acc: 0.8526 - val_loss: 0.4711 - val_acc: 0.8603\n",
      "Epoch 129/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4741 - acc: 0.8571 - val_loss: 0.4726 - val_acc: 0.8543\n",
      "Epoch 130/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4732 - acc: 0.8529 - val_loss: 0.4688 - val_acc: 0.8601\n",
      "Epoch 131/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4710 - acc: 0.8538 - val_loss: 0.4649 - val_acc: 0.8597\n",
      "Epoch 132/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4684 - acc: 0.8604 - val_loss: 0.4645 - val_acc: 0.8537\n",
      "Epoch 133/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4668 - acc: 0.8533 - val_loss: 0.4635 - val_acc: 0.8596\n",
      "Epoch 134/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4607 - acc: 0.8572 - val_loss: 0.4659 - val_acc: 0.8569\n",
      "Epoch 135/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4640 - acc: 0.8562 - val_loss: 0.4561 - val_acc: 0.8566\n",
      "Epoch 136/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4580 - acc: 0.8559 - val_loss: 0.4572 - val_acc: 0.8641\n",
      "Epoch 137/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4582 - acc: 0.8573 - val_loss: 0.4531 - val_acc: 0.8604\n",
      "Epoch 138/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4562 - acc: 0.8579 - val_loss: 0.4502 - val_acc: 0.8604\n",
      "Epoch 139/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4515 - acc: 0.8601 - val_loss: 0.4519 - val_acc: 0.8620\n",
      "Epoch 140/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4512 - acc: 0.8606 - val_loss: 0.4461 - val_acc: 0.8622\n",
      "Epoch 141/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4469 - acc: 0.8587 - val_loss: 0.4446 - val_acc: 0.8625\n",
      "Epoch 142/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4465 - acc: 0.8597 - val_loss: 0.4442 - val_acc: 0.8610\n",
      "Epoch 143/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4434 - acc: 0.8607 - val_loss: 0.4469 - val_acc: 0.8412\n",
      "Epoch 144/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4400 - acc: 0.8590 - val_loss: 0.4427 - val_acc: 0.8645\n",
      "Epoch 145/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4386 - acc: 0.8627 - val_loss: 0.4362 - val_acc: 0.8632\n",
      "Epoch 146/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4388 - acc: 0.8613 - val_loss: 0.4372 - val_acc: 0.8601\n",
      "Epoch 147/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4356 - acc: 0.8611 - val_loss: 0.4338 - val_acc: 0.8664\n",
      "Epoch 148/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.4342 - acc: 0.8610 - val_loss: 0.4309 - val_acc: 0.8632\n",
      "Epoch 149/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4305 - acc: 0.8638 - val_loss: 0.4285 - val_acc: 0.8614\n",
      "Epoch 150/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4302 - acc: 0.8618 - val_loss: 0.4281 - val_acc: 0.8653\n",
      "Epoch 151/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4274 - acc: 0.8611 - val_loss: 0.4275 - val_acc: 0.8503\n",
      "Epoch 152/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4261 - acc: 0.8607 - val_loss: 0.4210 - val_acc: 0.8643\n",
      "Epoch 153/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4242 - acc: 0.8635 - val_loss: 0.4214 - val_acc: 0.8491\n",
      "Epoch 154/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4222 - acc: 0.8597 - val_loss: 0.4194 - val_acc: 0.8607\n",
      "Epoch 155/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4206 - acc: 0.8627 - val_loss: 0.4210 - val_acc: 0.8624\n",
      "Epoch 156/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4195 - acc: 0.8639 - val_loss: 0.4194 - val_acc: 0.8614\n",
      "Epoch 157/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4166 - acc: 0.8650 - val_loss: 0.4137 - val_acc: 0.8674\n",
      "Epoch 158/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4184 - acc: 0.8632 - val_loss: 0.4100 - val_acc: 0.8670\n",
      "Epoch 159/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4119 - acc: 0.8666 - val_loss: 0.4101 - val_acc: 0.8662\n",
      "Epoch 160/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4147 - acc: 0.8650 - val_loss: 0.4092 - val_acc: 0.8684\n",
      "Epoch 161/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4117 - acc: 0.8655 - val_loss: 0.4051 - val_acc: 0.8705\n",
      "Epoch 162/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4109 - acc: 0.8648 - val_loss: 0.4038 - val_acc: 0.8713\n",
      "Epoch 163/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4068 - acc: 0.8659 - val_loss: 0.4028 - val_acc: 0.8704\n",
      "Epoch 164/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4059 - acc: 0.8673 - val_loss: 0.4051 - val_acc: 0.8645\n",
      "Epoch 165/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.4018 - acc: 0.8685 - val_loss: 0.4047 - val_acc: 0.8634\n",
      "Epoch 166/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.4041 - acc: 0.8671 - val_loss: 0.4027 - val_acc: 0.8674\n",
      "Epoch 167/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4023 - acc: 0.8650 - val_loss: 0.3987 - val_acc: 0.8695\n",
      "Epoch 168/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3983 - acc: 0.8683 - val_loss: 0.3955 - val_acc: 0.8730\n",
      "Epoch 169/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3968 - acc: 0.8687 - val_loss: 0.3933 - val_acc: 0.8702\n",
      "Epoch 170/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3954 - acc: 0.8690 - val_loss: 0.3936 - val_acc: 0.8704\n",
      "Epoch 171/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3936 - acc: 0.8680 - val_loss: 0.3918 - val_acc: 0.8670\n",
      "Epoch 172/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3915 - acc: 0.8701 - val_loss: 0.3913 - val_acc: 0.8683\n",
      "Epoch 173/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3926 - acc: 0.8684 - val_loss: 0.3891 - val_acc: 0.8762\n",
      "Epoch 174/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3900 - acc: 0.8715 - val_loss: 0.3884 - val_acc: 0.8649\n",
      "Epoch 175/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3893 - acc: 0.8683 - val_loss: 0.3860 - val_acc: 0.8676\n",
      "Epoch 176/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3859 - acc: 0.8719 - val_loss: 0.3817 - val_acc: 0.8739\n",
      "Epoch 177/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3842 - acc: 0.8716 - val_loss: 0.3895 - val_acc: 0.8635\n",
      "Epoch 178/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3829 - acc: 0.8694 - val_loss: 0.3823 - val_acc: 0.8699\n",
      "Epoch 179/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3828 - acc: 0.8723 - val_loss: 0.3778 - val_acc: 0.8753\n",
      "Epoch 180/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3818 - acc: 0.8732 - val_loss: 0.3809 - val_acc: 0.8669\n",
      "Epoch 181/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3827 - acc: 0.8698 - val_loss: 0.3768 - val_acc: 0.8753\n",
      "Epoch 182/300\n",
      "7143/7143 [==============================] - 0s 14us/step - loss: 0.3778 - acc: 0.8729 - val_loss: 0.3750 - val_acc: 0.8789\n",
      "Epoch 183/300\n",
      "7143/7143 [==============================] - 0s 14us/step - loss: 0.3776 - acc: 0.8734 - val_loss: 0.3755 - val_acc: 0.8802\n",
      "Epoch 184/300\n",
      "7143/7143 [==============================] - 0s 14us/step - loss: 0.3754 - acc: 0.8757 - val_loss: 0.3770 - val_acc: 0.8810\n",
      "Epoch 185/300\n",
      "7143/7143 [==============================] - 916s 128ms/step - loss: 0.3749 - acc: 0.8750 - val_loss: 0.3712 - val_acc: 0.8725\n",
      "Epoch 186/300\n",
      "7143/7143 [==============================] - 0s 54us/step - loss: 0.3730 - acc: 0.8778 - val_loss: 0.3723 - val_acc: 0.8727\n",
      "Epoch 187/300\n",
      "7143/7143 [==============================] - 1s 130us/step - loss: 0.3736 - acc: 0.8722 - val_loss: 0.3683 - val_acc: 0.8755\n",
      "Epoch 188/300\n",
      "7143/7143 [==============================] - 0s 36us/step - loss: 0.3706 - acc: 0.8734 - val_loss: 0.3766 - val_acc: 0.8681\n",
      "Epoch 189/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3693 - acc: 0.8758 - val_loss: 0.3676 - val_acc: 0.8783\n",
      "Epoch 190/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3682 - acc: 0.8785 - val_loss: 0.3627 - val_acc: 0.8739\n",
      "Epoch 191/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3661 - acc: 0.8741 - val_loss: 0.3613 - val_acc: 0.8786\n",
      "Epoch 192/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3685 - acc: 0.8746 - val_loss: 0.3627 - val_acc: 0.8765\n",
      "Epoch 193/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3626 - acc: 0.8761 - val_loss: 0.3598 - val_acc: 0.8786\n",
      "Epoch 194/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3645 - acc: 0.8755 - val_loss: 0.3621 - val_acc: 0.8781\n",
      "Epoch 195/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3600 - acc: 0.8757 - val_loss: 0.3596 - val_acc: 0.8778\n",
      "Epoch 196/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3607 - acc: 0.8792 - val_loss: 0.3583 - val_acc: 0.8824\n",
      "Epoch 197/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3607 - acc: 0.8729 - val_loss: 0.3613 - val_acc: 0.8859\n",
      "Epoch 198/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3574 - acc: 0.8785 - val_loss: 0.3544 - val_acc: 0.8835\n",
      "Epoch 199/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3565 - acc: 0.8771 - val_loss: 0.3571 - val_acc: 0.8762\n",
      "Epoch 200/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3592 - acc: 0.8767 - val_loss: 0.3551 - val_acc: 0.8831\n",
      "Epoch 201/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3545 - acc: 0.8767 - val_loss: 0.3518 - val_acc: 0.8783\n",
      "Epoch 202/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3545 - acc: 0.8789 - val_loss: 0.3527 - val_acc: 0.8744\n",
      "Epoch 203/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3542 - acc: 0.8771 - val_loss: 0.3630 - val_acc: 0.8725\n",
      "Epoch 204/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.3527 - acc: 0.8746 - val_loss: 0.3499 - val_acc: 0.8792\n",
      "Epoch 205/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3503 - acc: 0.8802 - val_loss: 0.3579 - val_acc: 0.8838\n",
      "Epoch 206/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3496 - acc: 0.8789 - val_loss: 0.3481 - val_acc: 0.8831\n",
      "Epoch 207/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3475 - acc: 0.8782 - val_loss: 0.3433 - val_acc: 0.8827\n",
      "Epoch 208/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3484 - acc: 0.8789 - val_loss: 0.3477 - val_acc: 0.8806\n",
      "Epoch 209/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3465 - acc: 0.8807 - val_loss: 0.3430 - val_acc: 0.8774\n",
      "Epoch 210/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3444 - acc: 0.8785 - val_loss: 0.3432 - val_acc: 0.8797\n",
      "Epoch 211/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3454 - acc: 0.8810 - val_loss: 0.3467 - val_acc: 0.8834\n",
      "Epoch 212/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3444 - acc: 0.8813 - val_loss: 0.3390 - val_acc: 0.8830\n",
      "Epoch 213/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3426 - acc: 0.8804 - val_loss: 0.3376 - val_acc: 0.8813\n",
      "Epoch 214/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3425 - acc: 0.8792 - val_loss: 0.3365 - val_acc: 0.8866\n",
      "Epoch 215/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3396 - acc: 0.8810 - val_loss: 0.3361 - val_acc: 0.8802\n",
      "Epoch 216/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3394 - acc: 0.8827 - val_loss: 0.3368 - val_acc: 0.8844\n",
      "Epoch 217/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3369 - acc: 0.8825 - val_loss: 0.3400 - val_acc: 0.8849\n",
      "Epoch 218/300\n",
      "7143/7143 [==============================] - 0s 41us/step - loss: 0.3371 - acc: 0.8820 - val_loss: 0.3329 - val_acc: 0.8835\n",
      "Epoch 219/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.3354 - acc: 0.8807 - val_loss: 0.3414 - val_acc: 0.8835\n",
      "Epoch 220/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3349 - acc: 0.8828 - val_loss: 0.3314 - val_acc: 0.8786\n",
      "Epoch 221/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3330 - acc: 0.8832 - val_loss: 0.3286 - val_acc: 0.8846\n",
      "Epoch 222/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.3325 - acc: 0.8838 - val_loss: 0.3335 - val_acc: 0.8860\n",
      "Epoch 223/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3315 - acc: 0.8851 - val_loss: 0.3292 - val_acc: 0.8823\n",
      "Epoch 224/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3305 - acc: 0.8862 - val_loss: 0.3298 - val_acc: 0.8797\n",
      "Epoch 225/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3309 - acc: 0.8851 - val_loss: 0.3292 - val_acc: 0.8831\n",
      "Epoch 226/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3286 - acc: 0.8855 - val_loss: 0.3302 - val_acc: 0.8838\n",
      "Epoch 227/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3289 - acc: 0.8851 - val_loss: 0.3237 - val_acc: 0.8855\n",
      "Epoch 228/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3282 - acc: 0.8823 - val_loss: 0.3272 - val_acc: 0.8900\n",
      "Epoch 229/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3248 - acc: 0.8876 - val_loss: 0.3269 - val_acc: 0.8811\n",
      "Epoch 230/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3242 - acc: 0.8859 - val_loss: 0.3262 - val_acc: 0.8863\n",
      "Epoch 231/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3273 - acc: 0.8862 - val_loss: 0.3217 - val_acc: 0.8876\n",
      "Epoch 232/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3225 - acc: 0.8845 - val_loss: 0.3173 - val_acc: 0.8893\n",
      "Epoch 233/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3209 - acc: 0.8877 - val_loss: 0.3222 - val_acc: 0.8845\n",
      "Epoch 234/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3229 - acc: 0.8855 - val_loss: 0.3164 - val_acc: 0.8935\n",
      "Epoch 235/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3203 - acc: 0.8880 - val_loss: 0.3202 - val_acc: 0.8862\n",
      "Epoch 236/300\n",
      "7143/7143 [==============================] - 0s 37us/step - loss: 0.3201 - acc: 0.8855 - val_loss: 0.3205 - val_acc: 0.8849\n",
      "Epoch 237/300\n",
      "7143/7143 [==============================] - 0s 36us/step - loss: 0.3198 - acc: 0.8881 - val_loss: 0.3150 - val_acc: 0.8902\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3205 - acc: 0.8859 - val_loss: 0.3150 - val_acc: 0.8858\n",
      "Epoch 239/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3159 - acc: 0.8869 - val_loss: 0.3198 - val_acc: 0.8810\n",
      "Epoch 240/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3154 - acc: 0.8904 - val_loss: 0.3120 - val_acc: 0.8895\n",
      "Epoch 241/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3156 - acc: 0.8877 - val_loss: 0.3114 - val_acc: 0.8851\n",
      "Epoch 242/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3145 - acc: 0.8870 - val_loss: 0.3117 - val_acc: 0.8881\n",
      "Epoch 243/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3125 - acc: 0.8876 - val_loss: 0.3103 - val_acc: 0.8849\n",
      "Epoch 244/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3099 - acc: 0.8873 - val_loss: 0.3129 - val_acc: 0.8862\n",
      "Epoch 245/300\n",
      "7143/7143 [==============================] - 0s 47us/step - loss: 0.3140 - acc: 0.8879 - val_loss: 0.3111 - val_acc: 0.8849\n",
      "Epoch 246/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.3124 - acc: 0.8872 - val_loss: 0.3137 - val_acc: 0.8893\n",
      "Epoch 247/300\n",
      "7143/7143 [==============================] - 1s 180us/step - loss: 0.3104 - acc: 0.8911 - val_loss: 0.3073 - val_acc: 0.8873\n",
      "Epoch 248/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3106 - acc: 0.8860 - val_loss: 0.3114 - val_acc: 0.8932\n",
      "Epoch 249/300\n",
      "7143/7143 [==============================] - 0s 44us/step - loss: 0.3116 - acc: 0.8869 - val_loss: 0.3041 - val_acc: 0.8914\n",
      "Epoch 250/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.3065 - acc: 0.8918 - val_loss: 0.3093 - val_acc: 0.8963\n",
      "Epoch 251/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3101 - acc: 0.8888 - val_loss: 0.3143 - val_acc: 0.8851\n",
      "Epoch 252/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3097 - acc: 0.8890 - val_loss: 0.3042 - val_acc: 0.8949\n",
      "Epoch 253/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3031 - acc: 0.8923 - val_loss: 0.3011 - val_acc: 0.8895\n",
      "Epoch 254/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3029 - val_acc: 0.8904\n",
      "Epoch 255/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8877 - val_loss: 0.3021 - val_acc: 0.8894\n",
      "Epoch 256/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3031 - acc: 0.8916 - val_loss: 0.3008 - val_acc: 0.8974\n",
      "Epoch 257/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3018 - acc: 0.8926 - val_loss: 0.2987 - val_acc: 0.8970\n",
      "Epoch 258/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3002 - acc: 0.8933 - val_loss: 0.3018 - val_acc: 0.8918\n",
      "Epoch 259/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3003 - acc: 0.8900 - val_loss: 0.2970 - val_acc: 0.8935\n",
      "Epoch 260/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3013 - acc: 0.8907 - val_loss: 0.3068 - val_acc: 0.8888\n",
      "Epoch 261/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3017 - acc: 0.8886 - val_loss: 0.2957 - val_acc: 0.8930\n",
      "Epoch 262/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2978 - acc: 0.8914 - val_loss: 0.3005 - val_acc: 0.8921\n",
      "Epoch 263/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3008 - acc: 0.8922 - val_loss: 0.2985 - val_acc: 0.8961\n",
      "Epoch 264/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2959 - acc: 0.8942 - val_loss: 0.2903 - val_acc: 0.8949\n",
      "Epoch 265/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2977 - acc: 0.8921 - val_loss: 0.2923 - val_acc: 0.8949\n",
      "Epoch 266/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2991 - acc: 0.8923 - val_loss: 0.2959 - val_acc: 0.8921\n",
      "Epoch 267/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2948 - acc: 0.8932 - val_loss: 0.2951 - val_acc: 0.8929\n",
      "Epoch 268/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2928 - acc: 0.8942 - val_loss: 0.2910 - val_acc: 0.9038\n",
      "Epoch 269/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2908 - acc: 0.8942 - val_loss: 0.2939 - val_acc: 0.8916\n",
      "Epoch 270/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2915 - acc: 0.8925 - val_loss: 0.2874 - val_acc: 0.8988\n",
      "Epoch 271/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2932 - acc: 0.8922 - val_loss: 0.2888 - val_acc: 0.9021\n",
      "Epoch 272/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2977 - acc: 0.8933 - val_loss: 0.3020 - val_acc: 0.8943\n",
      "Epoch 273/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2922 - acc: 0.8965 - val_loss: 0.2890 - val_acc: 0.8988\n",
      "Epoch 274/300\n",
      "7143/7143 [==============================] - 0s 44us/step - loss: 0.2897 - acc: 0.8960 - val_loss: 0.2858 - val_acc: 0.8950\n",
      "Epoch 275/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2872 - acc: 0.8970 - val_loss: 0.2969 - val_acc: 0.8890\n",
      "Epoch 276/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2895 - acc: 0.8940 - val_loss: 0.2930 - val_acc: 0.8974\n",
      "Epoch 277/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2932 - acc: 0.8922 - val_loss: 0.2908 - val_acc: 0.8870\n",
      "Epoch 278/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2864 - acc: 0.8961 - val_loss: 0.2834 - val_acc: 0.8947\n",
      "Epoch 279/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2838 - acc: 0.8958 - val_loss: 0.2845 - val_acc: 0.8900\n",
      "Epoch 280/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2858 - acc: 0.8936 - val_loss: 0.2820 - val_acc: 0.8928\n",
      "Epoch 281/300\n",
      "7143/7143 [==============================] - 0s 52us/step - loss: 0.2888 - acc: 0.8937 - val_loss: 0.2828 - val_acc: 0.8961\n",
      "Epoch 282/300\n",
      "7143/7143 [==============================] - 0s 39us/step - loss: 0.2839 - acc: 0.8950 - val_loss: 0.2824 - val_acc: 0.8894\n",
      "Epoch 283/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.2815 - acc: 0.8967 - val_loss: 0.2792 - val_acc: 0.9042\n",
      "Epoch 284/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2841 - acc: 0.8975 - val_loss: 0.2849 - val_acc: 0.8921\n",
      "Epoch 285/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2809 - acc: 0.8958 - val_loss: 0.2793 - val_acc: 0.8970\n",
      "Epoch 286/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2826 - acc: 0.8950 - val_loss: 0.2759 - val_acc: 0.8979\n",
      "Epoch 287/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2805 - acc: 0.8982 - val_loss: 0.2800 - val_acc: 0.9059\n",
      "Epoch 288/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.2807 - acc: 0.8968 - val_loss: 0.2791 - val_acc: 0.8991\n",
      "Epoch 289/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.2790 - acc: 0.8954 - val_loss: 0.2780 - val_acc: 0.9002\n",
      "Epoch 290/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2818 - acc: 0.8957 - val_loss: 0.2756 - val_acc: 0.9037\n",
      "Epoch 291/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.2788 - acc: 0.8956 - val_loss: 0.2759 - val_acc: 0.8971\n",
      "Epoch 292/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2762 - acc: 0.8993 - val_loss: 0.2828 - val_acc: 0.8964\n",
      "Epoch 293/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2792 - acc: 0.8979 - val_loss: 0.2768 - val_acc: 0.9062\n",
      "Epoch 294/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2757 - acc: 0.9007 - val_loss: 0.2722 - val_acc: 0.9012\n",
      "Epoch 295/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2774 - acc: 0.8995 - val_loss: 0.2766 - val_acc: 0.9005\n",
      "Epoch 296/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2752 - acc: 0.8974 - val_loss: 0.2832 - val_acc: 0.8975\n",
      "Epoch 297/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2757 - acc: 0.8982 - val_loss: 0.2719 - val_acc: 0.8942\n",
      "Epoch 298/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2733 - acc: 0.9003 - val_loss: 0.2728 - val_acc: 0.9012\n",
      "Epoch 299/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2740 - acc: 0.8968 - val_loss: 0.2734 - val_acc: 0.9017\n",
      "Epoch 300/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2734 - acc: 0.9020 - val_loss: 0.2761 - val_acc: 0.8992\n",
      "28574/28574 [==============================] - 3s 122us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7143 samples, validate on 7143 samples\n",
      "Epoch 1/300\n",
      "7143/7143 [==============================] - 5s 759us/step - loss: 3.3622 - acc: 0.0659 - val_loss: 3.3121 - val_acc: 0.1040\n",
      "Epoch 2/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 3.2538 - acc: 0.0911 - val_loss: 3.1702 - val_acc: 0.1019\n",
      "Epoch 3/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 3.0750 - acc: 0.1015 - val_loss: 2.9564 - val_acc: 0.1180\n",
      "Epoch 4/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 2.8638 - acc: 0.1176 - val_loss: 2.7638 - val_acc: 0.1476\n",
      "Epoch 5/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 2.6926 - acc: 0.1648 - val_loss: 2.6101 - val_acc: 0.1947\n",
      "Epoch 6/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 2.5469 - acc: 0.2031 - val_loss: 2.4645 - val_acc: 0.2811\n",
      "Epoch 7/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 2.3991 - acc: 0.2887 - val_loss: 2.3168 - val_acc: 0.2925\n",
      "Epoch 8/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 2.2539 - acc: 0.3038 - val_loss: 2.1769 - val_acc: 0.3692\n",
      "Epoch 9/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 2.1253 - acc: 0.3457 - val_loss: 2.0557 - val_acc: 0.3434\n",
      "Epoch 10/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 2.0112 - acc: 0.3651 - val_loss: 1.9529 - val_acc: 0.3872\n",
      "Epoch 11/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 1.9156 - acc: 0.3814 - val_loss: 1.8586 - val_acc: 0.3994\n",
      "Epoch 12/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.8272 - acc: 0.4119 - val_loss: 1.8120 - val_acc: 0.4189\n",
      "Epoch 13/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 1.7571 - acc: 0.4283 - val_loss: 1.7011 - val_acc: 0.4449\n",
      "Epoch 14/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 1.6800 - acc: 0.4612 - val_loss: 1.6415 - val_acc: 0.5219\n",
      "Epoch 15/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 1.6253 - acc: 0.4775 - val_loss: 1.5943 - val_acc: 0.4837\n",
      "Epoch 16/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 1.5651 - acc: 0.5037 - val_loss: 1.5386 - val_acc: 0.4935\n",
      "Epoch 17/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 1.5361 - acc: 0.4956 - val_loss: 1.4900 - val_acc: 0.5390\n",
      "Epoch 18/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 1.4816 - acc: 0.5183 - val_loss: 1.4453 - val_acc: 0.5307\n",
      "Epoch 19/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 1.4367 - acc: 0.5191 - val_loss: 1.4306 - val_acc: 0.5257\n",
      "Epoch 20/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 1.4136 - acc: 0.5355 - val_loss: 1.4281 - val_acc: 0.4924\n",
      "Epoch 21/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 1.3929 - acc: 0.5208 - val_loss: 1.4123 - val_acc: 0.4900\n",
      "Epoch 22/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.3680 - acc: 0.5352 - val_loss: 1.3258 - val_acc: 0.5516\n",
      "Epoch 23/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.3524 - acc: 0.5225 - val_loss: 1.3930 - val_acc: 0.4456\n",
      "Epoch 24/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 1.3445 - acc: 0.4997 - val_loss: 1.2904 - val_acc: 0.5845\n",
      "Epoch 25/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 1.3644 - acc: 0.4686 - val_loss: 1.2727 - val_acc: 0.5722\n",
      "Epoch 26/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.2919 - acc: 0.5353 - val_loss: 1.2525 - val_acc: 0.5566\n",
      "Epoch 27/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.2895 - acc: 0.5127 - val_loss: 1.2480 - val_acc: 0.5465\n",
      "Epoch 28/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.2394 - acc: 0.5582 - val_loss: 1.2633 - val_acc: 0.4893\n",
      "Epoch 29/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.2438 - acc: 0.5250 - val_loss: 1.3216 - val_acc: 0.4792\n",
      "Epoch 30/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.2869 - acc: 0.4883 - val_loss: 1.2631 - val_acc: 0.5212\n",
      "Epoch 31/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.2225 - acc: 0.5383 - val_loss: 1.2464 - val_acc: 0.4778\n",
      "Epoch 32/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.2458 - acc: 0.4887 - val_loss: 1.1885 - val_acc: 0.5404\n",
      "Epoch 33/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.2224 - acc: 0.5190 - val_loss: 1.3110 - val_acc: 0.4885\n",
      "Epoch 34/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.1920 - acc: 0.5386 - val_loss: 1.2514 - val_acc: 0.4766\n",
      "Epoch 35/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.1600 - acc: 0.5533 - val_loss: 1.2053 - val_acc: 0.5244\n",
      "Epoch 36/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.1618 - acc: 0.5583 - val_loss: 1.0900 - val_acc: 0.6065\n",
      "Epoch 37/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.1402 - acc: 0.5624 - val_loss: 1.2099 - val_acc: 0.5065\n",
      "Epoch 38/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.1209 - acc: 0.5621 - val_loss: 1.0461 - val_acc: 0.6198\n",
      "Epoch 39/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.1086 - acc: 0.5622 - val_loss: 1.3394 - val_acc: 0.4784\n",
      "Epoch 40/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0773 - acc: 0.5969 - val_loss: 1.1556 - val_acc: 0.5358\n",
      "Epoch 41/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.1206 - acc: 0.5619 - val_loss: 1.0104 - val_acc: 0.6296\n",
      "Epoch 42/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.0108 - acc: 0.6349 - val_loss: 1.0163 - val_acc: 0.6210\n",
      "Epoch 43/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.0786 - acc: 0.5650 - val_loss: 1.0993 - val_acc: 0.5467\n",
      "Epoch 44/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0661 - acc: 0.5698 - val_loss: 1.0741 - val_acc: 0.5764\n",
      "Epoch 45/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0557 - acc: 0.5776 - val_loss: 1.0405 - val_acc: 0.5733\n",
      "Epoch 46/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9920 - acc: 0.6259 - val_loss: 1.0089 - val_acc: 0.6013\n",
      "Epoch 47/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.0284 - acc: 0.5825 - val_loss: 0.9892 - val_acc: 0.6361\n",
      "Epoch 48/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0509 - acc: 0.5794 - val_loss: 1.0279 - val_acc: 0.6087\n",
      "Epoch 49/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0474 - acc: 0.5890 - val_loss: 1.1645 - val_acc: 0.5181\n",
      "Epoch 50/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.0307 - acc: 0.5804 - val_loss: 1.0187 - val_acc: 0.5867\n",
      "Epoch 51/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.9213 - acc: 0.6801 - val_loss: 1.0799 - val_acc: 0.5573\n",
      "Epoch 52/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.0348 - acc: 0.5703 - val_loss: 0.9554 - val_acc: 0.6104\n",
      "Epoch 53/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.9086 - acc: 0.6626 - val_loss: 0.9336 - val_acc: 0.6326\n",
      "Epoch 54/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9658 - acc: 0.6094 - val_loss: 1.0039 - val_acc: 0.5968\n",
      "Epoch 55/300\n",
      "7143/7143 [==============================] - 0s 15us/step - loss: 0.9171 - acc: 0.6508 - val_loss: 0.9129 - val_acc: 0.6283\n",
      "Epoch 56/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.9590 - acc: 0.6063 - val_loss: 0.8509 - val_acc: 0.7165\n",
      "Epoch 57/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.9224 - acc: 0.6478 - val_loss: 1.0209 - val_acc: 0.5953\n",
      "Epoch 58/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9339 - acc: 0.6289 - val_loss: 0.9094 - val_acc: 0.6475\n",
      "Epoch 59/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9145 - acc: 0.6282 - val_loss: 0.8763 - val_acc: 0.6506\n",
      "Epoch 60/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9350 - acc: 0.6115 - val_loss: 0.8435 - val_acc: 0.6839\n",
      "Epoch 61/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.8412 - acc: 0.6976 - val_loss: 0.8388 - val_acc: 0.6898\n",
      "Epoch 62/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.8737 - acc: 0.6646 - val_loss: 1.0002 - val_acc: 0.6132\n",
      "Epoch 63/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.8930 - acc: 0.6492 - val_loss: 0.8026 - val_acc: 0.7374\n",
      "Epoch 64/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.8222 - acc: 0.7099 - val_loss: 0.7763 - val_acc: 0.7151\n",
      "Epoch 65/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.8247 - acc: 0.6878 - val_loss: 0.8134 - val_acc: 0.7059\n",
      "Epoch 66/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.8522 - acc: 0.6566 - val_loss: 0.8255 - val_acc: 0.7012\n",
      "Epoch 67/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.8424 - acc: 0.6713 - val_loss: 0.8109 - val_acc: 0.7085\n",
      "Epoch 68/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.7577 - acc: 0.7344 - val_loss: 0.7594 - val_acc: 0.7581\n",
      "Epoch 69/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.7884 - acc: 0.7078 - val_loss: 0.8207 - val_acc: 0.6801\n",
      "Epoch 70/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.8205 - acc: 0.6679 - val_loss: 0.9093 - val_acc: 0.6399\n",
      "Epoch 71/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.8598 - acc: 0.6464 - val_loss: 0.9548 - val_acc: 0.5986\n",
      "Epoch 72/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.8125 - acc: 0.6802 - val_loss: 0.7386 - val_acc: 0.7474\n",
      "Epoch 73/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.8184 - acc: 0.6793 - val_loss: 0.8176 - val_acc: 0.6667\n",
      "Epoch 74/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.7934 - acc: 0.6854 - val_loss: 0.7713 - val_acc: 0.6790\n",
      "Epoch 75/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6994 - acc: 0.7656 - val_loss: 0.6659 - val_acc: 0.8088\n",
      "Epoch 76/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6902 - acc: 0.7781 - val_loss: 0.7000 - val_acc: 0.7631\n",
      "Epoch 77/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.7269 - acc: 0.7418 - val_loss: 0.7065 - val_acc: 0.7277\n",
      "Epoch 78/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.7020 - acc: 0.7662 - val_loss: 0.7265 - val_acc: 0.7235\n",
      "Epoch 79/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.7236 - acc: 0.7483 - val_loss: 0.6588 - val_acc: 0.7991\n",
      "Epoch 80/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.6475 - acc: 0.8051 - val_loss: 0.6287 - val_acc: 0.8174\n",
      "Epoch 81/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.6353 - acc: 0.8085 - val_loss: 0.6323 - val_acc: 0.8180\n",
      "Epoch 82/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.6313 - acc: 0.8051 - val_loss: 0.6248 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.6312 - acc: 0.8039 - val_loss: 0.6148 - val_acc: 0.8334\n",
      "Epoch 84/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.6382 - acc: 0.7957 - val_loss: 0.6225 - val_acc: 0.7999\n",
      "Epoch 85/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.6265 - acc: 0.8064 - val_loss: 0.6462 - val_acc: 0.7640\n",
      "Epoch 86/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.6182 - acc: 0.8020 - val_loss: 0.5986 - val_acc: 0.8160\n",
      "Epoch 87/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.6041 - acc: 0.8114 - val_loss: 0.6199 - val_acc: 0.7910\n",
      "Epoch 88/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5961 - acc: 0.8153 - val_loss: 0.6095 - val_acc: 0.7927\n",
      "Epoch 89/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5901 - acc: 0.8230 - val_loss: 0.6018 - val_acc: 0.8221\n",
      "Epoch 90/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.5843 - acc: 0.8285 - val_loss: 0.5787 - val_acc: 0.8412\n",
      "Epoch 91/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.5790 - acc: 0.8330 - val_loss: 0.5980 - val_acc: 0.8121\n",
      "Epoch 92/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5814 - acc: 0.8225 - val_loss: 0.5727 - val_acc: 0.8209\n",
      "Epoch 93/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5730 - acc: 0.8215 - val_loss: 0.5713 - val_acc: 0.8386\n",
      "Epoch 94/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5666 - acc: 0.8282 - val_loss: 0.5574 - val_acc: 0.8446\n",
      "Epoch 95/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.5674 - acc: 0.8288 - val_loss: 0.5625 - val_acc: 0.8222\n",
      "Epoch 96/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.5583 - acc: 0.8303 - val_loss: 0.5612 - val_acc: 0.8209\n",
      "Epoch 97/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5556 - acc: 0.8314 - val_loss: 0.5512 - val_acc: 0.8390\n",
      "Epoch 98/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.5528 - acc: 0.8337 - val_loss: 0.5489 - val_acc: 0.8415\n",
      "Epoch 99/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.5516 - acc: 0.8362 - val_loss: 0.5518 - val_acc: 0.8230\n",
      "Epoch 100/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.5475 - acc: 0.8368 - val_loss: 0.5375 - val_acc: 0.8384\n",
      "Epoch 101/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5411 - acc: 0.8281 - val_loss: 0.5367 - val_acc: 0.8362\n",
      "Epoch 102/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5375 - acc: 0.8405 - val_loss: 0.5295 - val_acc: 0.8461\n",
      "Epoch 103/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5344 - acc: 0.8365 - val_loss: 0.5300 - val_acc: 0.8370\n",
      "Epoch 104/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5311 - acc: 0.8352 - val_loss: 0.5286 - val_acc: 0.8345\n",
      "Epoch 105/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5304 - acc: 0.8380 - val_loss: 0.5212 - val_acc: 0.8264\n",
      "Epoch 106/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5253 - acc: 0.8396 - val_loss: 0.5193 - val_acc: 0.8488\n",
      "Epoch 107/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5225 - acc: 0.8389 - val_loss: 0.5175 - val_acc: 0.8303\n",
      "Epoch 108/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5190 - acc: 0.8348 - val_loss: 0.5139 - val_acc: 0.8439\n",
      "Epoch 109/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5191 - acc: 0.8366 - val_loss: 0.5213 - val_acc: 0.8412\n",
      "Epoch 110/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5122 - acc: 0.8424 - val_loss: 0.5049 - val_acc: 0.8279\n",
      "Epoch 111/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.5102 - acc: 0.8368 - val_loss: 0.5057 - val_acc: 0.8473\n",
      "Epoch 112/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.5104 - acc: 0.8429 - val_loss: 0.4974 - val_acc: 0.8502\n",
      "Epoch 113/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.5070 - acc: 0.8415 - val_loss: 0.4987 - val_acc: 0.8474\n",
      "Epoch 114/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4983 - acc: 0.8440 - val_loss: 0.4956 - val_acc: 0.8488\n",
      "Epoch 115/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4958 - acc: 0.8398 - val_loss: 0.4906 - val_acc: 0.8480\n",
      "Epoch 116/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4965 - acc: 0.8447 - val_loss: 0.4911 - val_acc: 0.8491\n",
      "Epoch 117/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4927 - acc: 0.8450 - val_loss: 0.4859 - val_acc: 0.8429\n",
      "Epoch 118/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4873 - acc: 0.8422 - val_loss: 0.4901 - val_acc: 0.8379\n",
      "Epoch 119/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4857 - acc: 0.8452 - val_loss: 0.4831 - val_acc: 0.8257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4824 - acc: 0.8466 - val_loss: 0.4816 - val_acc: 0.8460\n",
      "Epoch 121/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4829 - acc: 0.8454 - val_loss: 0.4779 - val_acc: 0.8509\n",
      "Epoch 122/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4802 - acc: 0.8475 - val_loss: 0.4761 - val_acc: 0.8356\n",
      "Epoch 123/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4757 - acc: 0.8463 - val_loss: 0.4778 - val_acc: 0.8485\n",
      "Epoch 124/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4736 - acc: 0.8481 - val_loss: 0.4701 - val_acc: 0.8517\n",
      "Epoch 125/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4721 - acc: 0.8481 - val_loss: 0.4667 - val_acc: 0.8522\n",
      "Epoch 126/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4669 - acc: 0.8508 - val_loss: 0.4661 - val_acc: 0.8491\n",
      "Epoch 127/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4649 - acc: 0.8517 - val_loss: 0.4616 - val_acc: 0.8484\n",
      "Epoch 128/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4623 - acc: 0.8481 - val_loss: 0.4588 - val_acc: 0.8571\n",
      "Epoch 129/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4609 - acc: 0.8502 - val_loss: 0.4617 - val_acc: 0.8520\n",
      "Epoch 130/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4621 - acc: 0.8509 - val_loss: 0.4618 - val_acc: 0.8452\n",
      "Epoch 131/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4618 - acc: 0.8452 - val_loss: 0.4530 - val_acc: 0.8576\n",
      "Epoch 132/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4547 - acc: 0.8547 - val_loss: 0.4519 - val_acc: 0.8502\n",
      "Epoch 133/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4543 - acc: 0.8519 - val_loss: 0.4495 - val_acc: 0.8533\n",
      "Epoch 134/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4506 - acc: 0.8530 - val_loss: 0.4469 - val_acc: 0.8621\n",
      "Epoch 135/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4490 - acc: 0.8520 - val_loss: 0.4478 - val_acc: 0.8578\n",
      "Epoch 136/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.4446 - acc: 0.8550 - val_loss: 0.4428 - val_acc: 0.8534\n",
      "Epoch 137/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4465 - acc: 0.8531 - val_loss: 0.4456 - val_acc: 0.8550\n",
      "Epoch 138/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4439 - acc: 0.8550 - val_loss: 0.4455 - val_acc: 0.8565\n",
      "Epoch 139/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4405 - acc: 0.8566 - val_loss: 0.4381 - val_acc: 0.8552\n",
      "Epoch 140/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4379 - acc: 0.8559 - val_loss: 0.4364 - val_acc: 0.8569\n",
      "Epoch 141/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4359 - acc: 0.8590 - val_loss: 0.4349 - val_acc: 0.8611\n",
      "Epoch 142/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4360 - acc: 0.8558 - val_loss: 0.4328 - val_acc: 0.8585\n",
      "Epoch 143/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4325 - acc: 0.8565 - val_loss: 0.4344 - val_acc: 0.8621\n",
      "Epoch 144/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4317 - acc: 0.8586 - val_loss: 0.4272 - val_acc: 0.8628\n",
      "Epoch 145/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4301 - acc: 0.8589 - val_loss: 0.4243 - val_acc: 0.8641\n",
      "Epoch 146/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.4250 - acc: 0.8607 - val_loss: 0.4230 - val_acc: 0.8516\n",
      "Epoch 147/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4263 - acc: 0.8590 - val_loss: 0.4214 - val_acc: 0.8629\n",
      "Epoch 148/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.4218 - acc: 0.8620 - val_loss: 0.4180 - val_acc: 0.8632\n",
      "Epoch 149/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4197 - acc: 0.8625 - val_loss: 0.4196 - val_acc: 0.8673\n",
      "Epoch 150/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4195 - acc: 0.8596 - val_loss: 0.4135 - val_acc: 0.8590\n",
      "Epoch 151/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4151 - acc: 0.8611 - val_loss: 0.4172 - val_acc: 0.8601\n",
      "Epoch 152/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4155 - acc: 0.8620 - val_loss: 0.4117 - val_acc: 0.8608\n",
      "Epoch 153/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4164 - acc: 0.8571 - val_loss: 0.4086 - val_acc: 0.8599\n",
      "Epoch 154/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4103 - acc: 0.8607 - val_loss: 0.4147 - val_acc: 0.8624\n",
      "Epoch 155/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4110 - acc: 0.8632 - val_loss: 0.4048 - val_acc: 0.8642\n",
      "Epoch 156/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4048 - acc: 0.8624 - val_loss: 0.4097 - val_acc: 0.8578\n",
      "Epoch 157/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4061 - acc: 0.8610 - val_loss: 0.4040 - val_acc: 0.8669\n",
      "Epoch 158/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4062 - acc: 0.8652 - val_loss: 0.4095 - val_acc: 0.8698\n",
      "Epoch 159/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4035 - acc: 0.8620 - val_loss: 0.4027 - val_acc: 0.8662\n",
      "Epoch 160/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4020 - acc: 0.8634 - val_loss: 0.4000 - val_acc: 0.8628\n",
      "Epoch 161/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4008 - acc: 0.8632 - val_loss: 0.3974 - val_acc: 0.8610\n",
      "Epoch 162/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3984 - acc: 0.8664 - val_loss: 0.3933 - val_acc: 0.8702\n",
      "Epoch 163/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3951 - acc: 0.8636 - val_loss: 0.3974 - val_acc: 0.8704\n",
      "Epoch 164/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3938 - acc: 0.8649 - val_loss: 0.3927 - val_acc: 0.8690\n",
      "Epoch 165/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3944 - acc: 0.8656 - val_loss: 0.3917 - val_acc: 0.8701\n",
      "Epoch 166/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3903 - acc: 0.8656 - val_loss: 0.3900 - val_acc: 0.8691\n",
      "Epoch 167/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3935 - acc: 0.8642 - val_loss: 0.3848 - val_acc: 0.8720\n",
      "Epoch 168/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.3883 - acc: 0.8691 - val_loss: 0.3881 - val_acc: 0.8685\n",
      "Epoch 169/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3908 - acc: 0.8681 - val_loss: 0.3847 - val_acc: 0.8726\n",
      "Epoch 170/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3875 - acc: 0.8692 - val_loss: 0.3833 - val_acc: 0.8639\n",
      "Epoch 171/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3832 - acc: 0.8691 - val_loss: 0.3813 - val_acc: 0.8708\n",
      "Epoch 172/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3834 - acc: 0.8680 - val_loss: 0.3823 - val_acc: 0.8720\n",
      "Epoch 173/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3841 - acc: 0.8664 - val_loss: 0.3785 - val_acc: 0.8716\n",
      "Epoch 174/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3819 - acc: 0.8719 - val_loss: 0.3764 - val_acc: 0.8765\n",
      "Epoch 175/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3800 - acc: 0.8702 - val_loss: 0.3763 - val_acc: 0.8664\n",
      "Epoch 176/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3788 - acc: 0.8715 - val_loss: 0.3753 - val_acc: 0.8692\n",
      "Epoch 177/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3775 - acc: 0.8680 - val_loss: 0.3766 - val_acc: 0.8705\n",
      "Epoch 178/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3790 - acc: 0.8699 - val_loss: 0.3734 - val_acc: 0.8673\n",
      "Epoch 179/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3756 - acc: 0.8732 - val_loss: 0.3686 - val_acc: 0.8727\n",
      "Epoch 180/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3749 - acc: 0.8690 - val_loss: 0.3689 - val_acc: 0.8755\n",
      "Epoch 181/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3709 - acc: 0.8729 - val_loss: 0.3739 - val_acc: 0.8681\n",
      "Epoch 182/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3713 - acc: 0.8715 - val_loss: 0.3675 - val_acc: 0.8760\n",
      "Epoch 183/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3691 - acc: 0.8753 - val_loss: 0.3676 - val_acc: 0.8694\n",
      "Epoch 184/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3663 - acc: 0.8748 - val_loss: 0.3603 - val_acc: 0.8734\n",
      "Epoch 185/300\n",
      "7143/7143 [==============================] - 0s 35us/step - loss: 0.3639 - acc: 0.8747 - val_loss: 0.3640 - val_acc: 0.8762\n",
      "Epoch 186/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3642 - acc: 0.8734 - val_loss: 0.3627 - val_acc: 0.8739\n",
      "Epoch 187/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3649 - acc: 0.8726 - val_loss: 0.3587 - val_acc: 0.8741\n",
      "Epoch 188/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3610 - acc: 0.8743 - val_loss: 0.3584 - val_acc: 0.8753\n",
      "Epoch 189/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.3630 - acc: 0.8730 - val_loss: 0.3650 - val_acc: 0.8638\n",
      "Epoch 190/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3616 - acc: 0.8739 - val_loss: 0.3596 - val_acc: 0.8719\n",
      "Epoch 191/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3606 - acc: 0.8726 - val_loss: 0.3602 - val_acc: 0.8652\n",
      "Epoch 192/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3564 - acc: 0.8772 - val_loss: 0.3540 - val_acc: 0.8779\n",
      "Epoch 193/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3550 - acc: 0.8779 - val_loss: 0.3518 - val_acc: 0.8743\n",
      "Epoch 194/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3516 - acc: 0.8795 - val_loss: 0.3539 - val_acc: 0.8831\n",
      "Epoch 195/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3506 - acc: 0.8800 - val_loss: 0.3482 - val_acc: 0.8853\n",
      "Epoch 196/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3522 - acc: 0.8783 - val_loss: 0.3548 - val_acc: 0.8915\n",
      "Epoch 197/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.3506 - acc: 0.8800 - val_loss: 0.3453 - val_acc: 0.8859\n",
      "Epoch 198/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3484 - acc: 0.8809 - val_loss: 0.3461 - val_acc: 0.8778\n",
      "Epoch 199/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3505 - acc: 0.8792 - val_loss: 0.3485 - val_acc: 0.8688\n",
      "Epoch 200/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3474 - acc: 0.8767 - val_loss: 0.3532 - val_acc: 0.8818\n",
      "Epoch 201/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3461 - acc: 0.8800 - val_loss: 0.3422 - val_acc: 0.8778\n",
      "Epoch 202/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3462 - acc: 0.8800 - val_loss: 0.3379 - val_acc: 0.8823\n",
      "Epoch 203/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3446 - acc: 0.8842 - val_loss: 0.3409 - val_acc: 0.8905\n",
      "Epoch 204/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3433 - acc: 0.8832 - val_loss: 0.3374 - val_acc: 0.8837\n",
      "Epoch 205/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3414 - acc: 0.8827 - val_loss: 0.3433 - val_acc: 0.8929\n",
      "Epoch 206/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3396 - acc: 0.8873 - val_loss: 0.3383 - val_acc: 0.8769\n",
      "Epoch 207/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3388 - acc: 0.8809 - val_loss: 0.3361 - val_acc: 0.8799\n",
      "Epoch 208/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3354 - acc: 0.8876 - val_loss: 0.3341 - val_acc: 0.8817\n",
      "Epoch 209/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3360 - acc: 0.8856 - val_loss: 0.3319 - val_acc: 0.8914\n",
      "Epoch 210/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3349 - acc: 0.8855 - val_loss: 0.3327 - val_acc: 0.8898\n",
      "Epoch 211/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3332 - acc: 0.8886 - val_loss: 0.3344 - val_acc: 0.8916\n",
      "Epoch 212/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3343 - acc: 0.8859 - val_loss: 0.3335 - val_acc: 0.8921\n",
      "Epoch 213/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3324 - acc: 0.8872 - val_loss: 0.3333 - val_acc: 0.8810\n",
      "Epoch 214/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3314 - acc: 0.8832 - val_loss: 0.3306 - val_acc: 0.8909\n",
      "Epoch 215/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3308 - acc: 0.8872 - val_loss: 0.3262 - val_acc: 0.8978\n",
      "Epoch 216/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3281 - acc: 0.8915 - val_loss: 0.3279 - val_acc: 0.8824\n",
      "Epoch 217/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3252 - acc: 0.8894 - val_loss: 0.3306 - val_acc: 0.8779\n",
      "Epoch 218/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3263 - acc: 0.8898 - val_loss: 0.3247 - val_acc: 0.8865\n",
      "Epoch 219/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3261 - acc: 0.8888 - val_loss: 0.3246 - val_acc: 0.8859\n",
      "Epoch 220/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3270 - acc: 0.8890 - val_loss: 0.3196 - val_acc: 0.8860\n",
      "Epoch 221/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3240 - acc: 0.8905 - val_loss: 0.3198 - val_acc: 0.8874\n",
      "Epoch 222/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3251 - acc: 0.8880 - val_loss: 0.3200 - val_acc: 0.8937\n",
      "Epoch 223/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3209 - acc: 0.8912 - val_loss: 0.3256 - val_acc: 0.9010\n",
      "Epoch 224/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3238 - acc: 0.8918 - val_loss: 0.3189 - val_acc: 0.9034\n",
      "Epoch 225/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3219 - acc: 0.8909 - val_loss: 0.3188 - val_acc: 0.9035\n",
      "Epoch 226/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3198 - acc: 0.8932 - val_loss: 0.3208 - val_acc: 0.8932\n",
      "Epoch 227/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3162 - acc: 0.8935 - val_loss: 0.3131 - val_acc: 0.8887\n",
      "Epoch 228/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3195 - acc: 0.8926 - val_loss: 0.3196 - val_acc: 0.8932\n",
      "Epoch 229/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3182 - acc: 0.8928 - val_loss: 0.3175 - val_acc: 0.8971\n",
      "Epoch 230/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3132 - acc: 0.8988 - val_loss: 0.3094 - val_acc: 0.8996\n",
      "Epoch 231/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3138 - acc: 0.8944 - val_loss: 0.3114 - val_acc: 0.8851\n",
      "Epoch 232/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3148 - acc: 0.8915 - val_loss: 0.3082 - val_acc: 0.8905\n",
      "Epoch 233/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3158 - acc: 0.8911 - val_loss: 0.3177 - val_acc: 0.8932\n",
      "Epoch 234/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3093 - acc: 0.8964 - val_loss: 0.3131 - val_acc: 0.8884\n",
      "Epoch 235/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3114 - acc: 0.8929 - val_loss: 0.3083 - val_acc: 0.8849\n",
      "Epoch 236/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3119 - acc: 0.8918 - val_loss: 0.3117 - val_acc: 0.8960\n",
      "Epoch 237/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3056 - acc: 0.8986 - val_loss: 0.3051 - val_acc: 0.9012\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3097 - acc: 0.8961 - val_loss: 0.3025 - val_acc: 0.8940\n",
      "Epoch 239/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3072 - acc: 0.8965 - val_loss: 0.3042 - val_acc: 0.9069\n",
      "Epoch 240/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3092 - acc: 0.8939 - val_loss: 0.3056 - val_acc: 0.9056\n",
      "Epoch 241/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3049 - acc: 0.8971 - val_loss: 0.3026 - val_acc: 0.8921\n",
      "Epoch 242/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3038 - acc: 0.8985 - val_loss: 0.3001 - val_acc: 0.9012\n",
      "Epoch 243/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.3019 - acc: 0.8995 - val_loss: 0.2977 - val_acc: 0.9035\n",
      "Epoch 244/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2996 - acc: 0.8991 - val_loss: 0.3019 - val_acc: 0.8975\n",
      "Epoch 245/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.3018 - acc: 0.8985 - val_loss: 0.3063 - val_acc: 0.8916\n",
      "Epoch 246/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2999 - acc: 0.8981 - val_loss: 0.2969 - val_acc: 0.9033\n",
      "Epoch 247/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2987 - acc: 0.9012 - val_loss: 0.3013 - val_acc: 0.9062\n",
      "Epoch 248/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2983 - acc: 0.9030 - val_loss: 0.2921 - val_acc: 0.8988\n",
      "Epoch 249/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2975 - acc: 0.8974 - val_loss: 0.2920 - val_acc: 0.9114\n",
      "Epoch 250/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2958 - acc: 0.9013 - val_loss: 0.2972 - val_acc: 0.9000\n",
      "Epoch 251/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.3019 - acc: 0.8965 - val_loss: 0.3056 - val_acc: 0.8981\n",
      "Epoch 252/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2997 - acc: 0.9026 - val_loss: 0.2978 - val_acc: 0.8995\n",
      "Epoch 253/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2996 - acc: 0.8970 - val_loss: 0.2901 - val_acc: 0.8937\n",
      "Epoch 254/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2954 - acc: 0.9009 - val_loss: 0.3036 - val_acc: 0.8937\n",
      "Epoch 255/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2942 - acc: 0.9021 - val_loss: 0.2916 - val_acc: 0.9115\n",
      "Epoch 256/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2917 - acc: 0.9037 - val_loss: 0.2903 - val_acc: 0.8918\n",
      "Epoch 257/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2912 - acc: 0.9040 - val_loss: 0.2890 - val_acc: 0.9002\n",
      "Epoch 258/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2905 - acc: 0.9033 - val_loss: 0.2960 - val_acc: 0.8989\n",
      "Epoch 259/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2929 - acc: 0.9017 - val_loss: 0.2877 - val_acc: 0.9157\n",
      "Epoch 260/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2905 - acc: 0.9016 - val_loss: 0.2869 - val_acc: 0.9059\n",
      "Epoch 261/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.2871 - acc: 0.9089 - val_loss: 0.2833 - val_acc: 0.9058\n",
      "Epoch 262/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2871 - acc: 0.9049 - val_loss: 0.2856 - val_acc: 0.8964\n",
      "Epoch 263/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2882 - acc: 0.9044 - val_loss: 0.3016 - val_acc: 0.8961\n",
      "Epoch 264/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2829 - acc: 0.9062 - val_loss: 0.2798 - val_acc: 0.9087\n",
      "Epoch 265/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2863 - acc: 0.9045 - val_loss: 0.2838 - val_acc: 0.8995\n",
      "Epoch 266/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2875 - acc: 0.9020 - val_loss: 0.2958 - val_acc: 0.9027\n",
      "Epoch 267/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2821 - acc: 0.9076 - val_loss: 0.2814 - val_acc: 0.9063\n",
      "Epoch 268/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2802 - acc: 0.9096 - val_loss: 0.2802 - val_acc: 0.9094\n",
      "Epoch 269/300\n",
      "7143/7143 [==============================] - ETA: 0s - loss: 0.2846 - acc: 0.901 - 0s 19us/step - loss: 0.2835 - acc: 0.9030 - val_loss: 0.2778 - val_acc: 0.9065\n",
      "Epoch 270/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2802 - acc: 0.9069 - val_loss: 0.2837 - val_acc: 0.9119\n",
      "Epoch 271/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2807 - acc: 0.9097 - val_loss: 0.2781 - val_acc: 0.8985\n",
      "Epoch 272/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2816 - acc: 0.9055 - val_loss: 0.2741 - val_acc: 0.9208\n",
      "Epoch 273/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2822 - acc: 0.9070 - val_loss: 0.2741 - val_acc: 0.9203\n",
      "Epoch 274/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2771 - acc: 0.9139 - val_loss: 0.2722 - val_acc: 0.9034\n",
      "Epoch 275/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2771 - acc: 0.9096 - val_loss: 0.2761 - val_acc: 0.9173\n",
      "Epoch 276/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2776 - acc: 0.9105 - val_loss: 0.2851 - val_acc: 0.9098\n",
      "Epoch 277/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2733 - acc: 0.9128 - val_loss: 0.2763 - val_acc: 0.9233\n",
      "Epoch 278/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2804 - acc: 0.9083 - val_loss: 0.2787 - val_acc: 0.9191\n",
      "Epoch 279/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.2751 - acc: 0.9112 - val_loss: 0.2700 - val_acc: 0.9217\n",
      "Epoch 280/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2764 - acc: 0.9084 - val_loss: 0.2711 - val_acc: 0.9132\n",
      "Epoch 281/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2725 - acc: 0.9114 - val_loss: 0.2708 - val_acc: 0.9094\n",
      "Epoch 282/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2706 - acc: 0.9110 - val_loss: 0.2717 - val_acc: 0.9097\n",
      "Epoch 283/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2745 - acc: 0.9089 - val_loss: 0.2690 - val_acc: 0.9163\n",
      "Epoch 284/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2722 - acc: 0.9086 - val_loss: 0.2744 - val_acc: 0.8993\n",
      "Epoch 285/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2723 - acc: 0.9061 - val_loss: 0.2724 - val_acc: 0.9023\n",
      "Epoch 286/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2697 - acc: 0.9069 - val_loss: 0.2795 - val_acc: 0.9058\n",
      "Epoch 287/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2687 - acc: 0.9096 - val_loss: 0.2717 - val_acc: 0.9065\n",
      "Epoch 288/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2697 - acc: 0.9105 - val_loss: 0.2637 - val_acc: 0.9136\n",
      "Epoch 289/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2716 - acc: 0.9087 - val_loss: 0.2739 - val_acc: 0.8991\n",
      "Epoch 290/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2670 - acc: 0.9121 - val_loss: 0.2642 - val_acc: 0.9084\n",
      "Epoch 291/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2670 - acc: 0.9139 - val_loss: 0.2594 - val_acc: 0.9230\n",
      "Epoch 292/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2679 - acc: 0.9124 - val_loss: 0.2618 - val_acc: 0.9142\n",
      "Epoch 293/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2633 - acc: 0.9161 - val_loss: 0.2677 - val_acc: 0.9104\n",
      "Epoch 294/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2649 - acc: 0.9097 - val_loss: 0.2663 - val_acc: 0.9125\n",
      "Epoch 295/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.2606 - acc: 0.9175 - val_loss: 0.2602 - val_acc: 0.9178\n",
      "Epoch 296/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2644 - acc: 0.9138 - val_loss: 0.2602 - val_acc: 0.9089\n",
      "Epoch 297/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2614 - acc: 0.9143 - val_loss: 0.2642 - val_acc: 0.9012\n",
      "Epoch 298/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.2605 - acc: 0.9153 - val_loss: 0.2651 - val_acc: 0.9121\n",
      "Epoch 299/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2607 - acc: 0.9157 - val_loss: 0.2662 - val_acc: 0.9140\n",
      "Epoch 300/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.2617 - acc: 0.9132 - val_loss: 0.2541 - val_acc: 0.9146\n",
      "28574/28574 [==============================] - 4s 134us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7143 samples, validate on 7143 samples\n",
      "Epoch 1/300\n",
      "7143/7143 [==============================] - 5s 641us/step - loss: 3.3619 - acc: 0.0566 - val_loss: 3.2925 - val_acc: 0.0858\n",
      "Epoch 2/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 3.2186 - acc: 0.0939 - val_loss: 3.1233 - val_acc: 0.0969\n",
      "Epoch 3/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 3.0420 - acc: 0.0809 - val_loss: 2.9465 - val_acc: 0.0756\n",
      "Epoch 4/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 2.8711 - acc: 0.0806 - val_loss: 2.7860 - val_acc: 0.0795\n",
      "Epoch 5/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 2.7259 - acc: 0.0830 - val_loss: 2.6568 - val_acc: 0.0993\n",
      "Epoch 6/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 2.6044 - acc: 0.1554 - val_loss: 2.5386 - val_acc: 0.2226\n",
      "Epoch 7/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 2.4837 - acc: 0.2386 - val_loss: 2.4079 - val_acc: 0.2440\n",
      "Epoch 8/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 2.3453 - acc: 0.2769 - val_loss: 2.2659 - val_acc: 0.2804\n",
      "Epoch 9/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 2.2033 - acc: 0.3035 - val_loss: 2.1271 - val_acc: 0.3202\n",
      "Epoch 10/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 2.0731 - acc: 0.3609 - val_loss: 2.0103 - val_acc: 0.3685\n",
      "Epoch 11/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.9576 - acc: 0.3983 - val_loss: 1.8989 - val_acc: 0.3636\n",
      "Epoch 12/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.8585 - acc: 0.4217 - val_loss: 1.8061 - val_acc: 0.4249\n",
      "Epoch 13/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.7760 - acc: 0.4319 - val_loss: 1.7288 - val_acc: 0.5034\n",
      "Epoch 14/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 1.6994 - acc: 0.4584 - val_loss: 1.6644 - val_acc: 0.4570\n",
      "Epoch 15/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.6342 - acc: 0.4798 - val_loss: 1.5940 - val_acc: 0.4697\n",
      "Epoch 16/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 1.5746 - acc: 0.4844 - val_loss: 1.5362 - val_acc: 0.4844\n",
      "Epoch 17/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 1.5196 - acc: 0.5099 - val_loss: 1.4891 - val_acc: 0.5299\n",
      "Epoch 18/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 1.4724 - acc: 0.5267 - val_loss: 1.4386 - val_acc: 0.5199\n",
      "Epoch 19/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 1.4253 - acc: 0.5374 - val_loss: 1.4064 - val_acc: 0.5352\n",
      "Epoch 20/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.3896 - acc: 0.5373 - val_loss: 1.3758 - val_acc: 0.5372\n",
      "Epoch 21/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.3447 - acc: 0.5649 - val_loss: 1.3273 - val_acc: 0.5307\n",
      "Epoch 22/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.3123 - acc: 0.5600 - val_loss: 1.2916 - val_acc: 0.5832\n",
      "Epoch 23/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 1.2801 - acc: 0.5755 - val_loss: 1.2942 - val_acc: 0.5587\n",
      "Epoch 24/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.2720 - acc: 0.5575 - val_loss: 1.2351 - val_acc: 0.6164\n",
      "Epoch 25/300\n",
      "7143/7143 [==============================] - ETA: 0s - loss: 1.2554 - acc: 0.579 - 0s 18us/step - loss: 1.2499 - acc: 0.5663 - val_loss: 1.2609 - val_acc: 0.5212\n",
      "Epoch 26/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.2391 - acc: 0.5485 - val_loss: 1.2025 - val_acc: 0.6119\n",
      "Epoch 27/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.1851 - acc: 0.5992 - val_loss: 1.1712 - val_acc: 0.5740\n",
      "Epoch 28/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.1523 - acc: 0.6163 - val_loss: 1.1682 - val_acc: 0.6244\n",
      "Epoch 29/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 1.1681 - acc: 0.5855 - val_loss: 1.1371 - val_acc: 0.6088\n",
      "Epoch 30/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.1181 - acc: 0.6216 - val_loss: 1.1458 - val_acc: 0.6296\n",
      "Epoch 31/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 1.1441 - acc: 0.5820 - val_loss: 1.1411 - val_acc: 0.6003\n",
      "Epoch 32/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 1.1171 - acc: 0.5993 - val_loss: 1.0952 - val_acc: 0.6136\n",
      "Epoch 33/300\n",
      "7143/7143 [==============================] - 0s 39us/step - loss: 1.1037 - acc: 0.6011 - val_loss: 1.0672 - val_acc: 0.6493\n",
      "Epoch 34/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.0877 - acc: 0.5997 - val_loss: 1.0962 - val_acc: 0.5996\n",
      "Epoch 35/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.1040 - acc: 0.5850 - val_loss: 1.0823 - val_acc: 0.5654\n",
      "Epoch 36/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 1.0899 - acc: 0.5850 - val_loss: 1.0104 - val_acc: 0.6566\n",
      "Epoch 37/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 1.0396 - acc: 0.6251 - val_loss: 1.0726 - val_acc: 0.5968\n",
      "Epoch 38/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.0395 - acc: 0.6156 - val_loss: 0.9884 - val_acc: 0.7022\n",
      "Epoch 39/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 1.0104 - acc: 0.6255 - val_loss: 1.0309 - val_acc: 0.6125\n",
      "Epoch 40/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 1.0267 - acc: 0.6088 - val_loss: 1.0339 - val_acc: 0.6436\n",
      "Epoch 41/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 1.0398 - acc: 0.6025 - val_loss: 1.0525 - val_acc: 0.5883\n",
      "Epoch 42/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 1.0115 - acc: 0.6168 - val_loss: 0.9360 - val_acc: 0.7092\n",
      "Epoch 43/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.9530 - acc: 0.6627 - val_loss: 0.9269 - val_acc: 0.6536\n",
      "Epoch 44/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.9726 - acc: 0.6261 - val_loss: 1.0684 - val_acc: 0.6030\n",
      "Epoch 45/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.9745 - acc: 0.6384 - val_loss: 1.0073 - val_acc: 0.5969\n",
      "Epoch 46/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.9753 - acc: 0.6167 - val_loss: 0.9217 - val_acc: 0.7273\n",
      "Epoch 47/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.9316 - acc: 0.6609 - val_loss: 0.8804 - val_acc: 0.7024\n",
      "Epoch 48/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.9311 - acc: 0.6630 - val_loss: 1.0152 - val_acc: 0.5712\n",
      "Epoch 49/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.9589 - acc: 0.6219 - val_loss: 1.0121 - val_acc: 0.5583\n",
      "Epoch 50/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.9277 - acc: 0.6513 - val_loss: 0.8578 - val_acc: 0.7005\n",
      "Epoch 51/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.8698 - acc: 0.6942 - val_loss: 0.8938 - val_acc: 0.6654\n",
      "Epoch 52/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.8964 - acc: 0.6637 - val_loss: 0.8973 - val_acc: 0.6601\n",
      "Epoch 53/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.9106 - acc: 0.6459 - val_loss: 0.9089 - val_acc: 0.6571\n",
      "Epoch 54/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.8849 - acc: 0.6571 - val_loss: 0.8826 - val_acc: 0.6409\n",
      "Epoch 55/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.8757 - acc: 0.6657 - val_loss: 0.8832 - val_acc: 0.6718\n",
      "Epoch 56/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.9403 - acc: 0.6275 - val_loss: 0.7997 - val_acc: 0.7138\n",
      "Epoch 57/300\n",
      "7143/7143 [==============================] - 0s 17us/step - loss: 0.7740 - acc: 0.7565 - val_loss: 0.7528 - val_acc: 0.7946\n",
      "Epoch 58/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.7772 - acc: 0.7498 - val_loss: 0.8325 - val_acc: 0.6842\n",
      "Epoch 59/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.7924 - acc: 0.7311 - val_loss: 0.8284 - val_acc: 0.6786\n",
      "Epoch 60/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.8186 - acc: 0.6991 - val_loss: 0.7589 - val_acc: 0.7365\n",
      "Epoch 61/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.7744 - acc: 0.7341 - val_loss: 0.8133 - val_acc: 0.6759\n",
      "Epoch 62/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.7820 - acc: 0.7225 - val_loss: 0.7784 - val_acc: 0.6951\n",
      "Epoch 63/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.8271 - acc: 0.6634 - val_loss: 0.7863 - val_acc: 0.6857\n",
      "Epoch 64/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.7694 - acc: 0.7256 - val_loss: 0.7931 - val_acc: 0.7001\n",
      "Epoch 65/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.8357 - acc: 0.6711 - val_loss: 0.8557 - val_acc: 0.6825\n",
      "Epoch 66/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.7268 - acc: 0.7683 - val_loss: 0.6725 - val_acc: 0.8114\n",
      "Epoch 67/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6799 - acc: 0.7959 - val_loss: 0.6726 - val_acc: 0.8085\n",
      "Epoch 68/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.6932 - acc: 0.7865 - val_loss: 0.7239 - val_acc: 0.7578\n",
      "Epoch 69/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6890 - acc: 0.7813 - val_loss: 0.6645 - val_acc: 0.8176\n",
      "Epoch 70/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6954 - acc: 0.7763 - val_loss: 0.6797 - val_acc: 0.7845\n",
      "Epoch 71/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6680 - acc: 0.7941 - val_loss: 0.6515 - val_acc: 0.8025\n",
      "Epoch 72/300\n",
      "7143/7143 [==============================] - 0s 16us/step - loss: 0.6559 - acc: 0.7995 - val_loss: 0.6386 - val_acc: 0.8260\n",
      "Epoch 73/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.6347 - acc: 0.8188 - val_loss: 0.6357 - val_acc: 0.8306\n",
      "Epoch 74/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.6407 - acc: 0.8123 - val_loss: 0.6516 - val_acc: 0.7985\n",
      "Epoch 75/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6344 - acc: 0.8060 - val_loss: 0.6249 - val_acc: 0.8264\n",
      "Epoch 76/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.6227 - acc: 0.8180 - val_loss: 0.6179 - val_acc: 0.8204\n",
      "Epoch 77/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.6208 - acc: 0.8183 - val_loss: 0.6205 - val_acc: 0.8092\n",
      "Epoch 78/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.6165 - acc: 0.8120 - val_loss: 0.6109 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.6062 - acc: 0.8176 - val_loss: 0.6066 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5992 - acc: 0.8256 - val_loss: 0.5870 - val_acc: 0.8279\n",
      "Epoch 81/300\n",
      "7143/7143 [==============================] - ETA: 0s - loss: 0.5946 - acc: 0.824 - 0s 28us/step - loss: 0.5923 - acc: 0.8254 - val_loss: 0.5908 - val_acc: 0.8179\n",
      "Epoch 82/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5839 - acc: 0.8223 - val_loss: 0.5782 - val_acc: 0.8418\n",
      "Epoch 83/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5837 - acc: 0.8223 - val_loss: 0.5913 - val_acc: 0.8124\n",
      "Epoch 84/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.5752 - acc: 0.8257 - val_loss: 0.5733 - val_acc: 0.8443\n",
      "Epoch 85/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.5719 - acc: 0.8271 - val_loss: 0.5658 - val_acc: 0.8295\n",
      "Epoch 86/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.5674 - acc: 0.8333 - val_loss: 0.5647 - val_acc: 0.8159\n",
      "Epoch 87/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.5758 - acc: 0.8177 - val_loss: 0.5613 - val_acc: 0.8338\n",
      "Epoch 88/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.5589 - acc: 0.8274 - val_loss: 0.5487 - val_acc: 0.8285\n",
      "Epoch 89/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.5573 - acc: 0.8236 - val_loss: 0.5495 - val_acc: 0.8482\n",
      "Epoch 90/300\n",
      "7143/7143 [==============================] - 0s 42us/step - loss: 0.5471 - acc: 0.8355 - val_loss: 0.5557 - val_acc: 0.8148\n",
      "Epoch 91/300\n",
      "7143/7143 [==============================] - 0s 35us/step - loss: 0.5496 - acc: 0.8300 - val_loss: 0.5406 - val_acc: 0.8382\n",
      "Epoch 92/300\n",
      "7143/7143 [==============================] - 0s 39us/step - loss: 0.5401 - acc: 0.8307 - val_loss: 0.5376 - val_acc: 0.8415\n",
      "Epoch 93/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.5393 - acc: 0.8292 - val_loss: 0.5384 - val_acc: 0.8373\n",
      "Epoch 94/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.5329 - acc: 0.8370 - val_loss: 0.5249 - val_acc: 0.8265\n",
      "Epoch 95/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.5262 - acc: 0.8365 - val_loss: 0.5349 - val_acc: 0.8174\n",
      "Epoch 96/300\n",
      "7143/7143 [==============================] - 0s 43us/step - loss: 0.5267 - acc: 0.8309 - val_loss: 0.5296 - val_acc: 0.8306\n",
      "Epoch 97/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5266 - acc: 0.8354 - val_loss: 0.5158 - val_acc: 0.8181\n",
      "Epoch 98/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.5173 - acc: 0.8363 - val_loss: 0.5131 - val_acc: 0.8375\n",
      "Epoch 99/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.5143 - acc: 0.8366 - val_loss: 0.5119 - val_acc: 0.8215\n",
      "Epoch 100/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.5104 - acc: 0.8355 - val_loss: 0.5032 - val_acc: 0.8512\n",
      "Epoch 101/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.5097 - acc: 0.8379 - val_loss: 0.5012 - val_acc: 0.8445\n",
      "Epoch 102/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.5009 - acc: 0.8415 - val_loss: 0.5020 - val_acc: 0.8466\n",
      "Epoch 103/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.5029 - acc: 0.8426 - val_loss: 0.4980 - val_acc: 0.8340\n",
      "Epoch 104/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.4972 - acc: 0.8405 - val_loss: 0.4909 - val_acc: 0.8418\n",
      "Epoch 105/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4949 - acc: 0.8404 - val_loss: 0.4940 - val_acc: 0.8508\n",
      "Epoch 106/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4923 - acc: 0.8438 - val_loss: 0.4830 - val_acc: 0.8509\n",
      "Epoch 107/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.4905 - acc: 0.8447 - val_loss: 0.4860 - val_acc: 0.8512\n",
      "Epoch 108/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.4845 - acc: 0.8499 - val_loss: 0.4806 - val_acc: 0.8495\n",
      "Epoch 109/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.4813 - acc: 0.8449 - val_loss: 0.4757 - val_acc: 0.8527\n",
      "Epoch 110/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.4790 - acc: 0.8485 - val_loss: 0.4801 - val_acc: 0.8403\n",
      "Epoch 111/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.4754 - acc: 0.8471 - val_loss: 0.4771 - val_acc: 0.8481\n",
      "Epoch 112/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4712 - acc: 0.8529 - val_loss: 0.4678 - val_acc: 0.8573\n",
      "Epoch 113/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.4685 - acc: 0.8508 - val_loss: 0.4669 - val_acc: 0.8559\n",
      "Epoch 114/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4641 - acc: 0.8543 - val_loss: 0.4602 - val_acc: 0.8575\n",
      "Epoch 115/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4638 - acc: 0.8468 - val_loss: 0.4598 - val_acc: 0.8544\n",
      "Epoch 116/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.4611 - acc: 0.8544 - val_loss: 0.4537 - val_acc: 0.8603\n",
      "Epoch 117/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.4573 - acc: 0.8548 - val_loss: 0.4517 - val_acc: 0.8564\n",
      "Epoch 118/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4549 - acc: 0.8501 - val_loss: 0.4500 - val_acc: 0.8629\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4512 - acc: 0.8537 - val_loss: 0.4474 - val_acc: 0.8583\n",
      "Epoch 120/300\n",
      "7143/7143 [==============================] - 0s 18us/step - loss: 0.4505 - acc: 0.8529 - val_loss: 0.4530 - val_acc: 0.8501\n",
      "Epoch 121/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.4455 - acc: 0.8576 - val_loss: 0.4467 - val_acc: 0.8613\n",
      "Epoch 122/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.4438 - acc: 0.8566 - val_loss: 0.4414 - val_acc: 0.8438\n",
      "Epoch 123/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.4418 - acc: 0.8524 - val_loss: 0.4394 - val_acc: 0.8625\n",
      "Epoch 124/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.4421 - acc: 0.8543 - val_loss: 0.4357 - val_acc: 0.8600\n",
      "Epoch 125/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4384 - acc: 0.8583 - val_loss: 0.4313 - val_acc: 0.8641\n",
      "Epoch 126/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.4335 - acc: 0.8604 - val_loss: 0.4299 - val_acc: 0.8620\n",
      "Epoch 127/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.4351 - acc: 0.8583 - val_loss: 0.4302 - val_acc: 0.8621\n",
      "Epoch 128/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.4305 - acc: 0.8582 - val_loss: 0.4255 - val_acc: 0.8629\n",
      "Epoch 129/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4269 - acc: 0.8628 - val_loss: 0.4234 - val_acc: 0.8639\n",
      "Epoch 130/300\n",
      "7143/7143 [==============================] - 0s 19us/step - loss: 0.4250 - acc: 0.8617 - val_loss: 0.4283 - val_acc: 0.8664\n",
      "Epoch 131/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.4245 - acc: 0.8613 - val_loss: 0.4202 - val_acc: 0.8618\n",
      "Epoch 132/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.4257 - acc: 0.8582 - val_loss: 0.4159 - val_acc: 0.8649\n",
      "Epoch 133/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.4193 - acc: 0.8604 - val_loss: 0.4162 - val_acc: 0.8664\n",
      "Epoch 134/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.4191 - acc: 0.8624 - val_loss: 0.4143 - val_acc: 0.8645\n",
      "Epoch 135/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.4129 - acc: 0.8643 - val_loss: 0.4106 - val_acc: 0.8655\n",
      "Epoch 136/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.4135 - acc: 0.8620 - val_loss: 0.4105 - val_acc: 0.8650\n",
      "Epoch 137/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.4106 - acc: 0.8649 - val_loss: 0.4087 - val_acc: 0.8656\n",
      "Epoch 138/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.4094 - acc: 0.8635 - val_loss: 0.4071 - val_acc: 0.8711\n",
      "Epoch 139/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.4071 - acc: 0.8666 - val_loss: 0.4074 - val_acc: 0.8667\n",
      "Epoch 140/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.4057 - acc: 0.8678 - val_loss: 0.4038 - val_acc: 0.8662\n",
      "Epoch 141/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.4037 - acc: 0.8670 - val_loss: 0.4001 - val_acc: 0.8636\n",
      "Epoch 142/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.4021 - acc: 0.8650 - val_loss: 0.3978 - val_acc: 0.8681\n",
      "Epoch 143/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3996 - acc: 0.8655 - val_loss: 0.3999 - val_acc: 0.8655\n",
      "Epoch 144/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3999 - acc: 0.8664 - val_loss: 0.3975 - val_acc: 0.8662\n",
      "Epoch 145/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3973 - acc: 0.8676 - val_loss: 0.3951 - val_acc: 0.8704\n",
      "Epoch 146/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3925 - acc: 0.8704 - val_loss: 0.3912 - val_acc: 0.8678\n",
      "Epoch 147/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3927 - acc: 0.8657 - val_loss: 0.3900 - val_acc: 0.8676\n",
      "Epoch 148/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3913 - acc: 0.8683 - val_loss: 0.3954 - val_acc: 0.8697\n",
      "Epoch 149/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3899 - acc: 0.8660 - val_loss: 0.3842 - val_acc: 0.8694\n",
      "Epoch 150/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3869 - acc: 0.8719 - val_loss: 0.3837 - val_acc: 0.8734\n",
      "Epoch 151/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3865 - acc: 0.8687 - val_loss: 0.3814 - val_acc: 0.8720\n",
      "Epoch 152/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3841 - acc: 0.8708 - val_loss: 0.3805 - val_acc: 0.8697\n",
      "Epoch 153/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3820 - acc: 0.8701 - val_loss: 0.3782 - val_acc: 0.8730\n",
      "Epoch 154/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3812 - acc: 0.8702 - val_loss: 0.3788 - val_acc: 0.8750\n",
      "Epoch 155/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3774 - acc: 0.8718 - val_loss: 0.3778 - val_acc: 0.8685\n",
      "Epoch 156/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3806 - acc: 0.8684 - val_loss: 0.3805 - val_acc: 0.8705\n",
      "Epoch 157/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3764 - acc: 0.8687 - val_loss: 0.3746 - val_acc: 0.8702\n",
      "Epoch 158/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3771 - acc: 0.8708 - val_loss: 0.3714 - val_acc: 0.8718\n",
      "Epoch 159/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3751 - acc: 0.8708 - val_loss: 0.3692 - val_acc: 0.8739\n",
      "Epoch 160/300\n",
      "7143/7143 [==============================] - 0s 36us/step - loss: 0.3707 - acc: 0.8723 - val_loss: 0.3663 - val_acc: 0.8781\n",
      "Epoch 161/300\n",
      "7143/7143 [==============================] - 0s 37us/step - loss: 0.3697 - acc: 0.8725 - val_loss: 0.3733 - val_acc: 0.8711\n",
      "Epoch 162/300\n",
      "7143/7143 [==============================] - 1s 71us/step - loss: 0.3700 - acc: 0.8720 - val_loss: 0.3656 - val_acc: 0.8755\n",
      "Epoch 163/300\n",
      "7143/7143 [==============================] - 0s 49us/step - loss: 0.3679 - acc: 0.8722 - val_loss: 0.3699 - val_acc: 0.8732\n",
      "Epoch 164/300\n",
      "7143/7143 [==============================] - 0s 44us/step - loss: 0.3660 - acc: 0.8722 - val_loss: 0.3641 - val_acc: 0.8762\n",
      "Epoch 165/300\n",
      "7143/7143 [==============================] - 0s 56us/step - loss: 0.3640 - acc: 0.8740 - val_loss: 0.3601 - val_acc: 0.8746\n",
      "Epoch 166/300\n",
      "7143/7143 [==============================] - 0s 49us/step - loss: 0.3669 - acc: 0.8722 - val_loss: 0.3641 - val_acc: 0.8733\n",
      "Epoch 167/300\n",
      "7143/7143 [==============================] - 0s 42us/step - loss: 0.3627 - acc: 0.8737 - val_loss: 0.3560 - val_acc: 0.8727\n",
      "Epoch 168/300\n",
      "7143/7143 [==============================] - 0s 57us/step - loss: 0.3594 - acc: 0.8719 - val_loss: 0.3586 - val_acc: 0.8758\n",
      "Epoch 169/300\n",
      "7143/7143 [==============================] - 1s 91us/step - loss: 0.3621 - acc: 0.8772 - val_loss: 0.3586 - val_acc: 0.8702\n",
      "Epoch 170/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.3611 - acc: 0.8730 - val_loss: 0.3574 - val_acc: 0.8744\n",
      "Epoch 171/300\n",
      "7143/7143 [==============================] - 0s 46us/step - loss: 0.3561 - acc: 0.8753 - val_loss: 0.3576 - val_acc: 0.8760\n",
      "Epoch 172/300\n",
      "7143/7143 [==============================] - 0s 64us/step - loss: 0.3576 - acc: 0.8761 - val_loss: 0.3504 - val_acc: 0.8754\n",
      "Epoch 173/300\n",
      "7143/7143 [==============================] - 0s 69us/step - loss: 0.3538 - acc: 0.8754 - val_loss: 0.3552 - val_acc: 0.8758\n",
      "Epoch 174/300\n",
      "7143/7143 [==============================] - 0s 43us/step - loss: 0.3529 - acc: 0.8750 - val_loss: 0.3492 - val_acc: 0.8781\n",
      "Epoch 175/300\n",
      "7143/7143 [==============================] - 0s 49us/step - loss: 0.3525 - acc: 0.8746 - val_loss: 0.3498 - val_acc: 0.8754\n",
      "Epoch 176/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.3485 - acc: 0.8768 - val_loss: 0.3442 - val_acc: 0.8772\n",
      "Epoch 177/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.3477 - acc: 0.8776 - val_loss: 0.3503 - val_acc: 0.8778\n",
      "Epoch 178/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.3462 - acc: 0.8767 - val_loss: 0.3463 - val_acc: 0.8775\n",
      "Epoch 179/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.3456 - acc: 0.8803 - val_loss: 0.3410 - val_acc: 0.8817\n",
      "Epoch 180/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.3436 - acc: 0.8804 - val_loss: 0.3406 - val_acc: 0.8774\n",
      "Epoch 181/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.3446 - acc: 0.8761 - val_loss: 0.3471 - val_acc: 0.8757\n",
      "Epoch 182/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3439 - acc: 0.8782 - val_loss: 0.3444 - val_acc: 0.8839\n",
      "Epoch 183/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3429 - acc: 0.8786 - val_loss: 0.3388 - val_acc: 0.8823\n",
      "Epoch 184/300\n",
      "7143/7143 [==============================] - 0s 46us/step - loss: 0.3414 - acc: 0.8782 - val_loss: 0.3417 - val_acc: 0.8761\n",
      "Epoch 185/300\n",
      "7143/7143 [==============================] - 0s 37us/step - loss: 0.3414 - acc: 0.8767 - val_loss: 0.3389 - val_acc: 0.8782\n",
      "Epoch 186/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3387 - acc: 0.8792 - val_loss: 0.3421 - val_acc: 0.8589\n",
      "Epoch 187/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3367 - acc: 0.8790 - val_loss: 0.3363 - val_acc: 0.8764\n",
      "Epoch 188/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3355 - acc: 0.8778 - val_loss: 0.3330 - val_acc: 0.8828\n",
      "Epoch 189/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.3358 - acc: 0.8828 - val_loss: 0.3358 - val_acc: 0.8753\n",
      "Epoch 190/300\n",
      "7143/7143 [==============================] - 0s 61us/step - loss: 0.3345 - acc: 0.8806 - val_loss: 0.3318 - val_acc: 0.8837\n",
      "Epoch 191/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3314 - acc: 0.8834 - val_loss: 0.3317 - val_acc: 0.8767\n",
      "Epoch 192/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3318 - acc: 0.8802 - val_loss: 0.3289 - val_acc: 0.8771\n",
      "Epoch 193/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.3283 - acc: 0.8809 - val_loss: 0.3305 - val_acc: 0.8793\n",
      "Epoch 194/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.3313 - acc: 0.8814 - val_loss: 0.3272 - val_acc: 0.8769\n",
      "Epoch 195/300\n",
      "7143/7143 [==============================] - 0s 46us/step - loss: 0.3296 - acc: 0.8831 - val_loss: 0.3234 - val_acc: 0.8814\n",
      "Epoch 196/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.3257 - acc: 0.8792 - val_loss: 0.3259 - val_acc: 0.8827\n",
      "Epoch 197/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.3250 - acc: 0.8832 - val_loss: 0.3217 - val_acc: 0.8821\n",
      "Epoch 198/300\n",
      "7143/7143 [==============================] - 0s 53us/step - loss: 0.3247 - acc: 0.8827 - val_loss: 0.3261 - val_acc: 0.8828\n",
      "Epoch 199/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.3267 - acc: 0.8800 - val_loss: 0.3242 - val_acc: 0.8806\n",
      "Epoch 200/300\n",
      "7143/7143 [==============================] - 0s 43us/step - loss: 0.3233 - acc: 0.8830 - val_loss: 0.3207 - val_acc: 0.8823\n",
      "Epoch 201/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.3208 - acc: 0.8849 - val_loss: 0.3237 - val_acc: 0.8827\n",
      "Epoch 202/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3221 - acc: 0.8825 - val_loss: 0.3192 - val_acc: 0.8817\n",
      "Epoch 203/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.3204 - acc: 0.8842 - val_loss: 0.3157 - val_acc: 0.8900\n",
      "Epoch 204/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.3188 - acc: 0.8838 - val_loss: 0.3151 - val_acc: 0.8832\n",
      "Epoch 205/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3184 - acc: 0.8866 - val_loss: 0.3158 - val_acc: 0.8835\n",
      "Epoch 206/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3153 - acc: 0.8824 - val_loss: 0.3168 - val_acc: 0.8873\n",
      "Epoch 207/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3166 - acc: 0.8839 - val_loss: 0.3185 - val_acc: 0.8888\n",
      "Epoch 208/300\n",
      "7143/7143 [==============================] - 0s 26us/step - loss: 0.3177 - acc: 0.8828 - val_loss: 0.3123 - val_acc: 0.8881\n",
      "Epoch 209/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.3141 - acc: 0.8855 - val_loss: 0.3146 - val_acc: 0.8916\n",
      "Epoch 210/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3138 - acc: 0.8860 - val_loss: 0.3132 - val_acc: 0.8834\n",
      "Epoch 211/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3131 - acc: 0.8873 - val_loss: 0.3115 - val_acc: 0.8865\n",
      "Epoch 212/300\n",
      "7143/7143 [==============================] - 0s 34us/step - loss: 0.3098 - acc: 0.8841 - val_loss: 0.3102 - val_acc: 0.8907\n",
      "Epoch 213/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.3106 - acc: 0.8884 - val_loss: 0.3050 - val_acc: 0.8859\n",
      "Epoch 214/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3082 - acc: 0.8874 - val_loss: 0.3082 - val_acc: 0.8876\n",
      "Epoch 215/300\n",
      "7143/7143 [==============================] - 0s 34us/step - loss: 0.3106 - acc: 0.8870 - val_loss: 0.3081 - val_acc: 0.8832\n",
      "Epoch 216/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.3088 - acc: 0.8872 - val_loss: 0.3036 - val_acc: 0.8886\n",
      "Epoch 217/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.3056 - acc: 0.8893 - val_loss: 0.3057 - val_acc: 0.8907\n",
      "Epoch 218/300\n",
      "7143/7143 [==============================] - 0s 37us/step - loss: 0.3067 - acc: 0.8883 - val_loss: 0.3028 - val_acc: 0.8870\n",
      "Epoch 219/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3024 - acc: 0.8900 - val_loss: 0.3036 - val_acc: 0.8947\n",
      "Epoch 220/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.3076 - acc: 0.8891 - val_loss: 0.3071 - val_acc: 0.8935\n",
      "Epoch 221/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.3050 - acc: 0.8898 - val_loss: 0.2998 - val_acc: 0.8926\n",
      "Epoch 222/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3023 - acc: 0.8883 - val_loss: 0.3020 - val_acc: 0.8827\n",
      "Epoch 223/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.3006 - acc: 0.8908 - val_loss: 0.2986 - val_acc: 0.8904\n",
      "Epoch 224/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.3029 - acc: 0.8887 - val_loss: 0.2992 - val_acc: 0.8901\n",
      "Epoch 225/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.3002 - acc: 0.8893 - val_loss: 0.3078 - val_acc: 0.8806\n",
      "Epoch 226/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.3005 - acc: 0.8901 - val_loss: 0.2974 - val_acc: 0.8900\n",
      "Epoch 227/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2979 - acc: 0.8898 - val_loss: 0.2950 - val_acc: 0.8879\n",
      "Epoch 228/300\n",
      "7143/7143 [==============================] - 0s 21us/step - loss: 0.2976 - acc: 0.8893 - val_loss: 0.3029 - val_acc: 0.8855\n",
      "Epoch 229/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2947 - acc: 0.8930 - val_loss: 0.2953 - val_acc: 0.8946\n",
      "Epoch 230/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2991 - acc: 0.8852 - val_loss: 0.2925 - val_acc: 0.8928\n",
      "Epoch 231/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2987 - acc: 0.8877 - val_loss: 0.2938 - val_acc: 0.8876\n",
      "Epoch 232/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.2947 - acc: 0.8915 - val_loss: 0.2978 - val_acc: 0.8872\n",
      "Epoch 233/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2962 - acc: 0.8916 - val_loss: 0.2884 - val_acc: 0.8929\n",
      "Epoch 234/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2922 - acc: 0.8923 - val_loss: 0.2907 - val_acc: 0.8880\n",
      "Epoch 235/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2914 - acc: 0.8916 - val_loss: 0.2894 - val_acc: 0.8926\n",
      "Epoch 236/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2913 - acc: 0.8916 - val_loss: 0.2893 - val_acc: 0.8897\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2927 - acc: 0.8907 - val_loss: 0.2866 - val_acc: 0.8897\n",
      "Epoch 238/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2904 - acc: 0.8902 - val_loss: 0.2883 - val_acc: 0.8963\n",
      "Epoch 239/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2894 - acc: 0.8915 - val_loss: 0.2939 - val_acc: 0.8873\n",
      "Epoch 240/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2880 - acc: 0.8916 - val_loss: 0.2876 - val_acc: 0.8916\n",
      "Epoch 241/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2860 - acc: 0.8932 - val_loss: 0.2880 - val_acc: 0.8873\n",
      "Epoch 242/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2857 - acc: 0.8946 - val_loss: 0.2824 - val_acc: 0.8954\n",
      "Epoch 243/300\n",
      "7143/7143 [==============================] - 0s 20us/step - loss: 0.2877 - acc: 0.8908 - val_loss: 0.2858 - val_acc: 0.8954\n",
      "Epoch 244/300\n",
      "7143/7143 [==============================] - 0s 23us/step - loss: 0.2844 - acc: 0.8933 - val_loss: 0.2892 - val_acc: 0.8929\n",
      "Epoch 245/300\n",
      "7143/7143 [==============================] - 0s 34us/step - loss: 0.2871 - acc: 0.8944 - val_loss: 0.2928 - val_acc: 0.8894\n",
      "Epoch 246/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.2879 - acc: 0.8909 - val_loss: 0.2999 - val_acc: 0.8989\n",
      "Epoch 247/300\n",
      "7143/7143 [==============================] - 0s 44us/step - loss: 0.2848 - acc: 0.8960 - val_loss: 0.2815 - val_acc: 0.8937\n",
      "Epoch 248/300\n",
      "7143/7143 [==============================] - 0s 44us/step - loss: 0.2828 - acc: 0.8921 - val_loss: 0.2873 - val_acc: 0.9027\n",
      "Epoch 249/300\n",
      "7143/7143 [==============================] - 0s 48us/step - loss: 0.2829 - acc: 0.8951 - val_loss: 0.2822 - val_acc: 0.8895\n",
      "Epoch 250/300\n",
      "7143/7143 [==============================] - 0s 46us/step - loss: 0.2832 - acc: 0.8919 - val_loss: 0.2797 - val_acc: 0.8977\n",
      "Epoch 251/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2813 - acc: 0.8939 - val_loss: 0.2785 - val_acc: 0.8954\n",
      "Epoch 252/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2797 - acc: 0.8951 - val_loss: 0.2760 - val_acc: 0.8954\n",
      "Epoch 253/300\n",
      "7143/7143 [==============================] - 0s 62us/step - loss: 0.2796 - acc: 0.8940 - val_loss: 0.2726 - val_acc: 0.8977\n",
      "Epoch 254/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.2782 - acc: 0.8960 - val_loss: 0.2802 - val_acc: 0.8915\n",
      "Epoch 255/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.2763 - acc: 0.8982 - val_loss: 0.2767 - val_acc: 0.8928\n",
      "Epoch 256/300\n",
      "7143/7143 [==============================] - 0s 40us/step - loss: 0.2755 - acc: 0.8981 - val_loss: 0.2727 - val_acc: 0.8968\n",
      "Epoch 257/300\n",
      "7143/7143 [==============================] - 0s 41us/step - loss: 0.2765 - acc: 0.8958 - val_loss: 0.2754 - val_acc: 0.8947\n",
      "Epoch 258/300\n",
      "7143/7143 [==============================] - 0s 46us/step - loss: 0.2775 - acc: 0.8950 - val_loss: 0.2847 - val_acc: 0.9065\n",
      "Epoch 259/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2746 - acc: 0.8986 - val_loss: 0.2806 - val_acc: 0.8912\n",
      "Epoch 260/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2747 - acc: 0.8958 - val_loss: 0.2824 - val_acc: 0.8985\n",
      "Epoch 261/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2735 - acc: 0.9003 - val_loss: 0.2705 - val_acc: 0.8915\n",
      "Epoch 262/300\n",
      "7143/7143 [==============================] - 0s 41us/step - loss: 0.2737 - acc: 0.8964 - val_loss: 0.2754 - val_acc: 0.9066\n",
      "Epoch 263/300\n",
      "7143/7143 [==============================] - 0s 31us/step - loss: 0.2747 - acc: 0.8958 - val_loss: 0.2731 - val_acc: 0.8902\n",
      "Epoch 264/300\n",
      "7143/7143 [==============================] - ETA: 0s - loss: 0.2756 - acc: 0.893 - 0s 28us/step - loss: 0.2764 - acc: 0.8939 - val_loss: 0.2664 - val_acc: 0.8996\n",
      "Epoch 265/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2695 - acc: 0.9003 - val_loss: 0.2685 - val_acc: 0.8940\n",
      "Epoch 266/300\n",
      "7143/7143 [==============================] - 0s 36us/step - loss: 0.2717 - acc: 0.8961 - val_loss: 0.2671 - val_acc: 0.8960\n",
      "Epoch 267/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2699 - acc: 0.8974 - val_loss: 0.2659 - val_acc: 0.8932\n",
      "Epoch 268/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2683 - acc: 0.8991 - val_loss: 0.2647 - val_acc: 0.9013\n",
      "Epoch 269/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2698 - acc: 0.8995 - val_loss: 0.2700 - val_acc: 0.9058\n",
      "Epoch 270/300\n",
      "7143/7143 [==============================] - 0s 22us/step - loss: 0.2683 - acc: 0.8975 - val_loss: 0.2684 - val_acc: 0.9087\n",
      "Epoch 271/300\n",
      "7143/7143 [==============================] - 0s 53us/step - loss: 0.2686 - acc: 0.8992 - val_loss: 0.2650 - val_acc: 0.8954\n",
      "Epoch 272/300\n",
      "7143/7143 [==============================] - 0s 38us/step - loss: 0.2639 - acc: 0.9016 - val_loss: 0.2661 - val_acc: 0.8961\n",
      "Epoch 273/300\n",
      "7143/7143 [==============================] - 0s 57us/step - loss: 0.2647 - acc: 0.9000 - val_loss: 0.2605 - val_acc: 0.9026\n",
      "Epoch 274/300\n",
      "7143/7143 [==============================] - 0s 41us/step - loss: 0.2692 - acc: 0.8989 - val_loss: 0.2655 - val_acc: 0.8928\n",
      "Epoch 275/300\n",
      "7143/7143 [==============================] - 0s 41us/step - loss: 0.2657 - acc: 0.8970 - val_loss: 0.2633 - val_acc: 0.9045\n",
      "Epoch 276/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.2666 - acc: 0.8982 - val_loss: 0.2673 - val_acc: 0.8963\n",
      "Epoch 277/300\n",
      "7143/7143 [==============================] - 0s 39us/step - loss: 0.2705 - acc: 0.8978 - val_loss: 0.2601 - val_acc: 0.9066\n",
      "Epoch 278/300\n",
      "7143/7143 [==============================] - 0s 24us/step - loss: 0.2651 - acc: 0.8986 - val_loss: 0.2599 - val_acc: 0.9061\n",
      "Epoch 279/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.2634 - acc: 0.9023 - val_loss: 0.2618 - val_acc: 0.9086\n",
      "Epoch 280/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2631 - acc: 0.8993 - val_loss: 0.2577 - val_acc: 0.9037\n",
      "Epoch 281/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2617 - acc: 0.9012 - val_loss: 0.2634 - val_acc: 0.9003\n",
      "Epoch 282/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2640 - acc: 0.8998 - val_loss: 0.2563 - val_acc: 0.9003\n",
      "Epoch 283/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.2595 - acc: 0.9027 - val_loss: 0.2554 - val_acc: 0.9114\n",
      "Epoch 284/300\n",
      "7143/7143 [==============================] - 0s 36us/step - loss: 0.2626 - acc: 0.9006 - val_loss: 0.2644 - val_acc: 0.8964\n",
      "Epoch 285/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2636 - acc: 0.8999 - val_loss: 0.2609 - val_acc: 0.9047\n",
      "Epoch 286/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2590 - acc: 0.9024 - val_loss: 0.2687 - val_acc: 0.8845\n",
      "Epoch 287/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.2596 - acc: 0.9003 - val_loss: 0.2589 - val_acc: 0.9070\n",
      "Epoch 288/300\n",
      "7143/7143 [==============================] - 0s 35us/step - loss: 0.2567 - acc: 0.9054 - val_loss: 0.2542 - val_acc: 0.9098\n",
      "Epoch 289/300\n",
      "7143/7143 [==============================] - 0s 30us/step - loss: 0.2603 - acc: 0.9017 - val_loss: 0.2592 - val_acc: 0.9131\n",
      "Epoch 290/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2559 - acc: 0.9051 - val_loss: 0.2539 - val_acc: 0.9076\n",
      "Epoch 291/300\n",
      "7143/7143 [==============================] - 0s 43us/step - loss: 0.2573 - acc: 0.9016 - val_loss: 0.2581 - val_acc: 0.9142\n",
      "Epoch 292/300\n",
      "7143/7143 [==============================] - 0s 27us/step - loss: 0.2607 - acc: 0.8979 - val_loss: 0.2603 - val_acc: 0.9065\n",
      "Epoch 293/300\n",
      "7143/7143 [==============================] - 0s 32us/step - loss: 0.2545 - acc: 0.9041 - val_loss: 0.2572 - val_acc: 0.9021\n",
      "Epoch 294/300\n",
      "7143/7143 [==============================] - 0s 35us/step - loss: 0.2535 - acc: 0.9038 - val_loss: 0.2536 - val_acc: 0.9111\n",
      "Epoch 295/300\n",
      "7143/7143 [==============================] - 0s 33us/step - loss: 0.2548 - acc: 0.9017 - val_loss: 0.2737 - val_acc: 0.9051\n",
      "Epoch 296/300\n",
      "7143/7143 [==============================] - 0s 29us/step - loss: 0.2548 - acc: 0.9055 - val_loss: 0.2498 - val_acc: 0.9051\n",
      "Epoch 297/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2536 - acc: 0.9024 - val_loss: 0.2528 - val_acc: 0.9003\n",
      "Epoch 298/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2544 - acc: 0.9030 - val_loss: 0.2534 - val_acc: 0.9021\n",
      "Epoch 299/300\n",
      "7143/7143 [==============================] - 0s 28us/step - loss: 0.2516 - acc: 0.9023 - val_loss: 0.2456 - val_acc: 0.9052\n",
      "Epoch 300/300\n",
      "7143/7143 [==============================] - 0s 25us/step - loss: 0.2512 - acc: 0.9062 - val_loss: 0.2516 - val_acc: 0.9048\n",
      "28574/28574 [==============================] - 4s 124us/step\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ave_acc_parking_ann=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(parking_x, parking_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        model = create_model(X_train1.shape[1],y_train1.shape[1])\n",
    "        history = model.fit(X_train1, y_train1,validation_data=(X_train1,y_train1),batch_size=512,epochs=300)\n",
    "        score += model.evaluate(X_test1,y_test1,verbose=1)[1]\n",
    "    ave_acc_parking_ann.append(score/3.0)\n",
    "    \n",
    "print ave_acc_parking_ann  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmcXGWZ9//PVdXV+5ruztadPQETliQkxERwRRRQFgcFRdxnIo7O6IzDA864zzzP6IyPjogDgvATBh8UwQUFNCBBhLB1YkJWIHt3J+k1ve9V1++POmmaJp10kq6u7q7v+/WqV06dc9ep6+Qk/e2z3Pcxd0dERAQglOwCRERk7FAoiIhIP4WCiIj0UyiIiEg/hYKIiPRTKIiISD+FgsgwmdlPzOzfhtl2r5m981TXIzLaFAoiItJPoSAiIv0UCjKhBKdtrjezF82s3czuMLMpZvaImbWa2WNmVjSg/WVmttXMmszsCTNbOGDZUjPbEHzu50DmoO96r5ltDD67zszOPsma/8bMdppZo5k9aGbTg/lmZt8zs1ozazGzzWZ2ZrDsEjPbFtRWbWb/dFJ/YSKDKBRkIroSuBA4DbgUeAT4Z6CU+L/5vwcws9OAe4EvBMseBn5rZulmlg78GvgfYBLwi2C9BJ9dCtwJfBooBn4EPGhmGSdSqJm9A/h34CpgGrAP+Fmw+F3AW4LtKAjaNATL7gA+7e55wJnA4yfyvSJDUSjIRPQDd69x92rgz8Bz7v4Xd+8CfgUsDdpdDTzk7o+6ey/wHSALeBOwEogA/+Xuve5+P/DCgO9YDfzI3Z9z96i73wV0B587ER8G7nT3De7eDXwJWGVms4FeIA94A2Duvt3dDwaf6wUWmVm+ux929w0n+L0iR6VQkImoZsB051He5wbT04n/Zg6Au8eASqAsWFbtrx0xct+A6VnAF4NTR01m1gTMCD53IgbX0Eb8aKDM3R8HbgZ+CNSa2W1mlh80vRK4BNhnZn8ys1Un+L0iR6VQkFR2gPgPdyB+Dp/4D/Zq4CBQFsw7YuaA6Urgf7t74YBXtrvfe4o15BA/HVUN4O43ufsyYBHx00jXB/NfcPfLgcnET3Pdd4LfK3JUCgVJZfcB7zGzC8wsAnyR+CmgdcAzQB/w92YWMbO/AlYM+OztwHVm9sbggnCOmb3HzPJOsIZ7gU+Y2ZLgesT/IX66a6+ZnRusPwK0A11ALLjm8WEzKwhOe7UAsVP4exDpp1CQlOXuLwHXAj8A6olflL7U3XvcvQf4K+DjQCPx6w+/HPDZCuBviJ/eOQzsDNqeaA2PAV8BHiB+dDIP+GCwOJ94+BwmfoqpAfjPYNlHgL1m1gJcR/zahMgpMz1kR0REjtCRgoiI9FMoiIhIv4SFgpllmtnzZrYp6DH6jaO0+biZ1QW9Qjea2V8nqh4RETm+tASuuxt4h7u3BXdPPGVmj7j7s4Pa/dzdP5fAOkREZJgSFgpBp5+24G0keJ3yVe2SkhKfPXv2qa5GRCSlrF+/vt7dS4/XLpFHCphZGFgPzAd+6O7PHaXZlWb2FuBl4B/cvfIo61lNfFgBZs6cSUVFRQKrFhGZeMxs3/FbJfhCczAmzBKgHFhxZITHAX4LzHb3s4FHgbuGWM9t7r7c3ZeXlh436ERE5CSNyt1H7t4ErAUuGjS/IRgEDODHwLLRqEdERI4ukXcflZpZYTCdRXwo4x2D2kwb8PYyYHui6hERkeNL5DWFacBdwXWFEHCfu//OzL4JVLj7g8THlbmM+BgzjZzEMAEAvb29VFVV0dXVNUKlj12ZmZmUl5cTiUSSXYqITEDjbpiL5cuX++ALzXv27CEvL4/i4mJeO6jlxOLuNDQ00Nraypw5c5JdjoiMI2a23t2XH6/dhOjR3NXVNeEDAcDMKC4uTokjIhFJjgkRCsCED4QjUmU7RSQ5JkwoHE9nb5RDzZ30RTXsvIjIUFImFHr6YtS2dtOTgFBoamriv//7v0/4c5dccglNTU0jXo+IyMlKmVCIhOOnXXqjI39hfahQ6OvrO+bnHn74YQoLC0e8HhGRk5XQYS7Gkkg4nn+9CThSuPHGG9m1axdLliwhEomQmZlJUVERO3bs4OWXX+aKK66gsrKSrq4uPv/5z7N69WoAZs+eTUVFBW1tbVx88cWcf/75rFu3jrKyMn7zm9+QlZU14rWKiBzLhAuFb/x2K9sOtBx1WUdPL2mhMOlpJ3aAtGh6Pl+79Iwhl3/rW99iy5YtbNy4kSeeeIL3vOc9bNmypf+20TvvvJNJkybR2dnJueeey5VXXklxcfFr1vHKK69w7733cvvtt3PVVVfxwAMPcO21155QnSIipyplTh8R6yObbvDEX2hesWLFa/oR3HTTTSxevJiVK1dSWVnJK6+88rrPzJkzhyVLlgCwbNky9u7dm/A6RUQGm3BHCkP+Rt/dCg07qQ6XUTZlckJryMnJ6Z9+4okneOyxx3jmmWfIzs7mbW9721H7GWRkZPRPh8NhOjs7E1qjiMjRpM6RQigYFiJ27Iu/JyMvL4/W1tajLmtubqaoqIjs7Gx27NjBs88OfsaQiMjYMeGOFIYUjm9qKNaHu49oJ7Di4mLOO+88zjzzTLKyspgyZUr/sosuuohbb72VhQsXcvrpp7Ny5coR+14RkZE2IcY+2r59OwsXLjz2B93xg5uo83yKps7uvxtpPBrW9oqIDJBSYx8NixkeSiNCNCG3pYqITASpEwqAhyKkEaUvAR3YREQmgpQKBQunxUMhpiMFEZGjSbFQiBChT0cKIiJDSLlQSLOYRkoVERlCSoXCkb4KsWhvkgsRERmbUjIUGOFQONmhswH+67/+i46OjhGtR0TkZKVWKAQd2Ea6V7NCQUQmitTp0Qz9Rwo2wqEwcOjsCy+8kMmTJ3PffffR3d3N+973Pr7xjW/Q3t7OVVddRVVVFdFolK985SvU1NRw4MAB3v72t1NSUsLatWtHtC4RkRM18ULhkRvh0OYhFjr0tFFKBE/PwBjmUBdTz4KLvzXk4oFDZ69Zs4b777+f559/Hnfnsssu48knn6Suro7p06fz0EMPAfExkQoKCvjud7/L2rVrKSkpOcENFREZeal1+gjDMQwnUaN7rFmzhjVr1rB06VLOOeccduzYwSuvvMJZZ53Fo48+yg033MCf//xnCgoKElOAiMgpSNiRgpllAk8CGcH33O/uXxvUJgO4G1gGNABXu/veU/riY/xGDxA7tI2OaJi0knnkZIz85rs7X/rSl/j0pz/9umUbNmzg4Ycf5stf/jIXXHABX/3qV0f8+0VETkUijxS6gXe4+2JgCXCRmQ0eIvRTwGF3nw98D/h2AuuJCwdDXcRG7lBh4NDZ7373u7nzzjtpa2sDoLq6mtraWg4cOEB2djbXXnst119/PRs2bHjdZ0VEki1hRwoeH361LXgbCV6DfxJfDnw9mL4fuNnMzBM4dKuFI6TRRdcIdmAbOHT2xRdfzDXXXMOqVasAyM3N5Z577mHnzp1cf/31hEIhIpEIt9xyCwCrV6/moosuYvr06brQLCJJl9Chs80sDKwH5gM/dPcbBi3fAlzk7lXB+13AG929flC71cBqgJkzZy7bt2/fa77nRIaS9uZqvK2Ourw3MCU/8+Q2LMk0dLaInKgxMXS2u0fdfQlQDqwwszNPcj23uftyd19eWlp6SjVZOI2QOdG+kX8Cm4jIeDcqdx+5exOwFrho0KJqYAaAmaUBBcQvOCdO0FfBNdSFiMjrJCwUzKzUzAqD6SzgQmDHoGYPAh8Lpt8PPH6y1xOG/bHwkWc1j89QGG9PyhOR8SWRRwrTgLVm9iLwAvCou//OzL5pZpcFbe4Ais1sJ/CPwI0n80WZmZk0NDQM7wdmKH5tfaR7NY8Gd6ehoYHMzPF5LURExr5E3n30IrD0KPO/OmC6C/jAqX5XeXk5VVVV1NXVHb9xLAYttTTTQXNT86l+9ajLzMykvLw82WWIyAQ1IYa5iEQizJkzZ3iN3Yl+863c3nsR137lJ+QmoAObiMh4lWLDXABmdGeWUGpN1LV2J7saEZExJfVCAejLnkwpTdS2dCW7FBGRMSUlQyGUN4VSa+KQQkFE5DVSMhQyiqZTas0cbFYoiIgMlJKhEMmfyiRrpbap7fiNRURSSEqGArmTCeG0Nx5IdiUiImNKaoZC3lQAepsOJbkQEZGxJTVDIb8MgHCbjhRERAZKzVAonAlAfvcBevpG7rkKIiLjXWqGQlYRveFsyqmjRrelioj0S81QMKM7t5xyq9dtqSIiA6RmKABWNJMyq6eysSPZpYiIjBkpGwqZJXMotzr2KxRERPqlbCiEi2aSbx3U1dUkuxQRkTEjZUPhyB1IvQ17k1uHiMgYkvKhQHNlcusQERlDUjgUZgGQ23mArt5okosRERkbUjcUsifRF86i3OqpOqyLzSIikMqhYEZvXrnuQBIRGSB1QwEIF82Kh0KDQkFEBFI8FCLFsymzevY3dia7FBGRMSGlQ8GKZlJo7dTV1ya7FBGRMSFhoWBmM8xsrZltM7OtZvb5o7R5m5k1m9nG4PXVRNVzVAUzAOht2DeqXysiMlalJXDdfcAX3X2DmeUB683sUXffNqjdn939vQmsY2jBbamh5krcHTNLShkiImNFwo4U3P2gu28IpluB7UBZor7vpAQd2EpjtTS09yS5GBGR5BuVawpmNhtYCjx3lMWrzGyTmT1iZmcM8fnVZlZhZhV1dXUjV1hOCdFwJuVWxz7dgSQikvhQMLNc4AHgC+7eMmjxBmCWuy8GfgD8+mjrcPfb3H25uy8vLS0dyeLoy59BudVR3aQ7kEREEhoKZhYhHgg/dfdfDl7u7i3u3hZMPwxEzKwkkTUNFi6aSbnVcUChICKS0LuPDLgD2O7u3x2izdSgHWa2IqinIVE1HU3apFnMCNUrFERESOzdR+cBHwE2m9nGYN4/AzMB3P1W4P3AZ8ysD+gEPujunsCaXq9wJoW00dA4qlkkIjImJSwU3P0p4Jj3eLr7zcDNiaphWII7kKKH9ye1DBGRsSClezQDUBAPhUhLVZILERFJPoVCcKRQ1HuQtu6+JBcjIpJcCoXcyURDGZRZPQd1sVlEUpxCwYye3DL1VRARQaEAgBXOpNzqOdDUlexSRESSSqEApJfMUgc2EREUCgCEimZRbK3UNzYmuxQRkaRSKED/ENo9eq6CiKQ4hQL035Yabq1MciEiIsmlUID+J7Bld1QTjY3uKBsiImOJQgEgdwpRizDN66hv6052NSIiSaNQAAiF6M4to9zq1VdBRFKaQiHgBXqugoiIQiEQKZ5JmUJBRFKcQiGQXjybUmuhrrEp2aWIiCSNQuGIoK9CV736KohI6lIoHBH0VbAmPWxHRFKXQuGIoK9CRrsetiMiqUuhcETeVKKWxqTeQ3T06GE7IpKaFApHhMJ0Zk/TbakiktIUCgNE82dSZvVUHVYoiEhqUigMECmeRblCQURSWMJCwcxmmNlaM9tmZlvN7PNHaWNmdpOZ7TSzF83snETVMxyZJbOZbE1U1zUkswwRkaRJ5JFCH/BFd18ErAQ+a2aLBrW5GFgQvFYDtySwnuMKFcX7KrTVqK+CiKSmhIWCux909w3BdCuwHSgb1Oxy4G6PexYoNLNpiarpuIK+CtFGhYKIpKZRuaZgZrOBpcBzgxaVAQOfbFPF64Nj9BTNBiCjbR/ueq6CiKSehIeCmeUCDwBfcPeWk1zHajOrMLOKurq6kS1woNyp9IUyKY8dpLZVz1UQkdST0FAwswjxQPipu//yKE2qgRkD3pcH817D3W9z9+Xuvry0tDQxxQKEQnTlz2K2HWJfQ0fivkdEZIxK5N1HBtwBbHf37w7R7EHgo8FdSCuBZnc/mKiahsOK5zHbDrGnvi2ZZYiIJEUijxTOAz4CvMPMNgavS8zsOjO7LmjzMLAb2AncDvxtAusZlqwppzHDatlVoyG0RST1pCVqxe7+FGDHaePAZxNVw8kIlcwj3aIcPrAHODvZ5YiIjCr1aB5s0jwAovU7k1yIiMjoG1YomNnnzSw/OPd/h5ltMLN3Jbq4pChZAEBh+x6NlioiKWe4RwqfDG4nfRdQRPxawbcSVlUy5U6mO2MSp1klu+vak12NiMioGm4oHLk2cAnwP+6+leNcLxjPoiVv4PRQFS8dak12KSIio2q4obDezNYQD4U/mFkeEEtcWcmVOf1MTrMqtlQfTnYpIiKjarh3H30KWALsdvcOM5sEfCJxZSVXaOoZ5FgXtftfAc5KdjkiIqNmuEcKq4CX3L3JzK4Fvgw0J66sJJscH8zVa7YRjWkMJBFJHcMNhVuADjNbDHwR2AXcnbCqkm3yIpwQp/kudtepZ7OIpI7hhkJf0NHscuBmd/8hkJe4spIsI5eeogWcbbt5sWriHhCJiAw23FBoNbMvEb8V9SEzCwGRxJWVfJGZy1gc2s3mKg13ISKpY7ihcDXQTby/wiHio5n+Z8KqGgNCZedQbC0cqnwl2aWIiIyaYYVCEAQ/BQrM7L1Al7tP3GsKAGXLAMio2UhfdMLefSsi8hrDHebiKuB54APAVcBzZvb+RBaWdFPPoi+cxRLfzi71bBaRFDHcfgr/Apzr7rUAZlYKPAbcn6jCki4coWf6uazct41NlU2cPnXiXlcXETliuNcUQkcCIdBwAp8dtzLnv5WFoUo2v7Ir2aWIiIyK4f5g/72Z/cHMPm5mHwceIv6AnAktNOfNAPTufpr4HbkiIhPbsE4fufv1ZnYl8aepAdzm7r9KXFljxPSl9IUzOb1rE3sbOphTkpPsikREEmrYT15z9weABxJYy9iTlk7v9BWs3LeNdbvqFQoiMuEd8/SRmbWaWctRXq1m1jJaRSZT5oL4dYVNL+9OdikiIgl3zCMFd0/5W25sdvy6QmzPU7i/DbMJ+xgJEZGJfwfRKZu+lL5wFmf0bOblGg2OJyITm0LheNLS6StbwcrQNp7aWZ/sakREEkqhMAyZ89/CwlAlFds0DpKITGwJCwUzu9PMas1syxDL32ZmzWa2MXh9NVG1nLLgukJ4/zrau/uSXIyISOIk8kjhJ8BFx2nzZ3dfEry+mcBaTs30pfRF8ngr63UKSUQmtISFgrs/CTQmav2jKi0de8N7eHfaeh7bXJnsakREEibZ1xRWmdkmM3vEzM4YqpGZrTazCjOrqKurG836+oXPvIJ82mnZ/jhdvdGk1CAikmjJDIUNwCx3Xwz8APj1UA3d/TZ3X+7uy0tLS0etwNeY+3b60nJ4e3Qdf3o5OcEkIpJoSQsFd29x97Zg+mEgYmYlyarnuCKZhN5wMRelVfDQxv3JrkZEJCGSFgpmNtWC7sFmtiKopSFZ9QxH6Iz3UUgb7TueoKNHdyGJyMSTyFtS7wWeAU43syoz+5SZXWdm1wVN3g9sMbNNwE3AB32sj089/wL60vP5II/wx+21x28vIjLODHuU1BPl7h86zvKbgZsT9f0JEckidN7fc+Haf+Ofn1zDpYs/nuyKRERGVLLvPhp3QiuvoytSyLtr72BzVXOyyxERGVEKhROVkQfnf563hl/kyT/+NtnViIiMKIXCSchcdR3t4Xxm7b5XfRZEZEJRKJyM9GxaZr+bt7KeJ7aqh7OITBwKhZM0ecVV5Fknf3nil4z1m6ZERIZLoXCSwvPfTlekiGUNv2PdrjHdvUJEZNgUCicrHCFtxSd5Z3gDG357C96u0VNFZPxTKJyCtFXX4aEIf9f8HarvvyHZ5YiInDKFwqnInUzfJ/9IRWgxod2Pc9tD66CnPdlViYicNIXCKcooP5sz3/VRplsj1z5/BZU/+4dklyQictIUCiMg8/R3ApBt3UR2reGeZ/YmtR4RkZOlUBgJRbNh4WXEZp7HVDvMk08/meyKREROikJhpFz9P4SuvA2AmYefpbKxI8kFiYicOIXCSCoop7v0TD4Ufpy12w4kuxoRkROmUBhhGW+/gXmhgxT8+evs27uL2tauZJckIjJsCoWRtvBSDpa+mcu7f0vLnX/F6rsqkl2RiMiwKRRGmhkl1z3IdzL/jrNCe/mP2tUc+PYKfv/Lu5JdmYjIcSkUEiASDvHZL3yFzkmLyLZuetsPc9amb7K1+nCySxMROSaFQoJkZWaQ+Zm1XJv9I+7J+jBlVs+P7rmXvfXt1LV2c6hZ1xpEZOyx8Tbs8/Lly72iYvycp9/f0EGWdzLplkX8Kno+laEyir2J78Wu5ocffSNvmleS7BJFJAWY2Xp3X368dmmjUUwqm1mcDWTD4qu4cuO9xGJRwsQoibTzb/f18tDfnosVlCW7TBERQKePRs/b/wULp2MZufTMv4R38hzXd3yXrrs/8Np2R47cnvwO/PuMV9+LiIwCHSmMlrypcM3PCYXCpDfugZ0P89bQi9BgtLW1kJudS/QXnyDWXk/kkw/B4/8a/1zdDpi8MLm1i0jKSFgomNmdwHuBWnc/8yjLDfg+cAnQAXzc3Tckqp4xYc6b43/mlAIQMgec//f9L3HtpO1k16wnDDS3dVCQXwYt1bD7TwoFERk1iTx99BPgomMsvxhYELxWA7cksJaxpXg+ZBX1v13dew+th3axl2kA/HTN04DFF+5+Iv5nzTb43T/C5vtHt1YRSSkJCwV3fxJoPEaTy4G7Pe5ZoNDMpiWqnjHFDOa9A2adD5mFANya9mHSrrgZgE0bK/DWg/G2+56GWAw2/hQq7oAHPgXdrcmqXEQmuGReaC4DKge8rwrmvY6ZrTazCjOrqKurG5XiEu6KW+Da+2HqWXgkhxv+8QbK5y4C4KzYdsyj+PRzoLuFjtpd0Fz16mdbDiapaBGZ6MbF3Ufufpu7L3f35aWlpckuZ2SkZUAkCy78BvaB/4/MnALInYqnZbIqtB2A71fOA+Brt/2MWFMlhIJLQK0agVVEEiOZoVANzBjwvjyYl1rKlsFp745Ph0JY0WyWhV4GoG3GW4kRZkbPTnoa9nEw/ywAWusqh1qbiMgpSWYoPAh81OJWAs3urvMik+b2T/6va96LlZ7GysgeMrvr+V1jOQAvbN76uo9tqmzi0h88RWtX76iVKiITT8JCwczuBZ4BTjezKjP7lJldZ2bXBU0eBnYDO4Hbgb9NVC3jysJL+yfTcwqxqWexwjcBsNdm0G45VO/f3f/D/8gwJTc88CKbq5vZXNU8+jWLyISRsH4K7v6h4yx34LOJ+v5xa8k1MPXs+B1GZjD7PNh8HwA3fvBC7PePUtLYyDMvVZO1/jaKKtdQ+MkH2F3fDkBLV18yqxeRcU49mseiqQP6+p3xV/DbzwOQN2U2sUllXNz8BD2/WkY68QB45p7PcFr0YrYwlzo96U1ETsG4uPsopWXmQ+Gs+HR+GaH8eFeOylgp3536bTbOWc2q7qf5XcaXmWWHqG3tTmKxIjLeKRTGg0//CT7xSPw21s74g3ruDl3BB676KIs/8m02rfo+AKsy91F3rFCIReGFH0NPx2hULSLjkEJhPMgqgllvik+f/w+w8FK++s9fZ8akbCwUYvE7PwzhdM7NqIwfKcSiR1/PX+6Bh76IP3fr6NUuIuOKQmG8mbkSrr6HcHrmq/PCEZi8kDewh0mNf4F/LYFbzqeuJt4Lurmjl/91/yYObX0SgN117cmoXETGAYXCRDFtMbN7d3FaewV4DGo2c+dN/0rLrRfxh6ef476KKpr3xAehPdykZ0WLyNEpFCaKqWeTE23hzb3PEC2aSxtZ/F3ar8g/9Ax563/I6baf+bE9APS11CS5WBEZqxQKE8W8dwCwMLSf3zeVszE2n2yLX3S+sPP3/CHjRrpIp9fDhDvrk1mpiIxhCoWJongebZPi/Rs2Recwc0k8JH4dPY+9GaexY+HnOL/7+1TYGWR2NySzUhEZw9R5bQLJXXY1PLqFGz/5IUJp6bD1Ft79qf9D1owluDv/e9EhJv35N2QdqqC5o5eC7EiySxaRMUahMJGc+9eQPYnQrJXxITK+VEVWWgYAZsYlZ01j3+aplNS08HJDO0s8CtmT4m1FRNDpo4klPRuWXvvqD/kgEAbKK55GtnWzf/vz8H9Ph62/HOUiRWQsUyikmKLJ8YfbZW78CcR66d76O9a+VEtnzxAd3kQkpSgUUozlTAbggvaHAWjd9hg/uusu/v23f0lmWSIyRigUUk1u/HGmYXNq0mdSYi38LP3fWLDx33l2t+5KEkl1CoVUM+UsePM/0ffGzzDlk/dC1iRieWVcE36cb9/x//j9pv2w7gfQsCvZlYpIEtiRJ3eNF8uXL/eKiopklzFxxKLQ1Uzsv99ET1sjr3g5Z9ku9mecRs8nHmX+1MJkVygiI8DM1rv78uO105FCqguF47exfvoJfMG7KAp1sC7jzczsfpndt3yAP37vEzz0izs42KThtkVSgY4U5PXcaf/91wi/cDvhWA8R+tjj09hxxj/ypks/QUGWOr2JjDfDPVJQKMjQYlHwGA1P3Un7Uz9iSs9+Hmc5NutNnJlRQ8HspeSd99fJrlJEhkGhICOro5Gu2y6kr7WW3GhL/+yHJq8mesYHeKY2jbLiPD779vmYekiLjDkKBUmMWIyWdT/mIKX0PXsbZ7StA6DLI/wmeh7PzvwbrizZz9KF84kseCfpabpsJTIWKBRkVMT2P0/9rg3kHd5C5ov3YMT/PcXcuDt0OVPnnklO+ZmsWPlmMmLdkFOc5IpFUtOYCAUzuwj4PhAGfuzu3xq0/OPAfwLVwayb3f3Hx1qnQmEM2/8cfnATL7SVULz9bubVrwUg6kaT5VNMM5tz3sTjC7/Bx96xhMLs9CQXLJI6kh4KZhYGXgYuBKqAF4APufu2AW0+Dix3988Nd70KhXGkq5m+1joaf/d12g7tZF1sEVf1/oZeD1Ppk9kdWcCzk6+ibNYCLj1nFunZ+ZTkvn4QPxE5dcMNhUQOnb0C2Onuu4OCfgZcDmw75qdk4sgsIC2zgMmfuIfJwFyAyhdoe/Ye0g/s5IKmdVxy8HE4CLFnjBd9LmtyV1KYk8XeeR9m7pRC3nrGLLLSw0neEJHUkchQKAMqB7yvAt54lHZXmtlbiB9V/IO7Vw5uYGargdUAM2fOTECpMmpmnMukGef2JMxgAAAOiElEQVQyCaC1Bnb9kUM1hzhQU8vsg79nScdPoQOitXcAUPHrhRzIXsj89AZipWfAG/+GBbNmkJ2uR4GIJEIiTx+9H7jI3f86eP8R4I0DTxWZWTHQ5u7dZvZp4Gp3f8ex1qvTRxOYO3S3Qt0O+rb+hkOtvaTvepSSrr1UU0qZ19JNhCov5fnMVYSKZtMz90Jmz5nHomn5lObp1JPIUMbC6aNqYMaA9+W8ekEZAHcfOCznj4H/SGA9MtaZQWY+zFhB2owVlB+ZH+2lPJRGzc4NdD//EzJqt/Hh5vuhBqj5DnXr8qnyyewM53AmuziQdzY1sy8jXLaUgrLTmVaYTbGuVYgMSyJD4QVggZnNIR4GHwSuGdjAzKa5+8Hg7WXA9gTWI+NVOIIBUxcsgwXL4vN62qFpP51bHyZW/RJTGvYQ6qhjc3gFc5s3cvqL6+BFaPRcno8tpD5zFvVpUwgVzmT2nHnMoworW0p7zkxOm5yn51WLBBIWCu7eZ2afA/5A/JbUO919q5l9E6hw9weBvzezy4A+oBH4eKLqkQkmPQcmLyRr8kKyBsyeCsT6+mjc9SztVVsJVT7Lmw49T07XesK9MegEgl9D+jzE87E38KQVEs0opKdkIRml84jkFFJUOp2pk/KZMmUa2VnZSdhAkeRQ5zVJDdE+aD2IN+2n8eAe6sKTydzzGJMOPUVvRzNZvYfJjrW/7mPdnsZOm8ne9NOIpGeSmZFOV8E8cssXUj5/CTm99eTm5JIx9fQkbJTI8CW9n0KiKBQkIdzpq99FtOUgHc2NNNUfoKWtnejh/RQc3sK0jh3gMUIeJZOe13y0z0M8aqvoyiihO3saFJaTX1hMWXaM9Ggn7ZZN0VnvwiJZTC/MIjOiW2xl9I2FC80i44cZaaXzSSudTwZQNHj5kV+e3Ok7vJ89L22k48B2OsikoG4DqxqfJatnPRndXXD49avvXhdhp0/nD8xgcpYzy6vYn3cOhbTROGUVzYWL6Gmpo2DRBSyYVkR9Ww8FWRFmFWdrgEEZVTpSEBkp7tB5GG+uorXlMAc6wnRaFnmd1fiux8lr2UlW08tYbyfVoanMj+6m0XOZYk39q2jxLHLpIobRSxrtls2ztpiZ6a3szz2bvvQiMmcsZmbvbmKli2DGuRRkZ1LUtJWsghLCxXPjd3GJDKLTRyJjXSxG1KFl/yZiNdvIzsykZfMj1HoBORlpdHV10duwlze0PUetlTAj+rp+nbR6Ft1EKLH4cOZVXkrEYrSH8phEM4cy5lBZtJJc66Yrq5TM4nLC+WWU5GfRkT6JvuwpzCvNIS9Td19NdAoFkYmmswnvaqbppaepyzudWO0OsqueJtrdzv78ZfR1t1NWv442ssjobeKw5zK3exvlsYPE3AjZ6/+vV3sxMUK0Wy5Z1kPI+9hjM8jPCBFKi7ArezG9hfMozE6nJCNKTl8TTZ5Ne8nZzCkIkZY3mfziaRTkZCbhL0ROhEJBROI6GiE9h+7DVRw6dIDY4UrqWrvJ66mhqPFFmjp7iXTW00060Ug2JZ176OpzIrFuyvu7EQ0t5ka1TWY35UyzwzR7Fgcz5lCUHiOaV0ZO32GirbXsm3QeJbnphEoWEGl8iXktz3NwwTXkly9k0qGnyV70Ll7uyCUELMjrJpRTQiTtBC/K12yDgvJ4J0h5DYWCiJy6lgN4czUt3VHqO42OSCEl3VVEG3ZR0xUh3FlPb0sN+Ye3UtRdTV3aNApjjRR37aeLdAq9hXbPoDeUSaE3v2bVHZ5BtnX3v+/zEPt9MgXWTrG1si66iJ70QmKRHNrCBTTEcpkaamJx34s05MynK3cG7cVn0dPZxuyu7RQ3rKe4dQf1+YtoXPQxIjlFtEw5l5zCUqYWZJGbEdxX4/7qdZfD+6C9HvKmQGYhZOSO1t/sqFMoiEjSeV83vZ5GukWh/iXaomHaDu4kK38S1RnzyHnlQToOH6Qqdwm51X9itlfTG8nnQG8OZ9Q9TLunE4l1kRdtJp1eeoiwNbSAudF95NHRf0qswzPY7HN4MTaXT4YfITzgVNlhz2W3T6OKKSwPvUQOXbycfgZ9FmFF9zrSiALQGsrnhazzyIm28Ep4PgcKl3H2lAzaQvnkTppCfqyZjMMvkZZbTCSvlLnzF1LTl8v+gzXMKM6jpb6a3dHJrJxbTGleBuHQoAv+7hDrg5otMOVMCA9xHSfaC+tugkVXQPG8EdsXCgURmTjcobcDItn9v+V3dXXSsfMpsvJLOZAxm7ysTIpzM2h+6c9UtvYR7Wghv2kH4aZdpDftJr99Hw0ZZRwKT2Nm+2bSvJftOSvYFD4T62rikp41lEeraAiXMD1afZyC4uo8n1J79Znl22Mz6SCDHLposEkssEoej53DpCzjLd1/otsjFFg7lZHZbC+6gJy2PXQVLqCg6wAZbZXECmdRktZJ2YE1HM5dwOazbiTU20Zo5kpKrJWiokJKyxec1F+hQkFE5ES4QywK4TSo30lv3cu0xjLJ6muhtfEQ3Z5GdPo5dLUdpuNwDd1Vm5jas4/QlDNo6+gkPTOHqfVP09rVS7enkdtRTV1kOgva12MeZVPBBYRCxoaeGbyzcw2zYvtptjwKvJVmcjgUns6UvgMUWjtPRc/g/PDW/tJ6PUzEolRMuYrln7n9pDZPnddERE6EWTwQAErmEymZH3/uB7xmfK1XXXPUuXkDpkuhP2yWBeteemRh52EKMgvx9noKsospCIVobu9hb+0+ZuVPp6nqCbqiBuk5+Lbf0BqZTNGSq09xI49PoSAikkgDw2agrHi/ecst7Z9VkJNOwZzg9FDxe15te+ZbmZbIGgcIjdL3iIjIOKBQEBGRfgoFERHpp1AQEZF+CgUREemnUBARkX4KBRER6adQEBGRfuNumAszqwP2neTHS4D6ESwnmbQtY5O2ZWzStsAsdy89XqNxFwqnwswqhjP2x3igbRmbtC1jk7Zl+HT6SERE+ikURESkX6qFwm3JLmAEaVvGJm3L2KRtGaaUuqYgIiLHlmpHCiIicgwKBRER6ZcyoWBmF5nZS2a208xuTHY9J8rM9prZZjPbaGYVwbxJZvaomb0S/FmU7DqPxszuNLNaM9syYN5Ra7e4m4L99KKZnZO8yl9viG35uplVB/tmo5ldMmDZl4JtecnM3p2cql/PzGaY2Voz22ZmW83s88H8cbdfjrEt43G/ZJrZ82a2KdiWbwTz55jZc0HNPzez9GB+RvB+Z7B89ikX4e4T/gWEgV3AXCAd2AQsSnZdJ7gNe4GSQfP+A7gxmL4R+Hay6xyi9rcA5wBbjlc7cAnwCGDASuC5ZNc/jG35OvBPR2m7KPi3lgHMCf4NhpO9DUFt04Bzguk84OWg3nG3X46xLeNxvxiQG0xHgOeCv+/7gA8G828FPhNM/y1wazD9QeDnp1pDqhwprAB2uvtud+8BfgZcnuSaRsLlwF3B9F3AFUmsZUju/iTQOGj2ULVfDtztcc8ChWY2Wk8iPK4htmUolwM/c/dud98D7CT+bzHp3P2gu28IpluB7UAZ43C/HGNbhjKW94u7e1vwNhK8HHgHcH8wf/B+ObK/7gcuMDM7lRpSJRTKgMoB76s49j+asciBNWa23sxWB/OmuPvBYPoQMCU5pZ2UoWofr/vqc8FplTsHnMYbF9sSnHJYSvy30nG9XwZtC4zD/WJmYTPbCNQCjxI/kmly976gycB6+7clWN4MFJ/K96dKKEwE57v7OcDFwGfN7C0DF3r8+HFc3l88nmsP3ALMA5YAB4H/m9xyhs/McoEHgC+4e8vAZeNtvxxlW8blfnH3qLsvAcqJH8G8YTS/P1VCoRqYMeB9eTBv3HD36uDPWuBXxP+x1Bw5hA/+rE1ehSdsqNrH3b5y95rgP3IMuJ1XT0WM6W0xswjxH6I/dfdfBrPH5X452raM1/1yhLs3AWuBVcRP16UFiwbW278twfICoOFUvjdVQuEFYEFwBT+d+AWZB5Nc07CZWY6Z5R2ZBt4FbCG+DR8Lmn0M+E1yKjwpQ9X+IPDR4G6XlUDzgNMZY9Kgc+vvI75vIL4tHwzuEJkDLACeH+36jiY473wHsN3dvztg0bjbL0NtyzjdL6VmVhhMZwEXEr9GshZ4f9Bs8H45sr/eDzweHOGdvGRfbR+tF/G7J14mfn7uX5JdzwnWPpf43RKbgK1H6id+7vCPwCvAY8CkZNc6RP33Ej987yV+PvRTQ9VO/O6LHwb7aTOwPNn1D2Nb/ieo9cXgP+m0Ae3/JdiWl4CLk13/gLrOJ35q6EVgY/C6ZDzul2Nsy3jcL2cDfwlq3gJ8NZg/l3hw7QR+AWQE8zOD9zuD5XNPtQYNcyEiIv1S5fSRiIgMg0JBRET6KRRERKSfQkFERPopFEREpJ9CQWQUmdnbzOx3ya5DZCgKBRER6adQEDkKM7s2GNd+o5n9KBikrM3MvheMc/9HMysN2i4xs2eDgdd+NeAZBPPN7LFgbPwNZjYvWH2umd1vZjvM7KenOqqlyEhSKIgMYmYLgauB8zw+MFkU+DCQA1S4+xnAn4CvBR+5G7jB3c8m3oP2yPyfAj9098XAm4j3hIb4KJ5fID6u/1zgvIRvlMgwpR2/iUjKuQBYBrwQ/BKfRXxguBjw86DNPcAvzawAKHT3PwXz7wJ+EYxVVebuvwJw9y6AYH3Pu3tV8H4jMBt4KvGbJXJ8CgWR1zPgLnf/0mtmmn1lULuTHSOme8B0FP0/lDFEp49EXu+PwPvNbDL0P7d4FvH/L0dGqrwGeMrdm4HDZvbmYP5HgD95/AlgVWZ2RbCODDPLHtWtEDkJ+g1FZBB332ZmXyb+pLsQ8RFRPwu0AyuCZbXErztAfOjiW4Mf+ruBTwTzPwL8yMy+GazjA6O4GSInRaOkigyTmbW5e26y6xBJJJ0+EhGRfjpSEBGRfjpSEBGRfgoFERHpp1AQEZF+CgUREemnUBARkX7/P5hb2FLQzN45AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6514 samples, validate on 6514 samples\n",
      "Epoch 1/50\n",
      "6514/6514 [==============================] - 5s 789us/step - loss: 0.4574 - acc: 0.8804 - val_loss: 0.2661 - val_acc: 0.9730\n",
      "Epoch 2/50\n",
      "6514/6514 [==============================] - 1s 188us/step - loss: 0.1871 - acc: 0.9739 - val_loss: 0.1365 - val_acc: 0.9754\n",
      "Epoch 3/50\n",
      "6514/6514 [==============================] - 1s 212us/step - loss: 0.1138 - acc: 0.9770 - val_loss: 0.0967 - val_acc: 0.9776\n",
      "Epoch 4/50\n",
      "6514/6514 [==============================] - 2s 246us/step - loss: 0.0869 - acc: 0.9793 - val_loss: 0.0785 - val_acc: 0.9814\n",
      "Epoch 5/50\n",
      "6514/6514 [==============================] - 1s 216us/step - loss: 0.0734 - acc: 0.9830 - val_loss: 0.0685 - val_acc: 0.9834\n",
      "Epoch 6/50\n",
      "6514/6514 [==============================] - 2s 247us/step - loss: 0.0655 - acc: 0.9834 - val_loss: 0.0624 - val_acc: 0.9836\n",
      "Epoch 7/50\n",
      "6514/6514 [==============================] - 2s 272us/step - loss: 0.0604 - acc: 0.9842 - val_loss: 0.0582 - val_acc: 0.9853\n",
      "Epoch 8/50\n",
      "6514/6514 [==============================] - 2s 239us/step - loss: 0.0569 - acc: 0.9853 - val_loss: 0.0553 - val_acc: 0.9857\n",
      "Epoch 9/50\n",
      "6514/6514 [==============================] - 1s 230us/step - loss: 0.0543 - acc: 0.9866 - val_loss: 0.0532 - val_acc: 0.9879\n",
      "Epoch 10/50\n",
      "6514/6514 [==============================] - 1s 224us/step - loss: 0.0526 - acc: 0.9886 - val_loss: 0.0516 - val_acc: 0.9889\n",
      "Epoch 11/50\n",
      "6514/6514 [==============================] - 2s 261us/step - loss: 0.0512 - acc: 0.9885 - val_loss: 0.0504 - val_acc: 0.9889\n",
      "Epoch 12/50\n",
      "6514/6514 [==============================] - 1s 216us/step - loss: 0.0501 - acc: 0.9889 - val_loss: 0.0495 - val_acc: 0.9889\n",
      "Epoch 13/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0492 - acc: 0.9889 - val_loss: 0.0486 - val_acc: 0.9889\n",
      "Epoch 14/50\n",
      "6514/6514 [==============================] - 1s 208us/step - loss: 0.0484 - acc: 0.9889 - val_loss: 0.0478 - val_acc: 0.9889\n",
      "Epoch 15/50\n",
      "6514/6514 [==============================] - 2s 252us/step - loss: 0.0475 - acc: 0.9889 - val_loss: 0.0472 - val_acc: 0.9889\n",
      "Epoch 16/50\n",
      "6514/6514 [==============================] - 2s 279us/step - loss: 0.0469 - acc: 0.9889 - val_loss: 0.0463 - val_acc: 0.9889\n",
      "Epoch 17/50\n",
      "6514/6514 [==============================] - 2s 241us/step - loss: 0.0462 - acc: 0.9888 - val_loss: 0.0457 - val_acc: 0.9888\n",
      "Epoch 18/50\n",
      "6514/6514 [==============================] - 2s 243us/step - loss: 0.0456 - acc: 0.9888 - val_loss: 0.0451 - val_acc: 0.9888\n",
      "Epoch 19/50\n",
      "6514/6514 [==============================] - 1s 208us/step - loss: 0.0450 - acc: 0.9886 - val_loss: 0.0445 - val_acc: 0.9888\n",
      "Epoch 20/50\n",
      "6514/6514 [==============================] - 2s 279us/step - loss: 0.0446 - acc: 0.9886 - val_loss: 0.0440 - val_acc: 0.9886\n",
      "Epoch 21/50\n",
      "6514/6514 [==============================] - 2s 253us/step - loss: 0.0439 - acc: 0.9886 - val_loss: 0.0435 - val_acc: 0.9886\n",
      "Epoch 22/50\n",
      "6514/6514 [==============================] - 2s 246us/step - loss: 0.0434 - acc: 0.9886 - val_loss: 0.0431 - val_acc: 0.9886\n",
      "Epoch 23/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0431 - acc: 0.9886 - val_loss: 0.0426 - val_acc: 0.9886\n",
      "Epoch 24/50\n",
      "6514/6514 [==============================] - 1s 212us/step - loss: 0.0426 - acc: 0.9886 - val_loss: 0.0421 - val_acc: 0.9886\n",
      "Epoch 25/50\n",
      "6514/6514 [==============================] - 2s 270us/step - loss: 0.0422 - acc: 0.9886 - val_loss: 0.0416 - val_acc: 0.9886\n",
      "Epoch 26/50\n",
      "6514/6514 [==============================] - 1s 227us/step - loss: 0.0416 - acc: 0.9886 - val_loss: 0.0413 - val_acc: 0.9886\n",
      "Epoch 27/50\n",
      "6514/6514 [==============================] - 1s 219us/step - loss: 0.0412 - acc: 0.9886 - val_loss: 0.0408 - val_acc: 0.9886\n",
      "Epoch 28/50\n",
      "6514/6514 [==============================] - 1s 208us/step - loss: 0.0409 - acc: 0.9886 - val_loss: 0.0404 - val_acc: 0.9886\n",
      "Epoch 29/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0405 - acc: 0.9886 - val_loss: 0.0400 - val_acc: 0.9886\n",
      "Epoch 30/50\n",
      "6514/6514 [==============================] - 1s 208us/step - loss: 0.0400 - acc: 0.9886 - val_loss: 0.0395 - val_acc: 0.9886\n",
      "Epoch 31/50\n",
      "6514/6514 [==============================] - 1s 216us/step - loss: 0.0396 - acc: 0.9886 - val_loss: 0.0391 - val_acc: 0.9886\n",
      "Epoch 32/50\n",
      "6514/6514 [==============================] - 1s 219us/step - loss: 0.0392 - acc: 0.9888 - val_loss: 0.0386 - val_acc: 0.9888\n",
      "Epoch 33/50\n",
      "6514/6514 [==============================] - 1s 214us/step - loss: 0.0387 - acc: 0.9888 - val_loss: 0.0383 - val_acc: 0.9888\n",
      "Epoch 34/50\n",
      "6514/6514 [==============================] - 1s 200us/step - loss: 0.0383 - acc: 0.9888 - val_loss: 0.0380 - val_acc: 0.9888\n",
      "Epoch 35/50\n",
      "6514/6514 [==============================] - 1s 195us/step - loss: 0.0380 - acc: 0.9889 - val_loss: 0.0376 - val_acc: 0.9889\n",
      "Epoch 36/50\n",
      "6514/6514 [==============================] - 1s 202us/step - loss: 0.0377 - acc: 0.9889 - val_loss: 0.0373 - val_acc: 0.9889\n",
      "Epoch 37/50\n",
      "6514/6514 [==============================] - 1s 211us/step - loss: 0.0374 - acc: 0.9889 - val_loss: 0.0370 - val_acc: 0.9889\n",
      "Epoch 38/50\n",
      "6514/6514 [==============================] - 2s 241us/step - loss: 0.0371 - acc: 0.9889 - val_loss: 0.0367 - val_acc: 0.9889\n",
      "Epoch 39/50\n",
      "6514/6514 [==============================] - 1s 219us/step - loss: 0.0369 - acc: 0.9889 - val_loss: 0.0365 - val_acc: 0.9889\n",
      "Epoch 40/50\n",
      "6514/6514 [==============================] - 1s 205us/step - loss: 0.0365 - acc: 0.9889 - val_loss: 0.0363 - val_acc: 0.9889\n",
      "Epoch 41/50\n",
      "6514/6514 [==============================] - 1s 203us/step - loss: 0.0363 - acc: 0.9889 - val_loss: 0.0359 - val_acc: 0.9889\n",
      "Epoch 42/50\n",
      "6514/6514 [==============================] - 1s 198us/step - loss: 0.0361 - acc: 0.9889 - val_loss: 0.0357 - val_acc: 0.9889\n",
      "Epoch 43/50\n",
      "6514/6514 [==============================] - 1s 199us/step - loss: 0.0359 - acc: 0.9889 - val_loss: 0.0354 - val_acc: 0.9889\n",
      "Epoch 44/50\n",
      "6514/6514 [==============================] - 1s 192us/step - loss: 0.0356 - acc: 0.9889 - val_loss: 0.0352 - val_acc: 0.9889\n",
      "Epoch 45/50\n",
      "6514/6514 [==============================] - 1s 197us/step - loss: 0.0353 - acc: 0.9889 - val_loss: 0.0349 - val_acc: 0.9889\n",
      "Epoch 46/50\n",
      "6514/6514 [==============================] - 1s 200us/step - loss: 0.0351 - acc: 0.9889 - val_loss: 0.0347 - val_acc: 0.9889\n",
      "Epoch 47/50\n",
      "6514/6514 [==============================] - 1s 217us/step - loss: 0.0348 - acc: 0.9889 - val_loss: 0.0343 - val_acc: 0.9889\n",
      "Epoch 48/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0345 - acc: 0.9889 - val_loss: 0.0341 - val_acc: 0.9889\n",
      "Epoch 49/50\n",
      "6514/6514 [==============================] - 1s 203us/step - loss: 0.0341 - acc: 0.9889 - val_loss: 0.0339 - val_acc: 0.9889\n",
      "Epoch 50/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0340 - acc: 0.9889 - val_loss: 0.0335 - val_acc: 0.9889\n",
      "1629/1629 [==============================] - 0s 100us/step\n",
      "Train on 6514 samples, validate on 6514 samples\n",
      "Epoch 1/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0317 - acc: 0.9899 - val_loss: 0.0312 - val_acc: 0.9899\n",
      "Epoch 2/50\n",
      "6514/6514 [==============================] - 1s 199us/step - loss: 0.0314 - acc: 0.9899 - val_loss: 0.0310 - val_acc: 0.9899\n",
      "Epoch 3/50\n",
      "6514/6514 [==============================] - 1s 214us/step - loss: 0.0312 - acc: 0.9899 - val_loss: 0.0308 - val_acc: 0.9899\n",
      "Epoch 4/50\n",
      "6514/6514 [==============================] - 1s 218us/step - loss: 0.0309 - acc: 0.9899 - val_loss: 0.0306 - val_acc: 0.9899\n",
      "Epoch 5/50\n",
      "6514/6514 [==============================] - 1s 213us/step - loss: 0.0307 - acc: 0.9899 - val_loss: 0.0303 - val_acc: 0.9899\n",
      "Epoch 6/50\n",
      "6514/6514 [==============================] - 1s 222us/step - loss: 0.0305 - acc: 0.9899 - val_loss: 0.0301 - val_acc: 0.9899\n",
      "Epoch 7/50\n",
      "6514/6514 [==============================] - 1s 207us/step - loss: 0.0303 - acc: 0.9899 - val_loss: 0.0299 - val_acc: 0.9899\n",
      "Epoch 8/50\n",
      "6514/6514 [==============================] - 1s 207us/step - loss: 0.0301 - acc: 0.9899 - val_loss: 0.0297 - val_acc: 0.9899\n",
      "Epoch 9/50\n",
      "6514/6514 [==============================] - 1s 230us/step - loss: 0.0298 - acc: 0.9899 - val_loss: 0.0295 - val_acc: 0.9899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "6514/6514 [==============================] - 2s 270us/step - loss: 0.0298 - acc: 0.9899 - val_loss: 0.0294 - val_acc: 0.9899\n",
      "Epoch 11/50\n",
      "6514/6514 [==============================] - 2s 250us/step - loss: 0.0296 - acc: 0.9899 - val_loss: 0.0292 - val_acc: 0.9899\n",
      "Epoch 12/50\n",
      "6514/6514 [==============================] - 2s 266us/step - loss: 0.0293 - acc: 0.9899 - val_loss: 0.0291 - val_acc: 0.9899\n",
      "Epoch 13/50\n",
      "6514/6514 [==============================] - 2s 274us/step - loss: 0.0293 - acc: 0.9899 - val_loss: 0.0289 - val_acc: 0.9899\n",
      "Epoch 14/50\n",
      "6514/6514 [==============================] - 2s 270us/step - loss: 0.0292 - acc: 0.9899 - val_loss: 0.0287 - val_acc: 0.9899\n",
      "Epoch 15/50\n",
      "6514/6514 [==============================] - 1s 227us/step - loss: 0.0289 - acc: 0.9899 - val_loss: 0.0285 - val_acc: 0.9899\n",
      "Epoch 16/50\n",
      "6514/6514 [==============================] - 1s 211us/step - loss: 0.0287 - acc: 0.9899 - val_loss: 0.0284 - val_acc: 0.9899\n",
      "Epoch 17/50\n",
      "6514/6514 [==============================] - 2s 248us/step - loss: 0.0286 - acc: 0.9899 - val_loss: 0.0282 - val_acc: 0.9899\n",
      "Epoch 18/50\n",
      "6514/6514 [==============================] - 1s 222us/step - loss: 0.0284 - acc: 0.9899 - val_loss: 0.0281 - val_acc: 0.9899\n",
      "Epoch 19/50\n",
      "6514/6514 [==============================] - 2s 230us/step - loss: 0.0283 - acc: 0.9899 - val_loss: 0.0279 - val_acc: 0.9899\n",
      "Epoch 20/50\n",
      "6514/6514 [==============================] - 1s 225us/step - loss: 0.0281 - acc: 0.9899 - val_loss: 0.0278 - val_acc: 0.9899\n",
      "Epoch 21/50\n",
      "6514/6514 [==============================] - 1s 214us/step - loss: 0.0281 - acc: 0.9899 - val_loss: 0.0276 - val_acc: 0.9899\n",
      "Epoch 22/50\n",
      "6514/6514 [==============================] - 2s 237us/step - loss: 0.0278 - acc: 0.9899 - val_loss: 0.0275 - val_acc: 0.9899\n",
      "Epoch 23/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0277 - acc: 0.9899 - val_loss: 0.0274 - val_acc: 0.9899\n",
      "Epoch 24/50\n",
      "6514/6514 [==============================] - 2s 252us/step - loss: 0.0277 - acc: 0.9899 - val_loss: 0.0272 - val_acc: 0.9899\n",
      "Epoch 25/50\n",
      "6514/6514 [==============================] - 2s 233us/step - loss: 0.0274 - acc: 0.9899 - val_loss: 0.0271 - val_acc: 0.9899\n",
      "Epoch 26/50\n",
      "6514/6514 [==============================] - 1s 200us/step - loss: 0.0272 - acc: 0.9899 - val_loss: 0.0270 - val_acc: 0.9899\n",
      "Epoch 27/50\n",
      "6514/6514 [==============================] - 2s 275us/step - loss: 0.0271 - acc: 0.9897 - val_loss: 0.0268 - val_acc: 0.9899\n",
      "Epoch 28/50\n",
      "6514/6514 [==============================] - 2s 257us/step - loss: 0.0271 - acc: 0.9899 - val_loss: 0.0267 - val_acc: 0.9899\n",
      "Epoch 29/50\n",
      "6514/6514 [==============================] - 1s 230us/step - loss: 0.0269 - acc: 0.9899 - val_loss: 0.0265 - val_acc: 0.9899\n",
      "Epoch 30/50\n",
      "6514/6514 [==============================] - 2s 247us/step - loss: 0.0268 - acc: 0.9899 - val_loss: 0.0264 - val_acc: 0.9899\n",
      "Epoch 31/50\n",
      "6514/6514 [==============================] - 2s 233us/step - loss: 0.0265 - acc: 0.9899 - val_loss: 0.0263 - val_acc: 0.9899\n",
      "Epoch 32/50\n",
      "6514/6514 [==============================] - 2s 248us/step - loss: 0.0265 - acc: 0.9899 - val_loss: 0.0264 - val_acc: 0.9900\n",
      "Epoch 33/50\n",
      "6514/6514 [==============================] - 2s 285us/step - loss: 0.0265 - acc: 0.9899 - val_loss: 0.0261 - val_acc: 0.9899\n",
      "Epoch 34/50\n",
      "6514/6514 [==============================] - 1s 228us/step - loss: 0.0263 - acc: 0.9899 - val_loss: 0.0260 - val_acc: 0.9899\n",
      "Epoch 35/50\n",
      "6514/6514 [==============================] - 1s 230us/step - loss: 0.0262 - acc: 0.9899 - val_loss: 0.0259 - val_acc: 0.9899\n",
      "Epoch 36/50\n",
      "6514/6514 [==============================] - 2s 248us/step - loss: 0.0261 - acc: 0.9899 - val_loss: 0.0258 - val_acc: 0.9899\n",
      "Epoch 37/50\n",
      "6514/6514 [==============================] - 2s 246us/step - loss: 0.0261 - acc: 0.9900 - val_loss: 0.0257 - val_acc: 0.9899\n",
      "Epoch 38/50\n",
      "6514/6514 [==============================] - 1s 217us/step - loss: 0.0259 - acc: 0.9899 - val_loss: 0.0255 - val_acc: 0.9899\n",
      "Epoch 39/50\n",
      "6514/6514 [==============================] - 2s 265us/step - loss: 0.0258 - acc: 0.9899 - val_loss: 0.0254 - val_acc: 0.9899\n",
      "Epoch 40/50\n",
      "6514/6514 [==============================] - 2s 247us/step - loss: 0.0257 - acc: 0.9899 - val_loss: 0.0253 - val_acc: 0.9899\n",
      "Epoch 41/50\n",
      "6514/6514 [==============================] - 1s 216us/step - loss: 0.0256 - acc: 0.9899 - val_loss: 0.0253 - val_acc: 0.9899\n",
      "Epoch 42/50\n",
      "6514/6514 [==============================] - 2s 254us/step - loss: 0.0255 - acc: 0.9899 - val_loss: 0.0251 - val_acc: 0.9899\n",
      "Epoch 43/50\n",
      "6514/6514 [==============================] - 2s 241us/step - loss: 0.0253 - acc: 0.9900 - val_loss: 0.0251 - val_acc: 0.9902\n",
      "Epoch 44/50\n",
      "6514/6514 [==============================] - 1s 229us/step - loss: 0.0253 - acc: 0.9900 - val_loss: 0.0249 - val_acc: 0.9902\n",
      "Epoch 45/50\n",
      "6514/6514 [==============================] - 1s 228us/step - loss: 0.0251 - acc: 0.9908 - val_loss: 0.0248 - val_acc: 0.9902\n",
      "Epoch 46/50\n",
      "6514/6514 [==============================] - 1s 229us/step - loss: 0.0250 - acc: 0.9908 - val_loss: 0.0247 - val_acc: 0.9911\n",
      "Epoch 47/50\n",
      "6514/6514 [==============================] - 1s 229us/step - loss: 0.0249 - acc: 0.9912 - val_loss: 0.0246 - val_acc: 0.9912\n",
      "Epoch 48/50\n",
      "6514/6514 [==============================] - 2s 248us/step - loss: 0.0248 - acc: 0.9911 - val_loss: 0.0245 - val_acc: 0.9912\n",
      "Epoch 49/50\n",
      "6514/6514 [==============================] - 1s 220us/step - loss: 0.0247 - acc: 0.9912 - val_loss: 0.0244 - val_acc: 0.9912\n",
      "Epoch 50/50\n",
      "6514/6514 [==============================] - 2s 238us/step - loss: 0.0247 - acc: 0.9909 - val_loss: 0.0243 - val_acc: 0.9912\n",
      "1629/1629 [==============================] - 0s 105us/step\n",
      "Train on 6514 samples, validate on 6514 samples\n",
      "Epoch 1/50\n",
      "6514/6514 [==============================] - 2s 241us/step - loss: 0.0262 - acc: 0.9908 - val_loss: 0.0258 - val_acc: 0.9902\n",
      "Epoch 2/50\n",
      "6514/6514 [==============================] - 1s 204us/step - loss: 0.0260 - acc: 0.9906 - val_loss: 0.0257 - val_acc: 0.9906\n",
      "Epoch 3/50\n",
      "6514/6514 [==============================] - 1s 203us/step - loss: 0.0260 - acc: 0.9905 - val_loss: 0.0257 - val_acc: 0.9903\n",
      "Epoch 4/50\n",
      "6514/6514 [==============================] - 1s 193us/step - loss: 0.0259 - acc: 0.9903 - val_loss: 0.0255 - val_acc: 0.9905\n",
      "Epoch 5/50\n",
      "6514/6514 [==============================] - 1s 193us/step - loss: 0.0259 - acc: 0.9902 - val_loss: 0.0255 - val_acc: 0.9909\n",
      "Epoch 6/50\n",
      "6514/6514 [==============================] - 1s 219us/step - loss: 0.0257 - acc: 0.9908 - val_loss: 0.0254 - val_acc: 0.9905\n",
      "Epoch 7/50\n",
      "6514/6514 [==============================] - 1s 207us/step - loss: 0.0256 - acc: 0.9906 - val_loss: 0.0253 - val_acc: 0.9908\n",
      "Epoch 8/50\n",
      "6514/6514 [==============================] - 1s 203us/step - loss: 0.0256 - acc: 0.9909 - val_loss: 0.0252 - val_acc: 0.9906\n",
      "Epoch 9/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0255 - acc: 0.9906 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "Epoch 10/50\n",
      "6514/6514 [==============================] - 2s 238us/step - loss: 0.0255 - acc: 0.9908 - val_loss: 0.0251 - val_acc: 0.9911\n",
      "Epoch 11/50\n",
      "6514/6514 [==============================] - 1s 204us/step - loss: 0.0254 - acc: 0.9908 - val_loss: 0.0251 - val_acc: 0.9922\n",
      "Epoch 12/50\n",
      "6514/6514 [==============================] - 1s 207us/step - loss: 0.0253 - acc: 0.9912 - val_loss: 0.0250 - val_acc: 0.9909\n",
      "Epoch 13/50\n",
      "6514/6514 [==============================] - 1s 228us/step - loss: 0.0252 - acc: 0.9908 - val_loss: 0.0249 - val_acc: 0.9912\n",
      "Epoch 14/50\n",
      "6514/6514 [==============================] - 2s 253us/step - loss: 0.0252 - acc: 0.9917 - val_loss: 0.0248 - val_acc: 0.9911\n",
      "Epoch 15/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0250 - acc: 0.9911 - val_loss: 0.0248 - val_acc: 0.9905\n",
      "Epoch 16/50\n",
      "6514/6514 [==============================] - 2s 250us/step - loss: 0.0250 - acc: 0.9908 - val_loss: 0.0246 - val_acc: 0.9909\n",
      "Epoch 17/50\n",
      "6514/6514 [==============================] - 2s 238us/step - loss: 0.0250 - acc: 0.9909 - val_loss: 0.0246 - val_acc: 0.9911\n",
      "Epoch 18/50\n",
      "6514/6514 [==============================] - 2s 245us/step - loss: 0.0248 - acc: 0.9909 - val_loss: 0.0246 - val_acc: 0.9911\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0248 - acc: 0.9908 - val_loss: 0.0245 - val_acc: 0.9914\n",
      "Epoch 20/50\n",
      "6514/6514 [==============================] - 1s 216us/step - loss: 0.0247 - acc: 0.9914 - val_loss: 0.0244 - val_acc: 0.9916\n",
      "Epoch 21/50\n",
      "6514/6514 [==============================] - 1s 216us/step - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0244 - val_acc: 0.9911\n",
      "Epoch 22/50\n",
      "6514/6514 [==============================] - 2s 242us/step - loss: 0.0246 - acc: 0.9911 - val_loss: 0.0243 - val_acc: 0.9912\n",
      "Epoch 23/50\n",
      "6514/6514 [==============================] - 2s 349us/step - loss: 0.0245 - acc: 0.9914 - val_loss: 0.0243 - val_acc: 0.9909\n",
      "Epoch 24/50\n",
      "6514/6514 [==============================] - 2s 281us/step - loss: 0.0245 - acc: 0.9911 - val_loss: 0.0242 - val_acc: 0.9916\n",
      "Epoch 25/50\n",
      "6514/6514 [==============================] - 2s 244us/step - loss: 0.0245 - acc: 0.9912 - val_loss: 0.0241 - val_acc: 0.9914\n",
      "Epoch 26/50\n",
      "6514/6514 [==============================] - 2s 261us/step - loss: 0.0244 - acc: 0.9919 - val_loss: 0.0241 - val_acc: 0.9912\n",
      "Epoch 27/50\n",
      "6514/6514 [==============================] - 2s 270us/step - loss: 0.0244 - acc: 0.9916 - val_loss: 0.0240 - val_acc: 0.9922\n",
      "Epoch 28/50\n",
      "6514/6514 [==============================] - 2s 318us/step - loss: 0.0243 - acc: 0.9923 - val_loss: 0.0241 - val_acc: 0.9914\n",
      "Epoch 29/50\n",
      "6514/6514 [==============================] - 2s 252us/step - loss: 0.0242 - acc: 0.9914 - val_loss: 0.0239 - val_acc: 0.9923\n",
      "Epoch 30/50\n",
      "6514/6514 [==============================] - 2s 291us/step - loss: 0.0242 - acc: 0.9926 - val_loss: 0.0239 - val_acc: 0.9914\n",
      "Epoch 31/50\n",
      "6514/6514 [==============================] - 2s 234us/step - loss: 0.0242 - acc: 0.9917 - val_loss: 0.0239 - val_acc: 0.9912\n",
      "Epoch 32/50\n",
      "6514/6514 [==============================] - 2s 260us/step - loss: 0.0242 - acc: 0.9919 - val_loss: 0.0238 - val_acc: 0.9916\n",
      "Epoch 33/50\n",
      "6514/6514 [==============================] - 2s 236us/step - loss: 0.0242 - acc: 0.9917 - val_loss: 0.0238 - val_acc: 0.9917\n",
      "Epoch 34/50\n",
      "6514/6514 [==============================] - 1s 202us/step - loss: 0.0241 - acc: 0.9912 - val_loss: 0.0237 - val_acc: 0.9914\n",
      "Epoch 35/50\n",
      "6514/6514 [==============================] - 1s 223us/step - loss: 0.0241 - acc: 0.9916 - val_loss: 0.0237 - val_acc: 0.9916\n",
      "Epoch 36/50\n",
      "6514/6514 [==============================] - 1s 224us/step - loss: 0.0240 - acc: 0.9919 - val_loss: 0.0238 - val_acc: 0.9932\n",
      "Epoch 37/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0240 - acc: 0.9919 - val_loss: 0.0237 - val_acc: 0.9932\n",
      "Epoch 38/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0240 - acc: 0.9931 - val_loss: 0.0236 - val_acc: 0.9920\n",
      "Epoch 39/50\n",
      "6514/6514 [==============================] - 2s 242us/step - loss: 0.0239 - acc: 0.9919 - val_loss: 0.0236 - val_acc: 0.9929\n",
      "Epoch 40/50\n",
      "6514/6514 [==============================] - 1s 219us/step - loss: 0.0238 - acc: 0.9925 - val_loss: 0.0236 - val_acc: 0.9922\n",
      "Epoch 41/50\n",
      "6514/6514 [==============================] - 1s 202us/step - loss: 0.0239 - acc: 0.9923 - val_loss: 0.0235 - val_acc: 0.9917\n",
      "Epoch 42/50\n",
      "6514/6514 [==============================] - 1s 200us/step - loss: 0.0239 - acc: 0.9919 - val_loss: 0.0235 - val_acc: 0.9923\n",
      "Epoch 43/50\n",
      "6514/6514 [==============================] - 1s 214us/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0234 - val_acc: 0.9917\n",
      "Epoch 44/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0238 - acc: 0.9922 - val_loss: 0.0235 - val_acc: 0.9919\n",
      "Epoch 45/50\n",
      "6514/6514 [==============================] - 2s 235us/step - loss: 0.0237 - acc: 0.9916 - val_loss: 0.0234 - val_acc: 0.9928\n",
      "Epoch 46/50\n",
      "6514/6514 [==============================] - 1s 203us/step - loss: 0.0237 - acc: 0.9923 - val_loss: 0.0233 - val_acc: 0.9919\n",
      "Epoch 47/50\n",
      "6514/6514 [==============================] - 1s 215us/step - loss: 0.0237 - acc: 0.9923 - val_loss: 0.0233 - val_acc: 0.9925\n",
      "Epoch 48/50\n",
      "6514/6514 [==============================] - 1s 209us/step - loss: 0.0237 - acc: 0.9922 - val_loss: 0.0233 - val_acc: 0.9931\n",
      "Epoch 49/50\n",
      "6514/6514 [==============================] - 2s 250us/step - loss: 0.0235 - acc: 0.9931 - val_loss: 0.0233 - val_acc: 0.9919\n",
      "Epoch 50/50\n",
      "6514/6514 [==============================] - 1s 230us/step - loss: 0.0237 - acc: 0.9923 - val_loss: 0.0232 - val_acc: 0.9928\n",
      "1629/1629 [==============================] - 0s 118us/step\n",
      "Train on 4071 samples, validate on 4071 samples\n",
      "Epoch 1/50\n",
      "4071/4071 [==============================] - 1s 218us/step - loss: 0.0221 - acc: 0.9929 - val_loss: 0.0215 - val_acc: 0.9939\n",
      "Epoch 2/50\n",
      "4071/4071 [==============================] - 1s 203us/step - loss: 0.0218 - acc: 0.9924 - val_loss: 0.0213 - val_acc: 0.9934\n",
      "Epoch 3/50\n",
      "4071/4071 [==============================] - 1s 199us/step - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0212 - val_acc: 0.9934\n",
      "Epoch 4/50\n",
      "4071/4071 [==============================] - 1s 228us/step - loss: 0.0215 - acc: 0.9921 - val_loss: 0.0212 - val_acc: 0.9936\n",
      "Epoch 5/50\n",
      "4071/4071 [==============================] - 1s 218us/step - loss: 0.0215 - acc: 0.9931 - val_loss: 0.0211 - val_acc: 0.9944\n",
      "Epoch 6/50\n",
      "4071/4071 [==============================] - 1s 211us/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.0211 - val_acc: 0.9939\n",
      "Epoch 7/50\n",
      "4071/4071 [==============================] - 1s 211us/step - loss: 0.0214 - acc: 0.9931 - val_loss: 0.0210 - val_acc: 0.9929\n",
      "Epoch 8/50\n",
      "4071/4071 [==============================] - 1s 180us/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.0209 - val_acc: 0.9936\n",
      "Epoch 9/50\n",
      "4071/4071 [==============================] - 1s 161us/step - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0209 - val_acc: 0.9934\n",
      "Epoch 10/50\n",
      "4071/4071 [==============================] - 1s 271us/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0208 - val_acc: 0.9941\n",
      "Epoch 11/50\n",
      "4071/4071 [==============================] - 1s 194us/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.0208 - val_acc: 0.9936\n",
      "Epoch 12/50\n",
      "4071/4071 [==============================] - 1s 197us/step - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0208 - val_acc: 0.9936\n",
      "Epoch 13/50\n",
      "4071/4071 [==============================] - 1s 162us/step - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0208 - val_acc: 0.9936\n",
      "Epoch 14/50\n",
      "4071/4071 [==============================] - 1s 145us/step - loss: 0.0211 - acc: 0.9929 - val_loss: 0.0207 - val_acc: 0.9939\n",
      "Epoch 15/50\n",
      "4071/4071 [==============================] - 1s 189us/step - loss: 0.0210 - acc: 0.9929 - val_loss: 0.0207 - val_acc: 0.9941\n",
      "Epoch 16/50\n",
      "4071/4071 [==============================] - 1s 162us/step - loss: 0.0211 - acc: 0.9934 - val_loss: 0.0206 - val_acc: 0.9939\n",
      "Epoch 17/50\n",
      "4071/4071 [==============================] - 1s 151us/step - loss: 0.0211 - acc: 0.9936 - val_loss: 0.0207 - val_acc: 0.9936\n",
      "Epoch 18/50\n",
      "4071/4071 [==============================] - 1s 164us/step - loss: 0.0209 - acc: 0.9939 - val_loss: 0.0206 - val_acc: 0.9936\n",
      "Epoch 19/50\n",
      "4071/4071 [==============================] - 1s 163us/step - loss: 0.0209 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 0.9939\n",
      "Epoch 20/50\n",
      "4071/4071 [==============================] - 1s 161us/step - loss: 0.0208 - acc: 0.9939 - val_loss: 0.0205 - val_acc: 0.9936\n",
      "Epoch 21/50\n",
      "4071/4071 [==============================] - 1s 160us/step - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0205 - val_acc: 0.9939\n",
      "Epoch 22/50\n",
      "4071/4071 [==============================] - 1s 165us/step - loss: 0.0209 - acc: 0.9931 - val_loss: 0.0205 - val_acc: 0.9939\n",
      "Epoch 23/50\n",
      "4071/4071 [==============================] - 1s 147us/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0204 - val_acc: 0.9944\n",
      "Epoch 24/50\n",
      "4071/4071 [==============================] - 1s 199us/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0204 - val_acc: 0.9936\n",
      "Epoch 25/50\n",
      "4071/4071 [==============================] - 1s 230us/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0204 - val_acc: 0.9936\n",
      "Epoch 26/50\n",
      "4071/4071 [==============================] - 1s 194us/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0204 - val_acc: 0.9939\n",
      "Epoch 27/50\n",
      "4071/4071 [==============================] - 1s 254us/step - loss: 0.0206 - acc: 0.9926 - val_loss: 0.0205 - val_acc: 0.9941\n",
      "Epoch 28/50\n",
      "4071/4071 [==============================] - 1s 237us/step - loss: 0.0207 - acc: 0.9941 - val_loss: 0.0203 - val_acc: 0.9941\n",
      "Epoch 29/50\n",
      "4071/4071 [==============================] - 1s 218us/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0203 - val_acc: 0.9946\n",
      "Epoch 30/50\n",
      "4071/4071 [==============================] - 1s 184us/step - loss: 0.0206 - acc: 0.9936 - val_loss: 0.0203 - val_acc: 0.9939\n",
      "Epoch 31/50\n",
      "4071/4071 [==============================] - 1s 191us/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.0202 - val_acc: 0.9941\n",
      "Epoch 32/50\n",
      "4071/4071 [==============================] - 1s 190us/step - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0203 - val_acc: 0.9936\n",
      "Epoch 33/50\n",
      "4071/4071 [==============================] - 1s 214us/step - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0202 - val_acc: 0.9944\n",
      "Epoch 34/50\n",
      "4071/4071 [==============================] - 1s 194us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 0.0202 - val_acc: 0.9939\n",
      "Epoch 35/50\n",
      "4071/4071 [==============================] - 1s 190us/step - loss: 0.0205 - acc: 0.9934 - val_loss: 0.0202 - val_acc: 0.9939\n",
      "Epoch 36/50\n",
      "4071/4071 [==============================] - 1s 167us/step - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0201 - val_acc: 0.9936\n",
      "Epoch 37/50\n",
      "4071/4071 [==============================] - 1s 159us/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0201 - val_acc: 0.9939\n",
      "Epoch 38/50\n",
      "4071/4071 [==============================] - 1s 172us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 0.0201 - val_acc: 0.9941\n",
      "Epoch 39/50\n",
      "4071/4071 [==============================] - 1s 164us/step - loss: 0.0203 - acc: 0.9939 - val_loss: 0.0202 - val_acc: 0.9931\n",
      "Epoch 40/50\n",
      "4071/4071 [==============================] - 1s 149us/step - loss: 0.0206 - acc: 0.9926 - val_loss: 0.0201 - val_acc: 0.9934\n",
      "Epoch 41/50\n",
      "4071/4071 [==============================] - 1s 156us/step - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0201 - val_acc: 0.9941\n",
      "Epoch 42/50\n",
      "4071/4071 [==============================] - 1s 168us/step - loss: 0.0204 - acc: 0.9946 - val_loss: 0.0200 - val_acc: 0.9936\n",
      "Epoch 43/50\n",
      "4071/4071 [==============================] - 1s 147us/step - loss: 0.0203 - acc: 0.9931 - val_loss: 0.0200 - val_acc: 0.9941\n",
      "Epoch 44/50\n",
      "4071/4071 [==============================] - 1s 154us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 0.0199 - val_acc: 0.9936\n",
      "Epoch 45/50\n",
      "4071/4071 [==============================] - 1s 148us/step - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0200 - val_acc: 0.9931\n",
      "Epoch 46/50\n",
      "4071/4071 [==============================] - 1s 155us/step - loss: 0.0204 - acc: 0.9931 - val_loss: 0.0199 - val_acc: 0.9946\n",
      "Epoch 47/50\n",
      "4071/4071 [==============================] - 1s 158us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9936\n",
      "Epoch 48/50\n",
      "4071/4071 [==============================] - 1s 150us/step - loss: 0.0204 - acc: 0.9929 - val_loss: 0.0199 - val_acc: 0.9946\n",
      "Epoch 49/50\n",
      "4071/4071 [==============================] - 1s 208us/step - loss: 0.0201 - acc: 0.9944 - val_loss: 0.0199 - val_acc: 0.9944\n",
      "Epoch 50/50\n",
      "4071/4071 [==============================] - 1s 199us/step - loss: 0.0202 - acc: 0.9931 - val_loss: 0.0199 - val_acc: 0.9934\n",
      "4072/4072 [==============================] - 0s 71us/step\n",
      "Train on 4071 samples, validate on 4071 samples\n",
      "Epoch 1/50\n",
      "4071/4071 [==============================] - 1s 156us/step - loss: 0.0247 - acc: 0.9936 - val_loss: 0.0237 - val_acc: 0.9941\n",
      "Epoch 2/50\n",
      "4071/4071 [==============================] - 1s 156us/step - loss: 0.0241 - acc: 0.9939 - val_loss: 0.0235 - val_acc: 0.9939\n",
      "Epoch 3/50\n",
      "4071/4071 [==============================] - 1s 162us/step - loss: 0.0239 - acc: 0.9939 - val_loss: 0.0234 - val_acc: 0.9934\n",
      "Epoch 4/50\n",
      "4071/4071 [==============================] - 1s 155us/step - loss: 0.0236 - acc: 0.9936 - val_loss: 0.0233 - val_acc: 0.9936\n",
      "Epoch 5/50\n",
      "4071/4071 [==============================] - 1s 149us/step - loss: 0.0236 - acc: 0.9934 - val_loss: 0.0232 - val_acc: 0.9936\n",
      "Epoch 6/50\n",
      "4071/4071 [==============================] - 1s 157us/step - loss: 0.0237 - acc: 0.9931 - val_loss: 0.0231 - val_acc: 0.9934\n",
      "Epoch 7/50\n",
      "4071/4071 [==============================] - 1s 157us/step - loss: 0.0234 - acc: 0.9931 - val_loss: 0.0231 - val_acc: 0.9941\n",
      "Epoch 8/50\n",
      "4071/4071 [==============================] - 1s 157us/step - loss: 0.0236 - acc: 0.9944 - val_loss: 0.0230 - val_acc: 0.9934\n",
      "Epoch 9/50\n",
      "4071/4071 [==============================] - 1s 157us/step - loss: 0.0234 - acc: 0.9936 - val_loss: 0.0230 - val_acc: 0.9934\n",
      "Epoch 10/50\n",
      "4071/4071 [==============================] - 1s 154us/step - loss: 0.0234 - acc: 0.9936 - val_loss: 0.0230 - val_acc: 0.9931\n",
      "Epoch 11/50\n",
      "4071/4071 [==============================] - 1s 153us/step - loss: 0.0234 - acc: 0.9934 - val_loss: 0.0229 - val_acc: 0.9936\n",
      "Epoch 12/50\n",
      "4071/4071 [==============================] - 1s 152us/step - loss: 0.0233 - acc: 0.9934 - val_loss: 0.0228 - val_acc: 0.9939\n",
      "Epoch 13/50\n",
      "4071/4071 [==============================] - 1s 162us/step - loss: 0.0233 - acc: 0.9939 - val_loss: 0.0229 - val_acc: 0.9939\n",
      "Epoch 14/50\n",
      "4071/4071 [==============================] - 1s 177us/step - loss: 0.0233 - acc: 0.9934 - val_loss: 0.0228 - val_acc: 0.9939\n",
      "Epoch 15/50\n",
      "4071/4071 [==============================] - 1s 155us/step - loss: 0.0233 - acc: 0.9936 - val_loss: 0.0228 - val_acc: 0.9936\n",
      "Epoch 16/50\n",
      "4071/4071 [==============================] - 1s 158us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0229 - val_acc: 0.9934\n",
      "Epoch 17/50\n",
      "4071/4071 [==============================] - 1s 154us/step - loss: 0.0232 - acc: 0.9934 - val_loss: 0.0227 - val_acc: 0.9939\n",
      "Epoch 18/50\n",
      "4071/4071 [==============================] - 1s 155us/step - loss: 0.0230 - acc: 0.9939 - val_loss: 0.0227 - val_acc: 0.9936\n",
      "Epoch 19/50\n",
      "4071/4071 [==============================] - 1s 158us/step - loss: 0.0231 - acc: 0.9934 - val_loss: 0.0227 - val_acc: 0.9936\n",
      "Epoch 20/50\n",
      "4071/4071 [==============================] - 1s 155us/step - loss: 0.0231 - acc: 0.9934 - val_loss: 0.0226 - val_acc: 0.9939\n",
      "Epoch 21/50\n",
      "4071/4071 [==============================] - 1s 155us/step - loss: 0.0231 - acc: 0.9939 - val_loss: 0.0226 - val_acc: 0.9939\n",
      "Epoch 22/50\n",
      "4071/4071 [==============================] - 1s 157us/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.0226 - val_acc: 0.9939\n",
      "Epoch 23/50\n",
      "4071/4071 [==============================] - 1s 160us/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.0225 - val_acc: 0.9939\n",
      "Epoch 24/50\n",
      "4071/4071 [==============================] - 1s 156us/step - loss: 0.0229 - acc: 0.9934 - val_loss: 0.0226 - val_acc: 0.9936\n",
      "Epoch 25/50\n",
      "4071/4071 [==============================] - 1s 158us/step - loss: 0.0230 - acc: 0.9939 - val_loss: 0.0225 - val_acc: 0.9939\n",
      "Epoch 26/50\n",
      "4071/4071 [==============================] - 1s 154us/step - loss: 0.0230 - acc: 0.9941 - val_loss: 0.0225 - val_acc: 0.9939\n",
      "Epoch 27/50\n",
      "4071/4071 [==============================] - 1s 159us/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.0225 - val_acc: 0.9936\n",
      "Epoch 28/50\n",
      "4071/4071 [==============================] - 1s 198us/step - loss: 0.0229 - acc: 0.9939 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 29/50\n",
      "4071/4071 [==============================] - 1s 183us/step - loss: 0.0229 - acc: 0.9939 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 30/50\n",
      "4071/4071 [==============================] - 1s 218us/step - loss: 0.0228 - acc: 0.9936 - val_loss: 0.0225 - val_acc: 0.9931\n",
      "Epoch 31/50\n",
      "4071/4071 [==============================] - 1s 183us/step - loss: 0.0229 - acc: 0.9931 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 32/50\n",
      "4071/4071 [==============================] - 1s 251us/step - loss: 0.0228 - acc: 0.9936 - val_loss: 0.0224 - val_acc: 0.9936\n",
      "Epoch 33/50\n",
      "4071/4071 [==============================] - 1s 283us/step - loss: 0.0228 - acc: 0.9936 - val_loss: 0.0225 - val_acc: 0.9936\n",
      "Epoch 34/50\n",
      "4071/4071 [==============================] - 2s 405us/step - loss: 0.0229 - acc: 0.9934 - val_loss: 0.0223 - val_acc: 0.9939\n",
      "Epoch 35/50\n",
      "4071/4071 [==============================] - 1s 326us/step - loss: 0.0227 - acc: 0.9934 - val_loss: 0.0223 - val_acc: 0.9936\n",
      "Epoch 36/50\n",
      "4071/4071 [==============================] - 1s 254us/step - loss: 0.0228 - acc: 0.9936 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4071/4071 [==============================] - 1s 169us/step - loss: 0.0228 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9941\n",
      "Epoch 38/50\n",
      "4071/4071 [==============================] - 1s 169us/step - loss: 0.0227 - acc: 0.9936 - val_loss: 0.0223 - val_acc: 0.9939\n",
      "Epoch 39/50\n",
      "4071/4071 [==============================] - 1s 162us/step - loss: 0.0227 - acc: 0.9934 - val_loss: 0.0223 - val_acc: 0.9941\n",
      "Epoch 40/50\n",
      "4071/4071 [==============================] - 1s 163us/step - loss: 0.0227 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9934\n",
      "Epoch 41/50\n",
      "4071/4071 [==============================] - 1s 154us/step - loss: 0.0227 - acc: 0.9936 - val_loss: 0.0222 - val_acc: 0.9934\n",
      "Epoch 42/50\n",
      "4071/4071 [==============================] - 1s 252us/step - loss: 0.0226 - acc: 0.9931 - val_loss: 0.0222 - val_acc: 0.9934\n",
      "Epoch 43/50\n",
      "4071/4071 [==============================] - 1s 260us/step - loss: 0.0226 - acc: 0.9934 - val_loss: 0.0222 - val_acc: 0.9939\n",
      "Epoch 44/50\n",
      "4071/4071 [==============================] - 1s 219us/step - loss: 0.0226 - acc: 0.9936 - val_loss: 0.0222 - val_acc: 0.9936\n",
      "Epoch 45/50\n",
      "4071/4071 [==============================] - 1s 274us/step - loss: 0.0226 - acc: 0.9936 - val_loss: 0.0222 - val_acc: 0.9936\n",
      "Epoch 46/50\n",
      "4071/4071 [==============================] - 1s 208us/step - loss: 0.0226 - acc: 0.9939 - val_loss: 0.0221 - val_acc: 0.9936\n",
      "Epoch 47/50\n",
      "4071/4071 [==============================] - 1s 206us/step - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "Epoch 48/50\n",
      "4071/4071 [==============================] - 1s 165us/step - loss: 0.0225 - acc: 0.9939 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "Epoch 49/50\n",
      "4071/4071 [==============================] - 1s 152us/step - loss: 0.0224 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "Epoch 50/50\n",
      "4071/4071 [==============================] - 1s 152us/step - loss: 0.0224 - acc: 0.9936 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "4072/4072 [==============================] - 0s 64us/step\n",
      "Train on 4071 samples, validate on 4071 samples\n",
      "Epoch 1/50\n",
      "4071/4071 [==============================] - 1s 164us/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.0238 - val_acc: 0.9919\n",
      "Epoch 2/50\n",
      "4071/4071 [==============================] - 1s 164us/step - loss: 0.0242 - acc: 0.9919 - val_loss: 0.0236 - val_acc: 0.9934\n",
      "Epoch 3/50\n",
      "4071/4071 [==============================] - 1s 150us/step - loss: 0.0239 - acc: 0.9924 - val_loss: 0.0235 - val_acc: 0.9921\n",
      "Epoch 4/50\n",
      "4071/4071 [==============================] - 1s 153us/step - loss: 0.0238 - acc: 0.9924 - val_loss: 0.0233 - val_acc: 0.9924\n",
      "Epoch 5/50\n",
      "4071/4071 [==============================] - 1s 168us/step - loss: 0.0237 - acc: 0.9916 - val_loss: 0.0232 - val_acc: 0.9924\n",
      "Epoch 6/50\n",
      "4071/4071 [==============================] - 1s 215us/step - loss: 0.0235 - acc: 0.9924 - val_loss: 0.0232 - val_acc: 0.9924\n",
      "Epoch 7/50\n",
      "4071/4071 [==============================] - 1s 193us/step - loss: 0.0236 - acc: 0.9921 - val_loss: 0.0232 - val_acc: 0.9924\n",
      "Epoch 8/50\n",
      "4071/4071 [==============================] - 1s 214us/step - loss: 0.0235 - acc: 0.9919 - val_loss: 0.0231 - val_acc: 0.9929\n",
      "Epoch 9/50\n",
      "4071/4071 [==============================] - 1s 193us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0230 - val_acc: 0.9926\n",
      "Epoch 10/50\n",
      "4071/4071 [==============================] - 1s 168us/step - loss: 0.0235 - acc: 0.9926 - val_loss: 0.0230 - val_acc: 0.9929\n",
      "Epoch 11/50\n",
      "4071/4071 [==============================] - 1s 167us/step - loss: 0.0233 - acc: 0.9924 - val_loss: 0.0229 - val_acc: 0.9924\n",
      "Epoch 12/50\n",
      "4071/4071 [==============================] - 1s 179us/step - loss: 0.0232 - acc: 0.9919 - val_loss: 0.0229 - val_acc: 0.9934\n",
      "Epoch 13/50\n",
      "4071/4071 [==============================] - 1s 257us/step - loss: 0.0233 - acc: 0.9921 - val_loss: 0.0229 - val_acc: 0.9926\n",
      "Epoch 14/50\n",
      "4071/4071 [==============================] - 1s 311us/step - loss: 0.0232 - acc: 0.9921 - val_loss: 0.0228 - val_acc: 0.9924\n",
      "Epoch 15/50\n",
      "4071/4071 [==============================] - 1s 266us/step - loss: 0.0232 - acc: 0.9919 - val_loss: 0.0227 - val_acc: 0.9921\n",
      "Epoch 16/50\n",
      "4071/4071 [==============================] - 1s 212us/step - loss: 0.0231 - acc: 0.9924 - val_loss: 0.0227 - val_acc: 0.9924\n",
      "Epoch 17/50\n",
      "4071/4071 [==============================] - 1s 211us/step - loss: 0.0230 - acc: 0.9919 - val_loss: 0.0227 - val_acc: 0.9921\n",
      "Epoch 18/50\n",
      "4071/4071 [==============================] - 1s 201us/step - loss: 0.0231 - acc: 0.9916 - val_loss: 0.0226 - val_acc: 0.9926\n",
      "Epoch 19/50\n",
      "4071/4071 [==============================] - 1s 257us/step - loss: 0.0229 - acc: 0.9921 - val_loss: 0.0226 - val_acc: 0.9921\n",
      "Epoch 20/50\n",
      "4071/4071 [==============================] - 1s 212us/step - loss: 0.0230 - acc: 0.9924 - val_loss: 0.0226 - val_acc: 0.9924\n",
      "Epoch 21/50\n",
      "4071/4071 [==============================] - 1s 253us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0227 - val_acc: 0.9924\n",
      "Epoch 22/50\n",
      "4071/4071 [==============================] - 1s 283us/step - loss: 0.0230 - acc: 0.9916 - val_loss: 0.0226 - val_acc: 0.9924\n",
      "Epoch 23/50\n",
      "4071/4071 [==============================] - 1s 248us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0225 - val_acc: 0.9924\n",
      "Epoch 24/50\n",
      "4071/4071 [==============================] - 1s 256us/step - loss: 0.0229 - acc: 0.9919 - val_loss: 0.0226 - val_acc: 0.9929\n",
      "Epoch 25/50\n",
      "4071/4071 [==============================] - 1s 227us/step - loss: 0.0229 - acc: 0.9924 - val_loss: 0.0225 - val_acc: 0.9926\n",
      "Epoch 26/50\n",
      "4071/4071 [==============================] - 1s 162us/step - loss: 0.0229 - acc: 0.9924 - val_loss: 0.0225 - val_acc: 0.9919\n",
      "Epoch 27/50\n",
      "4071/4071 [==============================] - 1s 206us/step - loss: 0.0229 - acc: 0.9916 - val_loss: 0.0224 - val_acc: 0.9919\n",
      "Epoch 28/50\n",
      "4071/4071 [==============================] - 1s 277us/step - loss: 0.0227 - acc: 0.9921 - val_loss: 0.0224 - val_acc: 0.9919\n",
      "Epoch 29/50\n",
      "4071/4071 [==============================] - 1s 261us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0224 - val_acc: 0.9919\n",
      "Epoch 30/50\n",
      "4071/4071 [==============================] - 1s 304us/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.0223 - val_acc: 0.9919\n",
      "Epoch 31/50\n",
      "4071/4071 [==============================] - 1s 202us/step - loss: 0.0226 - acc: 0.9919 - val_loss: 0.0223 - val_acc: 0.9919\n",
      "Epoch 32/50\n",
      "4071/4071 [==============================] - 1s 200us/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.0223 - val_acc: 0.9919\n",
      "Epoch 33/50\n",
      "4071/4071 [==============================] - 1s 181us/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.0223 - val_acc: 0.9921\n",
      "Epoch 34/50\n",
      "4071/4071 [==============================] - 1s 146us/step - loss: 0.0227 - acc: 0.9921 - val_loss: 0.0223 - val_acc: 0.9926\n",
      "Epoch 35/50\n",
      "4071/4071 [==============================] - 1s 150us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0223 - val_acc: 0.9921\n",
      "Epoch 36/50\n",
      "4071/4071 [==============================] - 1s 164us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0223 - val_acc: 0.9921\n",
      "Epoch 37/50\n",
      "4071/4071 [==============================] - 1s 161us/step - loss: 0.0226 - acc: 0.9921 - val_loss: 0.0223 - val_acc: 0.9921\n",
      "Epoch 38/50\n",
      "4071/4071 [==============================] - 1s 147us/step - loss: 0.0226 - acc: 0.9924 - val_loss: 0.0223 - val_acc: 0.9919\n",
      "Epoch 39/50\n",
      "4071/4071 [==============================] - 1s 151us/step - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0224 - val_acc: 0.9919\n",
      "Epoch 40/50\n",
      "4071/4071 [==============================] - 1s 152us/step - loss: 0.0229 - acc: 0.9921 - val_loss: 0.0223 - val_acc: 0.9931\n",
      "Epoch 41/50\n",
      "4071/4071 [==============================] - 1s 148us/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.0222 - val_acc: 0.9926\n",
      "Epoch 42/50\n",
      "4071/4071 [==============================] - 1s 151us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0223 - val_acc: 0.9936\n",
      "Epoch 43/50\n",
      "4071/4071 [==============================] - 1s 160us/step - loss: 0.0227 - acc: 0.9921 - val_loss: 0.0222 - val_acc: 0.9921\n",
      "Epoch 44/50\n",
      "4071/4071 [==============================] - 1s 161us/step - loss: 0.0226 - acc: 0.9924 - val_loss: 0.0222 - val_acc: 0.9921\n",
      "Epoch 45/50\n",
      "4071/4071 [==============================] - 1s 190us/step - loss: 0.0226 - acc: 0.9924 - val_loss: 0.0222 - val_acc: 0.9931\n",
      "Epoch 46/50\n",
      "4071/4071 [==============================] - 1s 181us/step - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0221 - val_acc: 0.9921\n",
      "Epoch 47/50\n",
      "4071/4071 [==============================] - 1s 204us/step - loss: 0.0226 - acc: 0.9916 - val_loss: 0.0222 - val_acc: 0.9921\n",
      "Epoch 48/50\n",
      "4071/4071 [==============================] - 1s 191us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0222 - val_acc: 0.9921\n",
      "Epoch 49/50\n",
      "4071/4071 [==============================] - 1s 167us/step - loss: 0.0225 - acc: 0.9919 - val_loss: 0.0221 - val_acc: 0.9924\n",
      "Epoch 50/50\n",
      "4071/4071 [==============================] - 1s 145us/step - loss: 0.0224 - acc: 0.9921 - val_loss: 0.0222 - val_acc: 0.9929\n",
      "4072/4072 [==============================] - 0s 99us/step\n",
      "Train on 1628 samples, validate on 1628 samples\n",
      "Epoch 1/50\n",
      "1628/1628 [==============================] - 0s 146us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0172 - val_acc: 0.9945\n",
      "Epoch 2/50\n",
      "1628/1628 [==============================] - 0s 187us/step - loss: 0.0169 - acc: 0.9951 - val_loss: 0.0163 - val_acc: 0.9951\n",
      "Epoch 3/50\n",
      "1628/1628 [==============================] - 0s 209us/step - loss: 0.0163 - acc: 0.9951 - val_loss: 0.0157 - val_acc: 0.9957\n",
      "Epoch 4/50\n",
      "1628/1628 [==============================] - 0s 193us/step - loss: 0.0158 - acc: 0.9957 - val_loss: 0.0155 - val_acc: 0.9957\n",
      "Epoch 5/50\n",
      "1628/1628 [==============================] - 0s 195us/step - loss: 0.0155 - acc: 0.9957 - val_loss: 0.0152 - val_acc: 0.9957\n",
      "Epoch 6/50\n",
      "1628/1628 [==============================] - 0s 214us/step - loss: 0.0153 - acc: 0.9957 - val_loss: 0.0150 - val_acc: 0.9957\n",
      "Epoch 7/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0149 - val_acc: 0.9957\n",
      "Epoch 8/50\n",
      "1628/1628 [==============================] - 0s 166us/step - loss: 0.0152 - acc: 0.9957 - val_loss: 0.0148 - val_acc: 0.9957\n",
      "Epoch 9/50\n",
      "1628/1628 [==============================] - 0s 265us/step - loss: 0.0149 - acc: 0.9957 - val_loss: 0.0147 - val_acc: 0.9957\n",
      "Epoch 10/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0148 - acc: 0.9957 - val_loss: 0.0146 - val_acc: 0.9951\n",
      "Epoch 11/50\n",
      "1628/1628 [==============================] - 0s 187us/step - loss: 0.0148 - acc: 0.9951 - val_loss: 0.0145 - val_acc: 0.9951\n",
      "Epoch 12/50\n",
      "1628/1628 [==============================] - 0s 255us/step - loss: 0.0147 - acc: 0.9951 - val_loss: 0.0145 - val_acc: 0.9951\n",
      "Epoch 13/50\n",
      "1628/1628 [==============================] - 0s 156us/step - loss: 0.0146 - acc: 0.9951 - val_loss: 0.0144 - val_acc: 0.9951\n",
      "Epoch 14/50\n",
      "1628/1628 [==============================] - 0s 235us/step - loss: 0.0145 - acc: 0.9951 - val_loss: 0.0143 - val_acc: 0.9951\n",
      "Epoch 15/50\n",
      "1628/1628 [==============================] - 0s 157us/step - loss: 0.0146 - acc: 0.9951 - val_loss: 0.0143 - val_acc: 0.9951\n",
      "Epoch 16/50\n",
      "1628/1628 [==============================] - 0s 241us/step - loss: 0.0147 - acc: 0.9945 - val_loss: 0.0143 - val_acc: 0.9951\n",
      "Epoch 17/50\n",
      "1628/1628 [==============================] - 0s 159us/step - loss: 0.0146 - acc: 0.9939 - val_loss: 0.0143 - val_acc: 0.9951\n",
      "Epoch 18/50\n",
      "1628/1628 [==============================] - 0s 202us/step - loss: 0.0144 - acc: 0.9945 - val_loss: 0.0142 - val_acc: 0.9945\n",
      "Epoch 19/50\n",
      "1628/1628 [==============================] - 0s 182us/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0142 - val_acc: 0.9951\n",
      "Epoch 20/50\n",
      "1628/1628 [==============================] - 0s 156us/step - loss: 0.0146 - acc: 0.9926 - val_loss: 0.0141 - val_acc: 0.9945\n",
      "Epoch 21/50\n",
      "1628/1628 [==============================] - 0s 208us/step - loss: 0.0144 - acc: 0.9945 - val_loss: 0.0141 - val_acc: 0.9951\n",
      "Epoch 22/50\n",
      "1628/1628 [==============================] - 0s 150us/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0141 - val_acc: 0.9951\n",
      "Epoch 23/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0141 - acc: 0.9951 - val_loss: 0.0140 - val_acc: 0.9951\n",
      "Epoch 24/50\n",
      "1628/1628 [==============================] - 0s 172us/step - loss: 0.0143 - acc: 0.9945 - val_loss: 0.0140 - val_acc: 0.9939\n",
      "Epoch 25/50\n",
      "1628/1628 [==============================] - 0s 166us/step - loss: 0.0141 - acc: 0.9939 - val_loss: 0.0140 - val_acc: 0.9939\n",
      "Epoch 26/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0142 - acc: 0.9939 - val_loss: 0.0140 - val_acc: 0.9945\n",
      "Epoch 27/50\n",
      "1628/1628 [==============================] - 0s 154us/step - loss: 0.0142 - acc: 0.9932 - val_loss: 0.0139 - val_acc: 0.9932\n",
      "Epoch 28/50\n",
      "1628/1628 [==============================] - 0s 161us/step - loss: 0.0142 - acc: 0.9945 - val_loss: 0.0139 - val_acc: 0.9939\n",
      "Epoch 29/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0142 - acc: 0.9926 - val_loss: 0.0139 - val_acc: 0.9939\n",
      "Epoch 30/50\n",
      "1628/1628 [==============================] - 0s 163us/step - loss: 0.0141 - acc: 0.9939 - val_loss: 0.0139 - val_acc: 0.9939\n",
      "Epoch 31/50\n",
      "1628/1628 [==============================] - 0s 153us/step - loss: 0.0141 - acc: 0.9932 - val_loss: 0.0138 - val_acc: 0.9932\n",
      "Epoch 32/50\n",
      "1628/1628 [==============================] - 0s 160us/step - loss: 0.0141 - acc: 0.9945 - val_loss: 0.0138 - val_acc: 0.9945\n",
      "Epoch 33/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0139 - acc: 0.9945 - val_loss: 0.0138 - val_acc: 0.9939\n",
      "Epoch 34/50\n",
      "1628/1628 [==============================] - ETA: 0s - loss: 0.0150 - acc: 0.991 - 0s 160us/step - loss: 0.0140 - acc: 0.9926 - val_loss: 0.0138 - val_acc: 0.9926\n",
      "Epoch 35/50\n",
      "1628/1628 [==============================] - 0s 168us/step - loss: 0.0141 - acc: 0.9945 - val_loss: 0.0138 - val_acc: 0.9939\n",
      "Epoch 36/50\n",
      "1628/1628 [==============================] - 0s 165us/step - loss: 0.0140 - acc: 0.9926 - val_loss: 0.0137 - val_acc: 0.9932\n",
      "Epoch 37/50\n",
      "1628/1628 [==============================] - 0s 167us/step - loss: 0.0140 - acc: 0.9932 - val_loss: 0.0137 - val_acc: 0.9926\n",
      "Epoch 38/50\n",
      "1628/1628 [==============================] - 0s 166us/step - loss: 0.0139 - acc: 0.9926 - val_loss: 0.0137 - val_acc: 0.9926\n",
      "Epoch 39/50\n",
      "1628/1628 [==============================] - 0s 171us/step - loss: 0.0139 - acc: 0.9932 - val_loss: 0.0136 - val_acc: 0.9926\n",
      "Epoch 40/50\n",
      "1628/1628 [==============================] - 0s 175us/step - loss: 0.0139 - acc: 0.9939 - val_loss: 0.0137 - val_acc: 0.9945\n",
      "Epoch 41/50\n",
      "1628/1628 [==============================] - 0s 226us/step - loss: 0.0137 - acc: 0.9939 - val_loss: 0.0136 - val_acc: 0.9926\n",
      "Epoch 42/50\n",
      "1628/1628 [==============================] - 0s 173us/step - loss: 0.0138 - acc: 0.9926 - val_loss: 0.0136 - val_acc: 0.9926\n",
      "Epoch 43/50\n",
      "1628/1628 [==============================] - 0s 160us/step - loss: 0.0139 - acc: 0.9932 - val_loss: 0.0136 - val_acc: 0.9945\n",
      "Epoch 44/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0138 - acc: 0.9926 - val_loss: 0.0136 - val_acc: 0.9926\n",
      "Epoch 45/50\n",
      "1628/1628 [==============================] - 0s 164us/step - loss: 0.0138 - acc: 0.9926 - val_loss: 0.0136 - val_acc: 0.9926\n",
      "Epoch 46/50\n",
      "1628/1628 [==============================] - 0s 211us/step - loss: 0.0139 - acc: 0.9939 - val_loss: 0.0135 - val_acc: 0.9939\n",
      "Epoch 47/50\n",
      "1628/1628 [==============================] - 0s 199us/step - loss: 0.0140 - acc: 0.9926 - val_loss: 0.0136 - val_acc: 0.9926\n",
      "Epoch 48/50\n",
      "1628/1628 [==============================] - 0s 165us/step - loss: 0.0138 - acc: 0.9939 - val_loss: 0.0135 - val_acc: 0.9926\n",
      "Epoch 49/50\n",
      "1628/1628 [==============================] - 0s 159us/step - loss: 0.0137 - acc: 0.9926 - val_loss: 0.0135 - val_acc: 0.9926\n",
      "Epoch 50/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0137 - acc: 0.9939 - val_loss: 0.0135 - val_acc: 0.9932\n",
      "6515/6515 [==============================] - 0s 65us/step\n",
      "Train on 1628 samples, validate on 1628 samples\n",
      "Epoch 1/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0264 - acc: 0.9926 - val_loss: 0.0245 - val_acc: 0.9932\n",
      "Epoch 2/50\n",
      "1628/1628 [==============================] - 0s 157us/step - loss: 0.0249 - acc: 0.9932 - val_loss: 0.0241 - val_acc: 0.9932\n",
      "Epoch 3/50\n",
      "1628/1628 [==============================] - 0s 159us/step - loss: 0.0246 - acc: 0.9932 - val_loss: 0.0240 - val_acc: 0.9932\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1628/1628 [==============================] - 0s 159us/step - loss: 0.0241 - acc: 0.9932 - val_loss: 0.0238 - val_acc: 0.9932\n",
      "Epoch 5/50\n",
      "1628/1628 [==============================] - 0s 143us/step - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0236 - val_acc: 0.9932\n",
      "Epoch 6/50\n",
      "1628/1628 [==============================] - 0s 145us/step - loss: 0.0243 - acc: 0.9932 - val_loss: 0.0235 - val_acc: 0.9932\n",
      "Epoch 7/50\n",
      "1628/1628 [==============================] - 0s 140us/step - loss: 0.0239 - acc: 0.9926 - val_loss: 0.0234 - val_acc: 0.9932\n",
      "Epoch 8/50\n",
      "1628/1628 [==============================] - 0s 139us/step - loss: 0.0237 - acc: 0.9932 - val_loss: 0.0233 - val_acc: 0.9932\n",
      "Epoch 9/50\n",
      "1628/1628 [==============================] - 0s 140us/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0232 - val_acc: 0.9932\n",
      "Epoch 10/50\n",
      "1628/1628 [==============================] - 0s 148us/step - loss: 0.0237 - acc: 0.9932 - val_loss: 0.0231 - val_acc: 0.9932\n",
      "Epoch 11/50\n",
      "1628/1628 [==============================] - 0s 142us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0231 - val_acc: 0.9932\n",
      "Epoch 12/50\n",
      "1628/1628 [==============================] - 0s 140us/step - loss: 0.0234 - acc: 0.9932 - val_loss: 0.0230 - val_acc: 0.9926\n",
      "Epoch 13/50\n",
      "1628/1628 [==============================] - 0s 139us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0230 - val_acc: 0.9932\n",
      "Epoch 14/50\n",
      "1628/1628 [==============================] - 0s 141us/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.0229 - val_acc: 0.9932\n",
      "Epoch 15/50\n",
      "1628/1628 [==============================] - 0s 149us/step - loss: 0.0234 - acc: 0.9926 - val_loss: 0.0229 - val_acc: 0.9926\n",
      "Epoch 16/50\n",
      "1628/1628 [==============================] - 0s 153us/step - loss: 0.0231 - acc: 0.9932 - val_loss: 0.0228 - val_acc: 0.9932\n",
      "Epoch 17/50\n",
      "1628/1628 [==============================] - 0s 154us/step - loss: 0.0232 - acc: 0.9932 - val_loss: 0.0227 - val_acc: 0.9926\n",
      "Epoch 18/50\n",
      "1628/1628 [==============================] - 0s 142us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0227 - val_acc: 0.9932\n",
      "Epoch 19/50\n",
      "1628/1628 [==============================] - 0s 143us/step - loss: 0.0230 - acc: 0.9926 - val_loss: 0.0226 - val_acc: 0.9932\n",
      "Epoch 20/50\n",
      "1628/1628 [==============================] - 0s 144us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0226 - val_acc: 0.9932\n",
      "Epoch 21/50\n",
      "1628/1628 [==============================] - 0s 166us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0226 - val_acc: 0.9932\n",
      "Epoch 22/50\n",
      "1628/1628 [==============================] - 0s 148us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0225 - val_acc: 0.9926\n",
      "Epoch 23/50\n",
      "1628/1628 [==============================] - 0s 149us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0225 - val_acc: 0.9932\n",
      "Epoch 24/50\n",
      "1628/1628 [==============================] - 0s 144us/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.0225 - val_acc: 0.9932\n",
      "Epoch 25/50\n",
      "1628/1628 [==============================] - 0s 143us/step - loss: 0.0228 - acc: 0.9932 - val_loss: 0.0224 - val_acc: 0.9932\n",
      "Epoch 26/50\n",
      "1628/1628 [==============================] - 0s 168us/step - loss: 0.0228 - acc: 0.9932 - val_loss: 0.0224 - val_acc: 0.9932\n",
      "Epoch 27/50\n",
      "1628/1628 [==============================] - 0s 194us/step - loss: 0.0230 - acc: 0.9932 - val_loss: 0.0223 - val_acc: 0.9932\n",
      "Epoch 28/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0223 - val_acc: 0.9926\n",
      "Epoch 29/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0227 - acc: 0.9926 - val_loss: 0.0223 - val_acc: 0.9932\n",
      "Epoch 30/50\n",
      "1628/1628 [==============================] - 0s 156us/step - loss: 0.0226 - acc: 0.9932 - val_loss: 0.0223 - val_acc: 0.9932\n",
      "Epoch 31/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0227 - acc: 0.9932 - val_loss: 0.0222 - val_acc: 0.9939\n",
      "Epoch 32/50\n",
      "1628/1628 [==============================] - 0s 150us/step - loss: 0.0225 - acc: 0.9926 - val_loss: 0.0222 - val_acc: 0.9926\n",
      "Epoch 33/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0225 - acc: 0.9932 - val_loss: 0.0222 - val_acc: 0.9932\n",
      "Epoch 34/50\n",
      "1628/1628 [==============================] - 0s 162us/step - loss: 0.0224 - acc: 0.9932 - val_loss: 0.0222 - val_acc: 0.9932\n",
      "Epoch 35/50\n",
      "1628/1628 [==============================] - 0s 174us/step - loss: 0.0224 - acc: 0.9926 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "Epoch 36/50\n",
      "1628/1628 [==============================] - 0s 154us/step - loss: 0.0225 - acc: 0.9939 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "Epoch 37/50\n",
      "1628/1628 [==============================] - 0s 161us/step - loss: 0.0225 - acc: 0.9926 - val_loss: 0.0221 - val_acc: 0.9932\n",
      "Epoch 38/50\n",
      "1628/1628 [==============================] - 0s 158us/step - loss: 0.0226 - acc: 0.9926 - val_loss: 0.0220 - val_acc: 0.9932\n",
      "Epoch 39/50\n",
      "1628/1628 [==============================] - 0s 152us/step - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0220 - val_acc: 0.9939\n",
      "Epoch 40/50\n",
      "1628/1628 [==============================] - 0s 155us/step - loss: 0.0224 - acc: 0.9932 - val_loss: 0.0220 - val_acc: 0.9939\n",
      "Epoch 41/50\n",
      "1628/1628 [==============================] - 0s 160us/step - loss: 0.0222 - acc: 0.9932 - val_loss: 0.0220 - val_acc: 0.9939\n",
      "Epoch 42/50\n",
      "1628/1628 [==============================] - 0s 165us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0219 - val_acc: 0.9939\n",
      "Epoch 43/50\n",
      "1628/1628 [==============================] - 0s 190us/step - loss: 0.0224 - acc: 0.9939 - val_loss: 0.0219 - val_acc: 0.9932\n",
      "Epoch 44/50\n",
      "1628/1628 [==============================] - 0s 169us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0219 - val_acc: 0.9932\n",
      "Epoch 45/50\n",
      "1628/1628 [==============================] - 0s 160us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0219 - val_acc: 0.9939\n",
      "Epoch 46/50\n",
      "1628/1628 [==============================] - 0s 151us/step - loss: 0.0222 - acc: 0.9932 - val_loss: 0.0218 - val_acc: 0.9939\n",
      "Epoch 47/50\n",
      "1628/1628 [==============================] - 0s 157us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0218 - val_acc: 0.9939\n",
      "Epoch 48/50\n",
      "1628/1628 [==============================] - 0s 143us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0218 - val_acc: 0.9939\n",
      "Epoch 49/50\n",
      "1628/1628 [==============================] - 0s 151us/step - loss: 0.0222 - acc: 0.9939 - val_loss: 0.0217 - val_acc: 0.9939\n",
      "Epoch 50/50\n",
      "1628/1628 [==============================] - 0s 153us/step - loss: 0.0221 - acc: 0.9939 - val_loss: 0.0216 - val_acc: 0.9939\n",
      "6515/6515 [==============================] - 0s 62us/step\n",
      "Train on 1628 samples, validate on 1628 samples\n",
      "Epoch 1/50\n",
      "1628/1628 [==============================] - 0s 143us/step - loss: 0.0245 - acc: 0.9932 - val_loss: 0.0234 - val_acc: 0.9939\n",
      "Epoch 2/50\n",
      "1628/1628 [==============================] - 0s 192us/step - loss: 0.0237 - acc: 0.9939 - val_loss: 0.0229 - val_acc: 0.9939\n",
      "Epoch 3/50\n",
      "1628/1628 [==============================] - 0s 292us/step - loss: 0.0232 - acc: 0.9939 - val_loss: 0.0226 - val_acc: 0.9939\n",
      "Epoch 4/50\n",
      "1628/1628 [==============================] - 0s 163us/step - loss: 0.0231 - acc: 0.9939 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 5/50\n",
      "1628/1628 [==============================] - 6720s 4s/step - loss: 0.0228 - acc: 0.9939 - val_loss: 0.0223 - val_acc: 0.9939\n",
      "Epoch 6/50\n",
      "1628/1628 [==============================] - 1s 630us/step - loss: 0.0228 - acc: 0.9939 - val_loss: 0.0222 - val_acc: 0.9945\n",
      "Epoch 7/50\n",
      "1628/1628 [==============================] - 1s 570us/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.0221 - val_acc: 0.9945\n",
      "Epoch 8/50\n",
      "1628/1628 [==============================] - 0s 301us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.0220 - val_acc: 0.9945\n",
      "Epoch 9/50\n",
      "1628/1628 [==============================] - 0s 188us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0220 - val_acc: 0.9945\n",
      "Epoch 10/50\n",
      "1628/1628 [==============================] - 0s 163us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.0219 - val_acc: 0.9945\n",
      "Epoch 11/50\n",
      "1628/1628 [==============================] - 0s 225us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0219 - val_acc: 0.9945\n",
      "Epoch 12/50\n",
      "1628/1628 [==============================] - 0s 156us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0218 - val_acc: 0.9945\n",
      "Epoch 13/50\n",
      "1628/1628 [==============================] - 0s 239us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0218 - val_acc: 0.9945\n",
      "Epoch 14/50\n",
      "1628/1628 [==============================] - 0s 198us/step - loss: 0.0221 - acc: 0.9945 - val_loss: 0.0217 - val_acc: 0.9945\n",
      "Epoch 15/50\n",
      "1628/1628 [==============================] - 0s 197us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.0217 - val_acc: 0.9945\n",
      "Epoch 16/50\n",
      "1628/1628 [==============================] - 0s 222us/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0216 - val_acc: 0.9945\n",
      "Epoch 17/50\n",
      "1628/1628 [==============================] - 0s 170us/step - loss: 0.0221 - acc: 0.9945 - val_loss: 0.0216 - val_acc: 0.9945\n",
      "Epoch 18/50\n",
      "1628/1628 [==============================] - 0s 237us/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.0215 - val_acc: 0.9945\n",
      "Epoch 19/50\n",
      "1628/1628 [==============================] - 0s 236us/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0215 - val_acc: 0.9945\n",
      "Epoch 20/50\n",
      "1628/1628 [==============================] - 0s 235us/step - loss: 0.0218 - acc: 0.9939 - val_loss: 0.0215 - val_acc: 0.9945\n",
      "Epoch 21/50\n",
      "1628/1628 [==============================] - 0s 250us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0214 - val_acc: 0.9945\n",
      "Epoch 22/50\n",
      "1628/1628 [==============================] - 0s 207us/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.0214 - val_acc: 0.9945\n",
      "Epoch 23/50\n",
      "1628/1628 [==============================] - 1s 480us/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0214 - val_acc: 0.9939\n",
      "Epoch 24/50\n",
      "1628/1628 [==============================] - 0s 210us/step - loss: 0.0218 - acc: 0.9945 - val_loss: 0.0213 - val_acc: 0.9945\n",
      "Epoch 25/50\n",
      "1628/1628 [==============================] - 0s 175us/step - loss: 0.0218 - acc: 0.9945 - val_loss: 0.0213 - val_acc: 0.9945\n",
      "Epoch 26/50\n",
      "1628/1628 [==============================] - 0s 181us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0212 - val_acc: 0.9945\n",
      "Epoch 27/50\n",
      "1628/1628 [==============================] - 0s 140us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.0212 - val_acc: 0.9945\n",
      "Epoch 28/50\n",
      "1628/1628 [==============================] - 0s 177us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.0212 - val_acc: 0.9945\n",
      "Epoch 29/50\n",
      "1628/1628 [==============================] - 0s 213us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0211 - val_acc: 0.9945\n",
      "Epoch 30/50\n",
      "1628/1628 [==============================] - 0s 198us/step - loss: 0.0215 - acc: 0.9945 - val_loss: 0.0211 - val_acc: 0.9945\n",
      "Epoch 31/50\n",
      "1628/1628 [==============================] - 0s 149us/step - loss: 0.0216 - acc: 0.9939 - val_loss: 0.0211 - val_acc: 0.9945\n",
      "Epoch 32/50\n",
      "1628/1628 [==============================] - 0s 164us/step - loss: 0.0215 - acc: 0.9945 - val_loss: 0.0211 - val_acc: 0.9945\n",
      "Epoch 33/50\n",
      "1628/1628 [==============================] - 0s 157us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.0211 - val_acc: 0.9945\n",
      "Epoch 34/50\n",
      "1628/1628 [==============================] - 0s 168us/step - loss: 0.0215 - acc: 0.9939 - val_loss: 0.0210 - val_acc: 0.9945\n",
      "Epoch 35/50\n",
      "1628/1628 [==============================] - 1s 335us/step - loss: 0.0215 - acc: 0.9945 - val_loss: 0.0211 - val_acc: 0.9945\n",
      "Epoch 36/50\n",
      "1628/1628 [==============================] - 0s 230us/step - loss: 0.0214 - acc: 0.9945 - val_loss: 0.0210 - val_acc: 0.9945\n",
      "Epoch 37/50\n",
      "1628/1628 [==============================] - 0s 237us/step - loss: 0.0215 - acc: 0.9939 - val_loss: 0.0210 - val_acc: 0.9939\n",
      "Epoch 38/50\n",
      "1628/1628 [==============================] - 1s 352us/step - loss: 0.0214 - acc: 0.9945 - val_loss: 0.0210 - val_acc: 0.9939\n",
      "Epoch 39/50\n",
      "1628/1628 [==============================] - 0s 248us/step - loss: 0.0214 - acc: 0.9945 - val_loss: 0.0209 - val_acc: 0.9945\n",
      "Epoch 40/50\n",
      "1628/1628 [==============================] - 0s 204us/step - loss: 0.0214 - acc: 0.9939 - val_loss: 0.0209 - val_acc: 0.9945\n",
      "Epoch 41/50\n",
      "1628/1628 [==============================] - 0s 256us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0209 - val_acc: 0.9945\n",
      "Epoch 42/50\n",
      "1628/1628 [==============================] - 0s 203us/step - loss: 0.0211 - acc: 0.9945 - val_loss: 0.0208 - val_acc: 0.9945\n",
      "Epoch 43/50\n",
      "1628/1628 [==============================] - 0s 166us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0208 - val_acc: 0.9945\n",
      "Epoch 44/50\n",
      "1628/1628 [==============================] - 0s 216us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 0.0208 - val_acc: 0.9945\n",
      "Epoch 45/50\n",
      "1628/1628 [==============================] - 0s 139us/step - loss: 0.0213 - acc: 0.9939 - val_loss: 0.0208 - val_acc: 0.9945\n",
      "Epoch 46/50\n",
      "1628/1628 [==============================] - 0s 165us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 0.0207 - val_acc: 0.9945\n",
      "Epoch 47/50\n",
      "1628/1628 [==============================] - 0s 172us/step - loss: 0.0211 - acc: 0.9939 - val_loss: 0.0207 - val_acc: 0.9945\n",
      "Epoch 48/50\n",
      "1628/1628 [==============================] - 0s 233us/step - loss: 0.0211 - acc: 0.9945 - val_loss: 0.0207 - val_acc: 0.9939\n",
      "Epoch 49/50\n",
      "1628/1628 [==============================] - 0s 134us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 0.0207 - val_acc: 0.9945\n",
      "Epoch 50/50\n",
      "1628/1628 [==============================] - 0s 135us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0207 - val_acc: 0.9945\n",
      "6515/6515 [==============================] - 0s 65us/step\n",
      "[0.9877225288296857, 0.9920595939751147, 0.9922742389357891]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8,activation='relu',input_dim=5))\n",
    "#model.add(Dense(units = 32, activation = 'relu'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "ave_acc_occ_ann=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(occ_x, occ_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        history = model.fit(X_train1, y_train1,validation_data=(X_train1,y_train1),batch_size=32,epochs=50)\n",
    "        score += model.evaluate(X_test1,y_test1,verbose=1)[1]\n",
    "    ave_acc_occ_ann.append(score/3.0)\n",
    "    \n",
    "print ave_acc_occ_ann  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 130000 samples, validate on 130000 samples\n",
      "Epoch 1/100\n",
      "130000/130000 [==============================] - 5s 41us/step - loss: 0.8285 - acc: 0.7709 - val_loss: 0.6000 - val_acc: 0.8292\n",
      "Epoch 2/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.5746 - acc: 0.8316 - val_loss: 0.5537 - val_acc: 0.8336\n",
      "Epoch 3/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.5383 - acc: 0.8344 - val_loss: 0.5210 - val_acc: 0.8358\n",
      "Epoch 4/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.5102 - acc: 0.8371 - val_loss: 0.4998 - val_acc: 0.8400\n",
      "Epoch 5/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4950 - acc: 0.8427 - val_loss: 0.4885 - val_acc: 0.8450\n",
      "Epoch 6/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4858 - acc: 0.8457 - val_loss: 0.4820 - val_acc: 0.8471\n",
      "Epoch 7/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4803 - acc: 0.8474 - val_loss: 0.4772 - val_acc: 0.8483\n",
      "Epoch 8/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4764 - acc: 0.8478 - val_loss: 0.4750 - val_acc: 0.8488\n",
      "Epoch 9/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4736 - acc: 0.8484 - val_loss: 0.4721 - val_acc: 0.8488\n",
      "Epoch 10/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4717 - acc: 0.8484 - val_loss: 0.4709 - val_acc: 0.8476\n",
      "Epoch 11/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4700 - acc: 0.8488 - val_loss: 0.4681 - val_acc: 0.8490\n",
      "Epoch 12/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4684 - acc: 0.8493 - val_loss: 0.4663 - val_acc: 0.8497\n",
      "Epoch 13/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4670 - acc: 0.8493 - val_loss: 0.4659 - val_acc: 0.8495\n",
      "Epoch 14/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4658 - acc: 0.8496 - val_loss: 0.4649 - val_acc: 0.8493\n",
      "Epoch 15/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4646 - acc: 0.8498 - val_loss: 0.4644 - val_acc: 0.8495\n",
      "Epoch 16/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4636 - acc: 0.8496 - val_loss: 0.4621 - val_acc: 0.8499\n",
      "Epoch 17/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4627 - acc: 0.8500 - val_loss: 0.4616 - val_acc: 0.8500\n",
      "Epoch 18/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4619 - acc: 0.8500 - val_loss: 0.4617 - val_acc: 0.8497\n",
      "Epoch 19/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4612 - acc: 0.8503 - val_loss: 0.4606 - val_acc: 0.8505\n",
      "Epoch 20/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4605 - acc: 0.8501 - val_loss: 0.4597 - val_acc: 0.8505\n",
      "Epoch 21/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4599 - acc: 0.8506 - val_loss: 0.4590 - val_acc: 0.8505\n",
      "Epoch 22/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4593 - acc: 0.8506 - val_loss: 0.4617 - val_acc: 0.8499\n",
      "Epoch 23/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4586 - acc: 0.8507 - val_loss: 0.4576 - val_acc: 0.8515\n",
      "Epoch 24/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4584 - acc: 0.8512 - val_loss: 0.4576 - val_acc: 0.8514\n",
      "Epoch 25/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4580 - acc: 0.8511 - val_loss: 0.4563 - val_acc: 0.8512\n",
      "Epoch 26/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4573 - acc: 0.8510 - val_loss: 0.4566 - val_acc: 0.8513\n",
      "Epoch 27/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4570 - acc: 0.8512 - val_loss: 0.4560 - val_acc: 0.8511\n",
      "Epoch 28/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4567 - acc: 0.8513 - val_loss: 0.4551 - val_acc: 0.8508\n",
      "Epoch 29/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4562 - acc: 0.8515 - val_loss: 0.4565 - val_acc: 0.8512\n",
      "Epoch 30/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4560 - acc: 0.8511 - val_loss: 0.4542 - val_acc: 0.8522\n",
      "Epoch 31/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4555 - acc: 0.8512 - val_loss: 0.4546 - val_acc: 0.8516\n",
      "Epoch 32/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4552 - acc: 0.8512 - val_loss: 0.4548 - val_acc: 0.8519\n",
      "Epoch 33/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4549 - acc: 0.8520 - val_loss: 0.4533 - val_acc: 0.8519\n",
      "Epoch 34/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4548 - acc: 0.8514 - val_loss: 0.4543 - val_acc: 0.8521\n",
      "Epoch 35/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4543 - acc: 0.8518 - val_loss: 0.4531 - val_acc: 0.8524\n",
      "Epoch 36/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4539 - acc: 0.8515 - val_loss: 0.4532 - val_acc: 0.8519\n",
      "Epoch 37/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4538 - acc: 0.8520 - val_loss: 0.4536 - val_acc: 0.8524\n",
      "Epoch 38/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4536 - acc: 0.8517 - val_loss: 0.4521 - val_acc: 0.8524\n",
      "Epoch 39/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4533 - acc: 0.8521 - val_loss: 0.4517 - val_acc: 0.8527\n",
      "Epoch 40/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4529 - acc: 0.8518 - val_loss: 0.4516 - val_acc: 0.8525\n",
      "Epoch 41/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4527 - acc: 0.8521 - val_loss: 0.4517 - val_acc: 0.8524\n",
      "Epoch 42/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4525 - acc: 0.8521 - val_loss: 0.4511 - val_acc: 0.8523\n",
      "Epoch 43/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4522 - acc: 0.8521 - val_loss: 0.4515 - val_acc: 0.8522\n",
      "Epoch 44/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4520 - acc: 0.8524 - val_loss: 0.4502 - val_acc: 0.8526\n",
      "Epoch 45/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4518 - acc: 0.8522 - val_loss: 0.4501 - val_acc: 0.8526\n",
      "Epoch 46/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4516 - acc: 0.8519 - val_loss: 0.4499 - val_acc: 0.8531\n",
      "Epoch 47/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4514 - acc: 0.8526 - val_loss: 0.4502 - val_acc: 0.8527\n",
      "Epoch 48/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4511 - acc: 0.8524 - val_loss: 0.4503 - val_acc: 0.8527\n",
      "Epoch 49/100\n",
      "130000/130000 [==============================] - 2s 16us/step - loss: 0.4511 - acc: 0.8523 - val_loss: 0.4510 - val_acc: 0.8530\n",
      "Epoch 50/100\n",
      "130000/130000 [==============================] - 2s 18us/step - loss: 0.4507 - acc: 0.8525 - val_loss: 0.4528 - val_acc: 0.8519\n",
      "Epoch 51/100\n",
      "130000/130000 [==============================] - 3s 23us/step - loss: 0.4504 - acc: 0.8525 - val_loss: 0.4501 - val_acc: 0.8528\n",
      "Epoch 52/100\n",
      "130000/130000 [==============================] - 2s 18us/step - loss: 0.4506 - acc: 0.8525 - val_loss: 0.4496 - val_acc: 0.8530\n",
      "Epoch 53/100\n",
      "130000/130000 [==============================] - 3s 23us/step - loss: 0.4504 - acc: 0.8525 - val_loss: 0.4493 - val_acc: 0.8526\n",
      "Epoch 54/100\n",
      "130000/130000 [==============================] - 2s 19us/step - loss: 0.4501 - acc: 0.8528 - val_loss: 0.4505 - val_acc: 0.8523\n",
      "Epoch 55/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4500 - acc: 0.8523 - val_loss: 0.4495 - val_acc: 0.8528\n",
      "Epoch 56/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4498 - acc: 0.8529 - val_loss: 0.4487 - val_acc: 0.8525\n",
      "Epoch 57/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4498 - acc: 0.8528 - val_loss: 0.4492 - val_acc: 0.8534\n",
      "Epoch 58/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4496 - acc: 0.8525 - val_loss: 0.4487 - val_acc: 0.8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4491 - acc: 0.8530 - val_loss: 0.4484 - val_acc: 0.8535\n",
      "Epoch 60/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4492 - acc: 0.8528 - val_loss: 0.4485 - val_acc: 0.8530\n",
      "Epoch 61/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4490 - acc: 0.8528 - val_loss: 0.4489 - val_acc: 0.8522\n",
      "Epoch 62/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4491 - acc: 0.8527 - val_loss: 0.4479 - val_acc: 0.8531\n",
      "Epoch 63/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4490 - acc: 0.8533 - val_loss: 0.4483 - val_acc: 0.8531\n",
      "Epoch 64/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4487 - acc: 0.8532 - val_loss: 0.4484 - val_acc: 0.8525\n",
      "Epoch 65/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4486 - acc: 0.8529 - val_loss: 0.4471 - val_acc: 0.8533\n",
      "Epoch 66/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4486 - acc: 0.8532 - val_loss: 0.4469 - val_acc: 0.8537\n",
      "Epoch 67/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4485 - acc: 0.8529 - val_loss: 0.4480 - val_acc: 0.8533\n",
      "Epoch 68/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4485 - acc: 0.8532 - val_loss: 0.4470 - val_acc: 0.8534\n",
      "Epoch 69/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4482 - acc: 0.8533 - val_loss: 0.4470 - val_acc: 0.8536\n",
      "Epoch 70/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4482 - acc: 0.8533 - val_loss: 0.4465 - val_acc: 0.8533\n",
      "Epoch 71/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4480 - acc: 0.8533 - val_loss: 0.4472 - val_acc: 0.8533\n",
      "Epoch 72/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4480 - acc: 0.8531 - val_loss: 0.4468 - val_acc: 0.8533\n",
      "Epoch 73/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4478 - acc: 0.8532 - val_loss: 0.4482 - val_acc: 0.8531\n",
      "Epoch 74/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4475 - acc: 0.8534 - val_loss: 0.4469 - val_acc: 0.8531\n",
      "Epoch 75/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4477 - acc: 0.8529 - val_loss: 0.4472 - val_acc: 0.8537\n",
      "Epoch 76/100\n",
      "130000/130000 [==============================] - 1s 12us/step - loss: 0.4476 - acc: 0.8535 - val_loss: 0.4468 - val_acc: 0.8533\n",
      "Epoch 77/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4474 - acc: 0.8532 - val_loss: 0.4473 - val_acc: 0.8532\n",
      "Epoch 78/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4474 - acc: 0.8533 - val_loss: 0.4471 - val_acc: 0.8529\n",
      "Epoch 79/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4472 - acc: 0.8534 - val_loss: 0.4457 - val_acc: 0.8539\n",
      "Epoch 80/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4470 - acc: 0.8535 - val_loss: 0.4455 - val_acc: 0.8539\n",
      "Epoch 81/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4469 - acc: 0.8534 - val_loss: 0.4452 - val_acc: 0.8541\n",
      "Epoch 82/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4467 - acc: 0.8536 - val_loss: 0.4459 - val_acc: 0.8536\n",
      "Epoch 83/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4469 - acc: 0.8534 - val_loss: 0.4464 - val_acc: 0.8532\n",
      "Epoch 84/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4468 - acc: 0.8536 - val_loss: 0.4453 - val_acc: 0.8541\n",
      "Epoch 85/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4467 - acc: 0.8536 - val_loss: 0.4451 - val_acc: 0.8544\n",
      "Epoch 86/100\n",
      "130000/130000 [==============================] - 1s 12us/step - loss: 0.4465 - acc: 0.8537 - val_loss: 0.4453 - val_acc: 0.8538\n",
      "Epoch 87/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4466 - acc: 0.8538 - val_loss: 0.4448 - val_acc: 0.8539\n",
      "Epoch 88/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4463 - acc: 0.8535 - val_loss: 0.4453 - val_acc: 0.8535\n",
      "Epoch 89/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4462 - acc: 0.8536 - val_loss: 0.4483 - val_acc: 0.8535\n",
      "Epoch 90/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4462 - acc: 0.8536 - val_loss: 0.4448 - val_acc: 0.8539\n",
      "Epoch 91/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4460 - acc: 0.8535 - val_loss: 0.4452 - val_acc: 0.8539\n",
      "Epoch 92/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4460 - acc: 0.8535 - val_loss: 0.4466 - val_acc: 0.8536\n",
      "Epoch 93/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4457 - acc: 0.8538 - val_loss: 0.4447 - val_acc: 0.8538\n",
      "Epoch 94/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4460 - acc: 0.8537 - val_loss: 0.4452 - val_acc: 0.8534\n",
      "Epoch 95/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4456 - acc: 0.8540 - val_loss: 0.4438 - val_acc: 0.8542\n",
      "Epoch 96/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4456 - acc: 0.8539 - val_loss: 0.4447 - val_acc: 0.8541\n",
      "Epoch 97/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4454 - acc: 0.8540 - val_loss: 0.4460 - val_acc: 0.8530\n",
      "Epoch 98/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4456 - acc: 0.8540 - val_loss: 0.4447 - val_acc: 0.8537\n",
      "Epoch 99/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4452 - acc: 0.8540 - val_loss: 0.4442 - val_acc: 0.8536\n",
      "Epoch 100/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4453 - acc: 0.8537 - val_loss: 0.4441 - val_acc: 0.8541\n",
      "32501/32501 [==============================] - 2s 69us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 130000 samples, validate on 130000 samples\n",
      "Epoch 1/100\n",
      "130000/130000 [==============================] - 4s 34us/step - loss: 0.8244 - acc: 0.7750 - val_loss: 0.6069 - val_acc: 0.8288\n",
      "Epoch 2/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.5842 - acc: 0.8308 - val_loss: 0.5624 - val_acc: 0.8331\n",
      "Epoch 3/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.5445 - acc: 0.8334 - val_loss: 0.5290 - val_acc: 0.8343\n",
      "Epoch 4/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.5180 - acc: 0.8350 - val_loss: 0.5080 - val_acc: 0.8354\n",
      "Epoch 5/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.5031 - acc: 0.8373 - val_loss: 0.4980 - val_acc: 0.8384\n",
      "Epoch 6/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4938 - acc: 0.8410 - val_loss: 0.4885 - val_acc: 0.8437\n",
      "Epoch 7/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4875 - acc: 0.8435 - val_loss: 0.4840 - val_acc: 0.8440\n",
      "Epoch 8/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4829 - acc: 0.8453 - val_loss: 0.4802 - val_acc: 0.8459\n",
      "Epoch 9/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4794 - acc: 0.8458 - val_loss: 0.4764 - val_acc: 0.8465\n",
      "Epoch 10/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4770 - acc: 0.8467 - val_loss: 0.4745 - val_acc: 0.8467\n",
      "Epoch 11/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4749 - acc: 0.8471 - val_loss: 0.4732 - val_acc: 0.8473\n",
      "Epoch 12/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4731 - acc: 0.8469 - val_loss: 0.4717 - val_acc: 0.8467\n",
      "Epoch 13/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4715 - acc: 0.8474 - val_loss: 0.4691 - val_acc: 0.8478\n",
      "Epoch 14/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4701 - acc: 0.8476 - val_loss: 0.4699 - val_acc: 0.8478\n",
      "Epoch 15/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4689 - acc: 0.8481 - val_loss: 0.4668 - val_acc: 0.8490\n",
      "Epoch 16/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4677 - acc: 0.8482 - val_loss: 0.4676 - val_acc: 0.8485\n",
      "Epoch 17/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4668 - acc: 0.8484 - val_loss: 0.4695 - val_acc: 0.8493\n",
      "Epoch 18/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4660 - acc: 0.8488 - val_loss: 0.4644 - val_acc: 0.8487\n",
      "Epoch 19/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4653 - acc: 0.8483 - val_loss: 0.4638 - val_acc: 0.8494\n",
      "Epoch 20/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4643 - acc: 0.8488 - val_loss: 0.4624 - val_acc: 0.8498\n",
      "Epoch 21/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4634 - acc: 0.8489 - val_loss: 0.4617 - val_acc: 0.8500\n",
      "Epoch 22/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4629 - acc: 0.8493 - val_loss: 0.4617 - val_acc: 0.8497\n",
      "Epoch 23/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4624 - acc: 0.8494 - val_loss: 0.4607 - val_acc: 0.8500\n",
      "Epoch 24/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4618 - acc: 0.8495 - val_loss: 0.4621 - val_acc: 0.8487\n",
      "Epoch 25/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4610 - acc: 0.8495 - val_loss: 0.4590 - val_acc: 0.8501\n",
      "Epoch 26/100\n",
      "130000/130000 [==============================] - 2s 16us/step - loss: 0.4606 - acc: 0.8500 - val_loss: 0.4596 - val_acc: 0.8498\n",
      "Epoch 27/100\n",
      "130000/130000 [==============================] - 2s 16us/step - loss: 0.4599 - acc: 0.8498 - val_loss: 0.4611 - val_acc: 0.8503\n",
      "Epoch 28/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4595 - acc: 0.8500 - val_loss: 0.4579 - val_acc: 0.8506\n",
      "Epoch 29/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4588 - acc: 0.8503 - val_loss: 0.4600 - val_acc: 0.8492\n",
      "Epoch 30/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4585 - acc: 0.8502 - val_loss: 0.4569 - val_acc: 0.8505\n",
      "Epoch 31/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4580 - acc: 0.8503 - val_loss: 0.4563 - val_acc: 0.8510\n",
      "Epoch 32/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4574 - acc: 0.8499 - val_loss: 0.4565 - val_acc: 0.8507\n",
      "Epoch 33/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4571 - acc: 0.8503 - val_loss: 0.4555 - val_acc: 0.8509\n",
      "Epoch 34/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4567 - acc: 0.8509 - val_loss: 0.4573 - val_acc: 0.8502\n",
      "Epoch 35/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4562 - acc: 0.8507 - val_loss: 0.4551 - val_acc: 0.8516\n",
      "Epoch 36/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4558 - acc: 0.8512 - val_loss: 0.4542 - val_acc: 0.8516\n",
      "Epoch 37/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4556 - acc: 0.8509 - val_loss: 0.4535 - val_acc: 0.8516\n",
      "Epoch 38/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4550 - acc: 0.8509 - val_loss: 0.4544 - val_acc: 0.8519\n",
      "Epoch 39/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4549 - acc: 0.8513 - val_loss: 0.4552 - val_acc: 0.8516\n",
      "Epoch 40/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4544 - acc: 0.8513 - val_loss: 0.4533 - val_acc: 0.8519\n",
      "Epoch 41/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4540 - acc: 0.8516 - val_loss: 0.4523 - val_acc: 0.8522\n",
      "Epoch 42/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4540 - acc: 0.8514 - val_loss: 0.4531 - val_acc: 0.8514\n",
      "Epoch 43/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4536 - acc: 0.8515 - val_loss: 0.4535 - val_acc: 0.8514\n",
      "Epoch 44/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4533 - acc: 0.8519 - val_loss: 0.4518 - val_acc: 0.8518\n",
      "Epoch 45/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4533 - acc: 0.8516 - val_loss: 0.4524 - val_acc: 0.8520\n",
      "Epoch 46/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4529 - acc: 0.8516 - val_loss: 0.4517 - val_acc: 0.8522\n",
      "Epoch 47/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4524 - acc: 0.8520 - val_loss: 0.4522 - val_acc: 0.8512\n",
      "Epoch 48/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4525 - acc: 0.8518 - val_loss: 0.4520 - val_acc: 0.8519\n",
      "Epoch 49/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4523 - acc: 0.8519 - val_loss: 0.4507 - val_acc: 0.8527\n",
      "Epoch 50/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4518 - acc: 0.8520 - val_loss: 0.4509 - val_acc: 0.8524\n",
      "Epoch 51/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4519 - acc: 0.8518 - val_loss: 0.4515 - val_acc: 0.8523\n",
      "Epoch 52/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4517 - acc: 0.8522 - val_loss: 0.4494 - val_acc: 0.8526\n",
      "Epoch 53/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4515 - acc: 0.8520 - val_loss: 0.4526 - val_acc: 0.8509\n",
      "Epoch 54/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4510 - acc: 0.8520 - val_loss: 0.4504 - val_acc: 0.8528\n",
      "Epoch 55/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4511 - acc: 0.8525 - val_loss: 0.4509 - val_acc: 0.8520\n",
      "Epoch 56/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4508 - acc: 0.8526 - val_loss: 0.4495 - val_acc: 0.8526\n",
      "Epoch 57/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4508 - acc: 0.8523 - val_loss: 0.4490 - val_acc: 0.8529\n",
      "Epoch 58/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4507 - acc: 0.8519 - val_loss: 0.4495 - val_acc: 0.8527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4506 - acc: 0.8524 - val_loss: 0.4493 - val_acc: 0.8523\n",
      "Epoch 60/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4503 - acc: 0.8528 - val_loss: 0.4537 - val_acc: 0.8500\n",
      "Epoch 61/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4502 - acc: 0.8524 - val_loss: 0.4518 - val_acc: 0.8515\n",
      "Epoch 62/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4500 - acc: 0.8527 - val_loss: 0.4493 - val_acc: 0.8523\n",
      "Epoch 63/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4500 - acc: 0.8526 - val_loss: 0.4490 - val_acc: 0.8530\n",
      "Epoch 64/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4496 - acc: 0.8529 - val_loss: 0.4480 - val_acc: 0.8534\n",
      "Epoch 65/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4497 - acc: 0.8520 - val_loss: 0.4475 - val_acc: 0.8531\n",
      "Epoch 66/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4497 - acc: 0.8525 - val_loss: 0.4506 - val_acc: 0.8517\n",
      "Epoch 67/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4490 - acc: 0.8528 - val_loss: 0.4489 - val_acc: 0.8532\n",
      "Epoch 68/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4491 - acc: 0.8528 - val_loss: 0.4495 - val_acc: 0.8521\n",
      "Epoch 69/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4490 - acc: 0.8528 - val_loss: 0.4485 - val_acc: 0.8527\n",
      "Epoch 70/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4492 - acc: 0.8527 - val_loss: 0.4475 - val_acc: 0.8532\n",
      "Epoch 71/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4490 - acc: 0.8528 - val_loss: 0.4475 - val_acc: 0.8533\n",
      "Epoch 72/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4486 - acc: 0.8528 - val_loss: 0.4481 - val_acc: 0.8526\n",
      "Epoch 73/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4485 - acc: 0.8528 - val_loss: 0.4492 - val_acc: 0.8523\n",
      "Epoch 74/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4485 - acc: 0.8525 - val_loss: 0.4472 - val_acc: 0.8530\n",
      "Epoch 75/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4485 - acc: 0.8524 - val_loss: 0.4506 - val_acc: 0.8525\n",
      "Epoch 76/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4483 - acc: 0.8527 - val_loss: 0.4504 - val_acc: 0.8517\n",
      "Epoch 77/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4482 - acc: 0.8528 - val_loss: 0.4482 - val_acc: 0.8530\n",
      "Epoch 78/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4483 - acc: 0.8528 - val_loss: 0.4465 - val_acc: 0.8535\n",
      "Epoch 79/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4479 - acc: 0.8529 - val_loss: 0.4472 - val_acc: 0.8533\n",
      "Epoch 80/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4480 - acc: 0.8530 - val_loss: 0.4465 - val_acc: 0.8531\n",
      "Epoch 81/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4477 - acc: 0.8533 - val_loss: 0.4464 - val_acc: 0.8536\n",
      "Epoch 82/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4477 - acc: 0.8526 - val_loss: 0.4458 - val_acc: 0.8533\n",
      "Epoch 83/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4477 - acc: 0.8525 - val_loss: 0.4458 - val_acc: 0.8537\n",
      "Epoch 84/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4476 - acc: 0.8531 - val_loss: 0.4458 - val_acc: 0.8537\n",
      "Epoch 85/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4475 - acc: 0.8528 - val_loss: 0.4465 - val_acc: 0.8532\n",
      "Epoch 86/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4473 - acc: 0.8527 - val_loss: 0.4477 - val_acc: 0.8524\n",
      "Epoch 87/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4471 - acc: 0.8533 - val_loss: 0.4455 - val_acc: 0.8533\n",
      "Epoch 88/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4470 - acc: 0.8528 - val_loss: 0.4477 - val_acc: 0.8523\n",
      "Epoch 89/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4471 - acc: 0.8528 - val_loss: 0.4467 - val_acc: 0.8531\n",
      "Epoch 90/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4469 - acc: 0.8530 - val_loss: 0.4468 - val_acc: 0.8529\n",
      "Epoch 91/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4468 - acc: 0.8529 - val_loss: 0.4453 - val_acc: 0.8539\n",
      "Epoch 92/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4468 - acc: 0.8530 - val_loss: 0.4460 - val_acc: 0.8534\n",
      "Epoch 93/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4468 - acc: 0.8530 - val_loss: 0.4468 - val_acc: 0.8529\n",
      "Epoch 94/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4465 - acc: 0.8533 - val_loss: 0.4446 - val_acc: 0.8540\n",
      "Epoch 95/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4468 - acc: 0.8529 - val_loss: 0.4448 - val_acc: 0.8540\n",
      "Epoch 96/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4464 - acc: 0.8533 - val_loss: 0.4468 - val_acc: 0.8533\n",
      "Epoch 97/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4467 - acc: 0.8533 - val_loss: 0.4448 - val_acc: 0.8538\n",
      "Epoch 98/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4460 - acc: 0.8531 - val_loss: 0.4448 - val_acc: 0.8536\n",
      "Epoch 99/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4463 - acc: 0.8533 - val_loss: 0.4458 - val_acc: 0.8532\n",
      "Epoch 100/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4462 - acc: 0.8531 - val_loss: 0.4522 - val_acc: 0.8495\n",
      "32501/32501 [==============================] - 2s 65us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 130000 samples, validate on 130000 samples\n",
      "Epoch 1/100\n",
      "130000/130000 [==============================] - 5s 35us/step - loss: 0.8360 - acc: 0.7603 - val_loss: 0.6043 - val_acc: 0.8274\n",
      "Epoch 2/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.5746 - acc: 0.8310 - val_loss: 0.5509 - val_acc: 0.8335\n",
      "Epoch 3/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.5353 - acc: 0.8349 - val_loss: 0.5237 - val_acc: 0.8361\n",
      "Epoch 4/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.5133 - acc: 0.8368 - val_loss: 0.5055 - val_acc: 0.8375\n",
      "Epoch 5/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.5012 - acc: 0.8380 - val_loss: 0.4955 - val_acc: 0.8382\n",
      "Epoch 6/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4937 - acc: 0.8382 - val_loss: 0.4894 - val_acc: 0.8386\n",
      "Epoch 7/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4880 - acc: 0.8404 - val_loss: 0.4845 - val_acc: 0.8427\n",
      "Epoch 8/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4837 - acc: 0.8432 - val_loss: 0.4808 - val_acc: 0.8443\n",
      "Epoch 9/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4801 - acc: 0.8451 - val_loss: 0.4787 - val_acc: 0.8460\n",
      "Epoch 10/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4773 - acc: 0.8464 - val_loss: 0.4746 - val_acc: 0.8471\n",
      "Epoch 11/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4751 - acc: 0.8470 - val_loss: 0.4731 - val_acc: 0.8475\n",
      "Epoch 12/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4733 - acc: 0.8475 - val_loss: 0.4719 - val_acc: 0.8476\n",
      "Epoch 13/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4717 - acc: 0.8479 - val_loss: 0.4710 - val_acc: 0.8475\n",
      "Epoch 14/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4704 - acc: 0.8477 - val_loss: 0.4686 - val_acc: 0.8489\n",
      "Epoch 15/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4694 - acc: 0.8482 - val_loss: 0.4672 - val_acc: 0.8488\n",
      "Epoch 16/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4679 - acc: 0.8487 - val_loss: 0.4665 - val_acc: 0.8489\n",
      "Epoch 17/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4673 - acc: 0.8485 - val_loss: 0.4652 - val_acc: 0.8487\n",
      "Epoch 18/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4662 - acc: 0.8487 - val_loss: 0.4664 - val_acc: 0.8497\n",
      "Epoch 19/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4653 - acc: 0.8488 - val_loss: 0.4634 - val_acc: 0.8498\n",
      "Epoch 20/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4645 - acc: 0.8492 - val_loss: 0.4650 - val_acc: 0.8495\n",
      "Epoch 21/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4636 - acc: 0.8492 - val_loss: 0.4625 - val_acc: 0.8496\n",
      "Epoch 22/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4629 - acc: 0.8496 - val_loss: 0.4619 - val_acc: 0.8501\n",
      "Epoch 23/100\n",
      "130000/130000 [==============================] - 2s 17us/step - loss: 0.4621 - acc: 0.8496 - val_loss: 0.4609 - val_acc: 0.8506\n",
      "Epoch 24/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4614 - acc: 0.8495 - val_loss: 0.4608 - val_acc: 0.8501\n",
      "Epoch 25/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4611 - acc: 0.8503 - val_loss: 0.4601 - val_acc: 0.8497\n",
      "Epoch 26/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4604 - acc: 0.8501 - val_loss: 0.4592 - val_acc: 0.8504\n",
      "Epoch 27/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4598 - acc: 0.8507 - val_loss: 0.4585 - val_acc: 0.8507\n",
      "Epoch 28/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4596 - acc: 0.8503 - val_loss: 0.4586 - val_acc: 0.8505\n",
      "Epoch 29/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4591 - acc: 0.8507 - val_loss: 0.4573 - val_acc: 0.8505\n",
      "Epoch 30/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4586 - acc: 0.8508 - val_loss: 0.4588 - val_acc: 0.8508\n",
      "Epoch 31/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4582 - acc: 0.8505 - val_loss: 0.4558 - val_acc: 0.8515\n",
      "Epoch 32/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4579 - acc: 0.8505 - val_loss: 0.4612 - val_acc: 0.8508\n",
      "Epoch 33/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4576 - acc: 0.8511 - val_loss: 0.4573 - val_acc: 0.8508\n",
      "Epoch 34/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4573 - acc: 0.8507 - val_loss: 0.4553 - val_acc: 0.8516\n",
      "Epoch 35/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4568 - acc: 0.8507 - val_loss: 0.4551 - val_acc: 0.8513\n",
      "Epoch 36/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4564 - acc: 0.8508 - val_loss: 0.4551 - val_acc: 0.8513\n",
      "Epoch 37/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4564 - acc: 0.8512 - val_loss: 0.4553 - val_acc: 0.8517\n",
      "Epoch 38/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4562 - acc: 0.8510 - val_loss: 0.4566 - val_acc: 0.8511\n",
      "Epoch 39/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4555 - acc: 0.8515 - val_loss: 0.4548 - val_acc: 0.8513\n",
      "Epoch 40/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4553 - acc: 0.8510 - val_loss: 0.4569 - val_acc: 0.8502\n",
      "Epoch 41/100\n",
      "130000/130000 [==============================] - 2s 17us/step - loss: 0.4552 - acc: 0.8511 - val_loss: 0.4539 - val_acc: 0.8517\n",
      "Epoch 42/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4551 - acc: 0.8513 - val_loss: 0.4538 - val_acc: 0.8517\n",
      "Epoch 43/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4549 - acc: 0.8514 - val_loss: 0.4548 - val_acc: 0.8520\n",
      "Epoch 44/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4545 - acc: 0.8513 - val_loss: 0.4546 - val_acc: 0.8518\n",
      "Epoch 45/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4543 - acc: 0.8517 - val_loss: 0.4536 - val_acc: 0.8516\n",
      "Epoch 46/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4541 - acc: 0.8518 - val_loss: 0.4534 - val_acc: 0.8513\n",
      "Epoch 47/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4539 - acc: 0.8515 - val_loss: 0.4536 - val_acc: 0.8517\n",
      "Epoch 48/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4535 - acc: 0.8518 - val_loss: 0.4524 - val_acc: 0.8523\n",
      "Epoch 49/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4535 - acc: 0.8520 - val_loss: 0.4526 - val_acc: 0.8522\n",
      "Epoch 50/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4531 - acc: 0.8518 - val_loss: 0.4529 - val_acc: 0.8523\n",
      "Epoch 51/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4530 - acc: 0.8519 - val_loss: 0.4514 - val_acc: 0.8524\n",
      "Epoch 52/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4526 - acc: 0.8519 - val_loss: 0.4532 - val_acc: 0.8523\n",
      "Epoch 53/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4526 - acc: 0.8524 - val_loss: 0.4512 - val_acc: 0.8523\n",
      "Epoch 54/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4524 - acc: 0.8518 - val_loss: 0.4509 - val_acc: 0.8520\n",
      "Epoch 55/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4522 - acc: 0.8525 - val_loss: 0.4501 - val_acc: 0.8529\n",
      "Epoch 56/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4519 - acc: 0.8524 - val_loss: 0.4498 - val_acc: 0.8531\n",
      "Epoch 57/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4516 - acc: 0.8528 - val_loss: 0.4501 - val_acc: 0.8526\n",
      "Epoch 58/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4517 - acc: 0.8524 - val_loss: 0.4512 - val_acc: 0.8525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4515 - acc: 0.8524 - val_loss: 0.4506 - val_acc: 0.8530\n",
      "Epoch 60/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4509 - acc: 0.8530 - val_loss: 0.4502 - val_acc: 0.8531\n",
      "Epoch 61/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4510 - acc: 0.8531 - val_loss: 0.4506 - val_acc: 0.8525\n",
      "Epoch 62/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4509 - acc: 0.8527 - val_loss: 0.4489 - val_acc: 0.8532\n",
      "Epoch 63/100\n",
      "130000/130000 [==============================] - 1s 12us/step - loss: 0.4506 - acc: 0.8528 - val_loss: 0.4500 - val_acc: 0.8520\n",
      "Epoch 64/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4507 - acc: 0.8530 - val_loss: 0.4495 - val_acc: 0.8529\n",
      "Epoch 65/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4506 - acc: 0.8527 - val_loss: 0.4498 - val_acc: 0.8534\n",
      "Epoch 66/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4503 - acc: 0.8529 - val_loss: 0.4496 - val_acc: 0.8532\n",
      "Epoch 67/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4503 - acc: 0.8528 - val_loss: 0.4484 - val_acc: 0.8534\n",
      "Epoch 68/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4500 - acc: 0.8530 - val_loss: 0.4486 - val_acc: 0.8533\n",
      "Epoch 69/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4500 - acc: 0.8532 - val_loss: 0.4536 - val_acc: 0.8503\n",
      "Epoch 70/100\n",
      "130000/130000 [==============================] - 2s 16us/step - loss: 0.4500 - acc: 0.8528 - val_loss: 0.4484 - val_acc: 0.8521\n",
      "Epoch 71/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4499 - acc: 0.8530 - val_loss: 0.4496 - val_acc: 0.8534\n",
      "Epoch 72/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4497 - acc: 0.8529 - val_loss: 0.4486 - val_acc: 0.8532\n",
      "Epoch 73/100\n",
      "130000/130000 [==============================] - 2s 17us/step - loss: 0.4494 - acc: 0.8536 - val_loss: 0.4498 - val_acc: 0.8529\n",
      "Epoch 74/100\n",
      "130000/130000 [==============================] - 2s 18us/step - loss: 0.4494 - acc: 0.8533 - val_loss: 0.4486 - val_acc: 0.8528\n",
      "Epoch 75/100\n",
      "130000/130000 [==============================] - 3s 22us/step - loss: 0.4495 - acc: 0.8534 - val_loss: 0.4476 - val_acc: 0.8538\n",
      "Epoch 76/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4493 - acc: 0.8533 - val_loss: 0.4472 - val_acc: 0.8537\n",
      "Epoch 77/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4491 - acc: 0.8533 - val_loss: 0.4513 - val_acc: 0.8530\n",
      "Epoch 78/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4489 - acc: 0.8533 - val_loss: 0.4490 - val_acc: 0.8527\n",
      "Epoch 79/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4490 - acc: 0.8533 - val_loss: 0.4496 - val_acc: 0.8536\n",
      "Epoch 80/100\n",
      "130000/130000 [==============================] - 3s 19us/step - loss: 0.4487 - acc: 0.8531 - val_loss: 0.4482 - val_acc: 0.8542\n",
      "Epoch 81/100\n",
      "130000/130000 [==============================] - 3s 23us/step - loss: 0.4488 - acc: 0.8534 - val_loss: 0.4469 - val_acc: 0.8542\n",
      "Epoch 82/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4484 - acc: 0.8533 - val_loss: 0.4477 - val_acc: 0.8535\n",
      "Epoch 83/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4483 - acc: 0.8533 - val_loss: 0.4479 - val_acc: 0.8537\n",
      "Epoch 84/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4484 - acc: 0.8535 - val_loss: 0.4465 - val_acc: 0.8537\n",
      "Epoch 85/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4483 - acc: 0.8533 - val_loss: 0.4489 - val_acc: 0.8525\n",
      "Epoch 86/100\n",
      "130000/130000 [==============================] - 2s 14us/step - loss: 0.4482 - acc: 0.8534 - val_loss: 0.4469 - val_acc: 0.8542\n",
      "Epoch 87/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4482 - acc: 0.8533 - val_loss: 0.4496 - val_acc: 0.8528\n",
      "Epoch 88/100\n",
      "130000/130000 [==============================] - 2s 13us/step - loss: 0.4481 - acc: 0.8534 - val_loss: 0.4463 - val_acc: 0.8537\n",
      "Epoch 89/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4478 - acc: 0.8536 - val_loss: 0.4486 - val_acc: 0.8531\n",
      "Epoch 90/100\n",
      "130000/130000 [==============================] - 1s 11us/step - loss: 0.4479 - acc: 0.8537 - val_loss: 0.4463 - val_acc: 0.8539\n",
      "Epoch 91/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4478 - acc: 0.8537 - val_loss: 0.4460 - val_acc: 0.8538\n",
      "Epoch 92/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4478 - acc: 0.8534 - val_loss: 0.4485 - val_acc: 0.8524\n",
      "Epoch 93/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4477 - acc: 0.8531 - val_loss: 0.4465 - val_acc: 0.8537\n",
      "Epoch 94/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4475 - acc: 0.8539 - val_loss: 0.4455 - val_acc: 0.8547\n",
      "Epoch 95/100\n",
      "130000/130000 [==============================] - 2s 17us/step - loss: 0.4475 - acc: 0.8534 - val_loss: 0.4462 - val_acc: 0.8539\n",
      "Epoch 96/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4475 - acc: 0.8538 - val_loss: 0.4467 - val_acc: 0.8534\n",
      "Epoch 97/100\n",
      "130000/130000 [==============================] - 2s 12us/step - loss: 0.4473 - acc: 0.8536 - val_loss: 0.4461 - val_acc: 0.8532\n",
      "Epoch 98/100\n",
      "130000/130000 [==============================] - 2s 16us/step - loss: 0.4471 - acc: 0.8534 - val_loss: 0.4479 - val_acc: 0.8538\n",
      "Epoch 99/100\n",
      "130000/130000 [==============================] - 2s 17us/step - loss: 0.4472 - acc: 0.8537 - val_loss: 0.4461 - val_acc: 0.8530\n",
      "Epoch 100/100\n",
      "130000/130000 [==============================] - 2s 15us/step - loss: 0.4470 - acc: 0.8533 - val_loss: 0.4460 - val_acc: 0.8544\n",
      "32501/32501 [==============================] - 3s 98us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81250 samples, validate on 81250 samples\n",
      "Epoch 1/100\n",
      "81250/81250 [==============================] - 5s 63us/step - loss: 0.9081 - acc: 0.7459 - val_loss: 0.6503 - val_acc: 0.8224\n",
      "Epoch 2/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.6156 - acc: 0.8256 - val_loss: 0.5897 - val_acc: 0.8283\n",
      "Epoch 3/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.5748 - acc: 0.8307 - val_loss: 0.5610 - val_acc: 0.8328\n",
      "Epoch 4/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.5486 - acc: 0.8331 - val_loss: 0.5370 - val_acc: 0.8345\n",
      "Epoch 5/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.5247 - acc: 0.8345 - val_loss: 0.5131 - val_acc: 0.8360\n",
      "Epoch 6/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.5074 - acc: 0.8374 - val_loss: 0.5027 - val_acc: 0.8374\n",
      "Epoch 7/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4975 - acc: 0.8414 - val_loss: 0.4925 - val_acc: 0.8441\n",
      "Epoch 8/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4903 - acc: 0.8444 - val_loss: 0.4865 - val_acc: 0.8456\n",
      "Epoch 9/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4854 - acc: 0.8453 - val_loss: 0.4840 - val_acc: 0.8463\n",
      "Epoch 10/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4820 - acc: 0.8468 - val_loss: 0.4786 - val_acc: 0.8474\n",
      "Epoch 11/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4791 - acc: 0.8473 - val_loss: 0.4784 - val_acc: 0.8472\n",
      "Epoch 12/100\n",
      "81250/81250 [==============================] - 2s 22us/step - loss: 0.4767 - acc: 0.8479 - val_loss: 0.4751 - val_acc: 0.8483\n",
      "Epoch 13/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4750 - acc: 0.8484 - val_loss: 0.4735 - val_acc: 0.8492\n",
      "Epoch 14/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4733 - acc: 0.8484 - val_loss: 0.4728 - val_acc: 0.8482\n",
      "Epoch 15/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4718 - acc: 0.8492 - val_loss: 0.4700 - val_acc: 0.8493\n",
      "Epoch 16/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4704 - acc: 0.8499 - val_loss: 0.4690 - val_acc: 0.8494\n",
      "Epoch 17/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4694 - acc: 0.8496 - val_loss: 0.4676 - val_acc: 0.8504\n",
      "Epoch 18/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4684 - acc: 0.8494 - val_loss: 0.4688 - val_acc: 0.8506\n",
      "Epoch 19/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4677 - acc: 0.8501 - val_loss: 0.4683 - val_acc: 0.8486\n",
      "Epoch 20/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4667 - acc: 0.8496 - val_loss: 0.4659 - val_acc: 0.8498\n",
      "Epoch 21/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4659 - acc: 0.8500 - val_loss: 0.4654 - val_acc: 0.8499\n",
      "Epoch 22/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4655 - acc: 0.8499 - val_loss: 0.4639 - val_acc: 0.8504\n",
      "Epoch 23/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4649 - acc: 0.8497 - val_loss: 0.4632 - val_acc: 0.8500\n",
      "Epoch 24/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4642 - acc: 0.8504 - val_loss: 0.4638 - val_acc: 0.8504\n",
      "Epoch 25/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4636 - acc: 0.8502 - val_loss: 0.4641 - val_acc: 0.8492\n",
      "Epoch 26/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4633 - acc: 0.8504 - val_loss: 0.4619 - val_acc: 0.8503\n",
      "Epoch 27/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4631 - acc: 0.8504 - val_loss: 0.4642 - val_acc: 0.8490\n",
      "Epoch 28/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4625 - acc: 0.8509 - val_loss: 0.4601 - val_acc: 0.8505\n",
      "Epoch 29/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4619 - acc: 0.8506 - val_loss: 0.4602 - val_acc: 0.8503\n",
      "Epoch 30/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4614 - acc: 0.8506 - val_loss: 0.4601 - val_acc: 0.8514\n",
      "Epoch 31/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4607 - acc: 0.8506 - val_loss: 0.4616 - val_acc: 0.8511\n",
      "Epoch 32/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4603 - acc: 0.8515 - val_loss: 0.4594 - val_acc: 0.8502\n",
      "Epoch 33/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4599 - acc: 0.8510 - val_loss: 0.4639 - val_acc: 0.8494\n",
      "Epoch 34/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4595 - acc: 0.8516 - val_loss: 0.4590 - val_acc: 0.8505\n",
      "Epoch 35/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4594 - acc: 0.8517 - val_loss: 0.4583 - val_acc: 0.8520\n",
      "Epoch 36/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4588 - acc: 0.8517 - val_loss: 0.4567 - val_acc: 0.8519\n",
      "Epoch 37/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4587 - acc: 0.8514 - val_loss: 0.4565 - val_acc: 0.8524\n",
      "Epoch 38/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4583 - acc: 0.8517 - val_loss: 0.4572 - val_acc: 0.8518\n",
      "Epoch 39/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4577 - acc: 0.8508 - val_loss: 0.4559 - val_acc: 0.8522\n",
      "Epoch 40/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4576 - acc: 0.8512 - val_loss: 0.4591 - val_acc: 0.8515\n",
      "Epoch 41/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4572 - acc: 0.8513 - val_loss: 0.4626 - val_acc: 0.8521\n",
      "Epoch 42/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4573 - acc: 0.8521 - val_loss: 0.4584 - val_acc: 0.8519\n",
      "Epoch 43/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4567 - acc: 0.8520 - val_loss: 0.4561 - val_acc: 0.8518\n",
      "Epoch 44/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4564 - acc: 0.8522 - val_loss: 0.4554 - val_acc: 0.8524\n",
      "Epoch 45/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4562 - acc: 0.8516 - val_loss: 0.4548 - val_acc: 0.8527\n",
      "Epoch 46/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4560 - acc: 0.8518 - val_loss: 0.4539 - val_acc: 0.8527\n",
      "Epoch 47/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4557 - acc: 0.8521 - val_loss: 0.4562 - val_acc: 0.8514\n",
      "Epoch 48/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4558 - acc: 0.8521 - val_loss: 0.4558 - val_acc: 0.8515\n",
      "Epoch 49/100\n",
      "81250/81250 [==============================] - 1s 11us/step - loss: 0.4552 - acc: 0.8521 - val_loss: 0.4554 - val_acc: 0.8525\n",
      "Epoch 50/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4553 - acc: 0.8525 - val_loss: 0.4536 - val_acc: 0.8522\n",
      "Epoch 51/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4550 - acc: 0.8520 - val_loss: 0.4532 - val_acc: 0.8521\n",
      "Epoch 52/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4549 - acc: 0.8523 - val_loss: 0.4546 - val_acc: 0.8520\n",
      "Epoch 53/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4548 - acc: 0.8520 - val_loss: 0.4571 - val_acc: 0.8507\n",
      "Epoch 54/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4546 - acc: 0.8521 - val_loss: 0.4539 - val_acc: 0.8529\n",
      "Epoch 55/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4546 - acc: 0.8524 - val_loss: 0.4520 - val_acc: 0.8527\n",
      "Epoch 56/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4543 - acc: 0.8524 - val_loss: 0.4529 - val_acc: 0.8532\n",
      "Epoch 57/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4539 - acc: 0.8525 - val_loss: 0.4569 - val_acc: 0.8530\n",
      "Epoch 58/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4540 - acc: 0.8524 - val_loss: 0.4516 - val_acc: 0.8528\n",
      "Epoch 59/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4536 - acc: 0.8522 - val_loss: 0.4598 - val_acc: 0.8511\n",
      "Epoch 60/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4535 - acc: 0.8520 - val_loss: 0.4514 - val_acc: 0.8532\n",
      "Epoch 61/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4534 - acc: 0.8526 - val_loss: 0.4544 - val_acc: 0.8516\n",
      "Epoch 62/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4533 - acc: 0.8522 - val_loss: 0.4517 - val_acc: 0.8527\n",
      "Epoch 63/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4530 - acc: 0.8526 - val_loss: 0.4558 - val_acc: 0.8503\n",
      "Epoch 64/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4526 - acc: 0.8526 - val_loss: 0.4520 - val_acc: 0.8521\n",
      "Epoch 65/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4528 - acc: 0.8526 - val_loss: 0.4517 - val_acc: 0.8523\n",
      "Epoch 66/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4527 - acc: 0.8516 - val_loss: 0.4520 - val_acc: 0.8529\n",
      "Epoch 67/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4525 - acc: 0.8528 - val_loss: 0.4508 - val_acc: 0.8536\n",
      "Epoch 68/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4525 - acc: 0.8524 - val_loss: 0.4540 - val_acc: 0.8525\n",
      "Epoch 69/100\n",
      "81250/81250 [==============================] - 2s 23us/step - loss: 0.4521 - acc: 0.8526 - val_loss: 0.4504 - val_acc: 0.8536\n",
      "Epoch 70/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4523 - acc: 0.8523 - val_loss: 0.4514 - val_acc: 0.8532\n",
      "Epoch 71/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4518 - acc: 0.8531 - val_loss: 0.4513 - val_acc: 0.8526\n",
      "Epoch 72/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4522 - acc: 0.8530 - val_loss: 0.4536 - val_acc: 0.8517\n",
      "Epoch 73/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4518 - acc: 0.8524 - val_loss: 0.4497 - val_acc: 0.8531\n",
      "Epoch 74/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4517 - acc: 0.8528 - val_loss: 0.4494 - val_acc: 0.8536\n",
      "Epoch 75/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4514 - acc: 0.8525 - val_loss: 0.4521 - val_acc: 0.8530\n",
      "Epoch 76/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4512 - acc: 0.8528 - val_loss: 0.4530 - val_acc: 0.8532\n",
      "Epoch 77/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4512 - acc: 0.8532 - val_loss: 0.4499 - val_acc: 0.8527\n",
      "Epoch 78/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4513 - acc: 0.8532 - val_loss: 0.4521 - val_acc: 0.8526\n",
      "Epoch 79/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4511 - acc: 0.8532 - val_loss: 0.4508 - val_acc: 0.8525\n",
      "Epoch 80/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4510 - acc: 0.8528 - val_loss: 0.4498 - val_acc: 0.8539\n",
      "Epoch 81/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4509 - acc: 0.8528 - val_loss: 0.4501 - val_acc: 0.8533\n",
      "Epoch 82/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4505 - acc: 0.8536 - val_loss: 0.4487 - val_acc: 0.8529\n",
      "Epoch 83/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4504 - acc: 0.8528 - val_loss: 0.4500 - val_acc: 0.8536\n",
      "Epoch 84/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4501 - acc: 0.8531 - val_loss: 0.4485 - val_acc: 0.8536\n",
      "Epoch 85/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4503 - acc: 0.8532 - val_loss: 0.4500 - val_acc: 0.8526\n",
      "Epoch 86/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4500 - acc: 0.8527 - val_loss: 0.4493 - val_acc: 0.8530\n",
      "Epoch 87/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4500 - acc: 0.8538 - val_loss: 0.4480 - val_acc: 0.8538\n",
      "Epoch 88/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4503 - acc: 0.8534 - val_loss: 0.4513 - val_acc: 0.8524\n",
      "Epoch 89/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4495 - acc: 0.8530 - val_loss: 0.4511 - val_acc: 0.8519\n",
      "Epoch 90/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4498 - acc: 0.8536 - val_loss: 0.4479 - val_acc: 0.8539\n",
      "Epoch 91/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4496 - acc: 0.8534 - val_loss: 0.4480 - val_acc: 0.8540\n",
      "Epoch 92/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4494 - acc: 0.8531 - val_loss: 0.4490 - val_acc: 0.8528\n",
      "Epoch 93/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4496 - acc: 0.8529 - val_loss: 0.4487 - val_acc: 0.8529\n",
      "Epoch 94/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4494 - acc: 0.8532 - val_loss: 0.4481 - val_acc: 0.8531\n",
      "Epoch 95/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4489 - acc: 0.8535 - val_loss: 0.4556 - val_acc: 0.8503\n",
      "Epoch 96/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4493 - acc: 0.8530 - val_loss: 0.4480 - val_acc: 0.8533\n",
      "Epoch 97/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4490 - acc: 0.8535 - val_loss: 0.4470 - val_acc: 0.8537\n",
      "Epoch 98/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4488 - acc: 0.8535 - val_loss: 0.4516 - val_acc: 0.8523\n",
      "Epoch 99/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4490 - acc: 0.8533 - val_loss: 0.4482 - val_acc: 0.8537\n",
      "Epoch 100/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4490 - acc: 0.8535 - val_loss: 0.4480 - val_acc: 0.8541\n",
      "81251/81251 [==============================] - 6s 73us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81250 samples, validate on 81250 samples\n",
      "Epoch 1/100\n",
      "81250/81250 [==============================] - 5s 60us/step - loss: 1.0047 - acc: 0.7223 - val_loss: 0.6498 - val_acc: 0.8179\n",
      "Epoch 2/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.6154 - acc: 0.8255 - val_loss: 0.5979 - val_acc: 0.8277\n",
      "Epoch 3/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.5841 - acc: 0.8308 - val_loss: 0.5721 - val_acc: 0.8311\n",
      "Epoch 4/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.5608 - acc: 0.8326 - val_loss: 0.5475 - val_acc: 0.8336\n",
      "Epoch 5/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.5381 - acc: 0.8346 - val_loss: 0.5282 - val_acc: 0.8354\n",
      "Epoch 6/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.5232 - acc: 0.8356 - val_loss: 0.5171 - val_acc: 0.8358\n",
      "Epoch 7/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.5128 - acc: 0.8363 - val_loss: 0.5072 - val_acc: 0.8367\n",
      "Epoch 8/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.5045 - acc: 0.8370 - val_loss: 0.5005 - val_acc: 0.8369\n",
      "Epoch 9/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4990 - acc: 0.8377 - val_loss: 0.4951 - val_acc: 0.8384\n",
      "Epoch 10/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4941 - acc: 0.8393 - val_loss: 0.4901 - val_acc: 0.8414\n",
      "Epoch 11/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4900 - acc: 0.8418 - val_loss: 0.4878 - val_acc: 0.8414\n",
      "Epoch 12/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4867 - acc: 0.8432 - val_loss: 0.4857 - val_acc: 0.8438\n",
      "Epoch 13/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4843 - acc: 0.8443 - val_loss: 0.4814 - val_acc: 0.8457\n",
      "Epoch 14/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4818 - acc: 0.8455 - val_loss: 0.4799 - val_acc: 0.8465\n",
      "Epoch 15/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4800 - acc: 0.8457 - val_loss: 0.4791 - val_acc: 0.8469\n",
      "Epoch 16/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4785 - acc: 0.8467 - val_loss: 0.4769 - val_acc: 0.8466\n",
      "Epoch 17/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4769 - acc: 0.8472 - val_loss: 0.4746 - val_acc: 0.8483\n",
      "Epoch 18/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4757 - acc: 0.8478 - val_loss: 0.4735 - val_acc: 0.8482\n",
      "Epoch 19/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4745 - acc: 0.8482 - val_loss: 0.4733 - val_acc: 0.8472\n",
      "Epoch 20/100\n",
      "81250/81250 [==============================] - 2s 25us/step - loss: 0.4736 - acc: 0.8484 - val_loss: 0.4733 - val_acc: 0.8477\n",
      "Epoch 21/100\n",
      "81250/81250 [==============================] - 3s 41us/step - loss: 0.4727 - acc: 0.8480 - val_loss: 0.4735 - val_acc: 0.8481\n",
      "Epoch 22/100\n",
      "81250/81250 [==============================] - 2s 24us/step - loss: 0.4718 - acc: 0.8490 - val_loss: 0.4732 - val_acc: 0.8475\n",
      "Epoch 23/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4708 - acc: 0.8487 - val_loss: 0.4708 - val_acc: 0.8481\n",
      "Epoch 24/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4703 - acc: 0.8488 - val_loss: 0.4710 - val_acc: 0.8498\n",
      "Epoch 25/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4693 - acc: 0.8493 - val_loss: 0.4685 - val_acc: 0.8496\n",
      "Epoch 26/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4693 - acc: 0.8495 - val_loss: 0.4677 - val_acc: 0.8495\n",
      "Epoch 27/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4685 - acc: 0.8495 - val_loss: 0.4664 - val_acc: 0.8505\n",
      "Epoch 28/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4676 - acc: 0.8493 - val_loss: 0.4677 - val_acc: 0.8500\n",
      "Epoch 29/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4671 - acc: 0.8492 - val_loss: 0.4656 - val_acc: 0.8498\n",
      "Epoch 30/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4668 - acc: 0.8499 - val_loss: 0.4648 - val_acc: 0.8502\n",
      "Epoch 31/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4664 - acc: 0.8500 - val_loss: 0.4642 - val_acc: 0.8504\n",
      "Epoch 32/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4656 - acc: 0.8501 - val_loss: 0.4689 - val_acc: 0.8486\n",
      "Epoch 33/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4651 - acc: 0.8498 - val_loss: 0.4653 - val_acc: 0.8496\n",
      "Epoch 34/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4650 - acc: 0.8500 - val_loss: 0.4629 - val_acc: 0.8513\n",
      "Epoch 35/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4646 - acc: 0.8500 - val_loss: 0.4628 - val_acc: 0.8509\n",
      "Epoch 36/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4646 - acc: 0.8502 - val_loss: 0.4630 - val_acc: 0.8507\n",
      "Epoch 37/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4640 - acc: 0.8506 - val_loss: 0.4631 - val_acc: 0.8507\n",
      "Epoch 38/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4634 - acc: 0.8503 - val_loss: 0.4663 - val_acc: 0.8491\n",
      "Epoch 39/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4631 - acc: 0.8504 - val_loss: 0.4615 - val_acc: 0.8512\n",
      "Epoch 40/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4628 - acc: 0.8510 - val_loss: 0.4612 - val_acc: 0.8513\n",
      "Epoch 41/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4627 - acc: 0.8508 - val_loss: 0.4607 - val_acc: 0.8515\n",
      "Epoch 42/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4621 - acc: 0.8508 - val_loss: 0.4602 - val_acc: 0.8513\n",
      "Epoch 43/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4621 - acc: 0.8511 - val_loss: 0.4599 - val_acc: 0.8512\n",
      "Epoch 44/100\n",
      "81250/81250 [==============================] - 1s 12us/step - loss: 0.4617 - acc: 0.8515 - val_loss: 0.4608 - val_acc: 0.8513\n",
      "Epoch 45/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4615 - acc: 0.8508 - val_loss: 0.4595 - val_acc: 0.8512\n",
      "Epoch 46/100\n",
      "81250/81250 [==============================] - 2s 25us/step - loss: 0.4609 - acc: 0.8514 - val_loss: 0.4602 - val_acc: 0.8508\n",
      "Epoch 47/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4610 - acc: 0.8511 - val_loss: 0.4637 - val_acc: 0.8499\n",
      "Epoch 48/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4608 - acc: 0.8515 - val_loss: 0.4623 - val_acc: 0.8504\n",
      "Epoch 49/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4606 - acc: 0.8515 - val_loss: 0.4590 - val_acc: 0.8516\n",
      "Epoch 50/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4600 - acc: 0.8514 - val_loss: 0.4616 - val_acc: 0.8505\n",
      "Epoch 51/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4601 - acc: 0.8513 - val_loss: 0.4597 - val_acc: 0.8510\n",
      "Epoch 52/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4600 - acc: 0.8508 - val_loss: 0.4595 - val_acc: 0.8512\n",
      "Epoch 53/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4596 - acc: 0.8515 - val_loss: 0.4582 - val_acc: 0.8520\n",
      "Epoch 54/100\n",
      "81250/81250 [==============================] - 1s 16us/step - loss: 0.4592 - acc: 0.8512 - val_loss: 0.4610 - val_acc: 0.8508\n",
      "Epoch 55/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4595 - acc: 0.8515 - val_loss: 0.4579 - val_acc: 0.8514\n",
      "Epoch 56/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4590 - acc: 0.8516 - val_loss: 0.4576 - val_acc: 0.8516\n",
      "Epoch 57/100\n",
      "81250/81250 [==============================] - 1s 13us/step - loss: 0.4587 - acc: 0.8515 - val_loss: 0.4577 - val_acc: 0.8517\n",
      "Epoch 58/100\n",
      "81250/81250 [==============================] - 1s 14us/step - loss: 0.4587 - acc: 0.8517 - val_loss: 0.4586 - val_acc: 0.8513\n",
      "Epoch 59/100\n",
      "81250/81250 [==============================] - 1s 15us/step - loss: 0.4584 - acc: 0.8518 - val_loss: 0.4568 - val_acc: 0.8520\n",
      "Epoch 60/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4582 - acc: 0.8516 - val_loss: 0.4580 - val_acc: 0.8522\n",
      "Epoch 61/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4582 - acc: 0.8513 - val_loss: 0.4561 - val_acc: 0.8522\n",
      "Epoch 62/100\n",
      "81250/81250 [==============================] - 2s 18us/step - loss: 0.4579 - acc: 0.8515 - val_loss: 0.4565 - val_acc: 0.8521\n",
      "Epoch 63/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4579 - acc: 0.8519 - val_loss: 0.4571 - val_acc: 0.8517\n",
      "Epoch 64/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4577 - acc: 0.8516 - val_loss: 0.4569 - val_acc: 0.8515\n",
      "Epoch 65/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4574 - acc: 0.8515 - val_loss: 0.4563 - val_acc: 0.8525\n",
      "Epoch 66/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4571 - acc: 0.8520 - val_loss: 0.4575 - val_acc: 0.8509\n",
      "Epoch 67/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4574 - acc: 0.8513 - val_loss: 0.4557 - val_acc: 0.8524\n",
      "Epoch 68/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4575 - acc: 0.8521 - val_loss: 0.4587 - val_acc: 0.8514\n",
      "Epoch 69/100\n",
      "81250/81250 [==============================] - 2s 23us/step - loss: 0.4570 - acc: 0.8518 - val_loss: 0.4562 - val_acc: 0.8517\n",
      "Epoch 70/100\n",
      "81250/81250 [==============================] - 2s 22us/step - loss: 0.4566 - acc: 0.8523 - val_loss: 0.4555 - val_acc: 0.8525\n",
      "Epoch 71/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4568 - acc: 0.8514 - val_loss: 0.4595 - val_acc: 0.8514\n",
      "Epoch 72/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4566 - acc: 0.8517 - val_loss: 0.4559 - val_acc: 0.8522\n",
      "Epoch 73/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4564 - acc: 0.8521 - val_loss: 0.4591 - val_acc: 0.8516\n",
      "Epoch 74/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4562 - acc: 0.8517 - val_loss: 0.4548 - val_acc: 0.8520\n",
      "Epoch 75/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4561 - acc: 0.8516 - val_loss: 0.4574 - val_acc: 0.8520\n",
      "Epoch 76/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4561 - acc: 0.8521 - val_loss: 0.4566 - val_acc: 0.8515\n",
      "Epoch 77/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4564 - acc: 0.8522 - val_loss: 0.4561 - val_acc: 0.8514\n",
      "Epoch 78/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4558 - acc: 0.8520 - val_loss: 0.4544 - val_acc: 0.8520\n",
      "Epoch 79/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4559 - acc: 0.8519 - val_loss: 0.4563 - val_acc: 0.8515\n",
      "Epoch 80/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4557 - acc: 0.8521 - val_loss: 0.4562 - val_acc: 0.8518\n",
      "Epoch 81/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4556 - acc: 0.8522 - val_loss: 0.4559 - val_acc: 0.8520\n",
      "Epoch 82/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4556 - acc: 0.8519 - val_loss: 0.4551 - val_acc: 0.8523\n",
      "Epoch 83/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4552 - acc: 0.8521 - val_loss: 0.4555 - val_acc: 0.8519\n",
      "Epoch 84/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4551 - acc: 0.8526 - val_loss: 0.4560 - val_acc: 0.8522\n",
      "Epoch 85/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4550 - acc: 0.8524 - val_loss: 0.4536 - val_acc: 0.8524\n",
      "Epoch 86/100\n",
      "81250/81250 [==============================] - 2s 24us/step - loss: 0.4548 - acc: 0.8525 - val_loss: 0.4537 - val_acc: 0.8520\n",
      "Epoch 87/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4548 - acc: 0.8522 - val_loss: 0.4546 - val_acc: 0.8523\n",
      "Epoch 88/100\n",
      "81250/81250 [==============================] - 2s 22us/step - loss: 0.4549 - acc: 0.8522 - val_loss: 0.4574 - val_acc: 0.8514\n",
      "Epoch 89/100\n",
      "81250/81250 [==============================] - 2s 26us/step - loss: 0.4546 - acc: 0.8523 - val_loss: 0.4542 - val_acc: 0.8525\n",
      "Epoch 90/100\n",
      "81250/81250 [==============================] - 2s 23us/step - loss: 0.4546 - acc: 0.8527 - val_loss: 0.4535 - val_acc: 0.8523\n",
      "Epoch 91/100\n",
      "81250/81250 [==============================] - 2s 23us/step - loss: 0.4544 - acc: 0.8524 - val_loss: 0.4544 - val_acc: 0.8525\n",
      "Epoch 92/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4547 - acc: 0.8521 - val_loss: 0.4522 - val_acc: 0.8531\n",
      "Epoch 93/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4541 - acc: 0.8527 - val_loss: 0.4557 - val_acc: 0.8522\n",
      "Epoch 94/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4543 - acc: 0.8521 - val_loss: 0.4538 - val_acc: 0.8524\n",
      "Epoch 95/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4544 - acc: 0.8525 - val_loss: 0.4539 - val_acc: 0.8522\n",
      "Epoch 96/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4539 - acc: 0.8527 - val_loss: 0.4538 - val_acc: 0.8524\n",
      "Epoch 97/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4539 - acc: 0.8527 - val_loss: 0.4530 - val_acc: 0.8528\n",
      "Epoch 98/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4541 - acc: 0.8521 - val_loss: 0.4549 - val_acc: 0.8517\n",
      "Epoch 99/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4540 - acc: 0.8520 - val_loss: 0.4551 - val_acc: 0.8525\n",
      "Epoch 100/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4538 - acc: 0.8524 - val_loss: 0.4541 - val_acc: 0.8528\n",
      "81251/81251 [==============================] - 9s 117us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81250 samples, validate on 81250 samples\n",
      "Epoch 1/100\n",
      "81250/81250 [==============================] - 6s 71us/step - loss: 0.9318 - acc: 0.7393 - val_loss: 0.6431 - val_acc: 0.8205\n",
      "Epoch 2/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.6140 - acc: 0.8256 - val_loss: 0.5965 - val_acc: 0.8272\n",
      "Epoch 3/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.5872 - acc: 0.8290 - val_loss: 0.5771 - val_acc: 0.8304\n",
      "Epoch 4/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.5701 - acc: 0.8308 - val_loss: 0.5610 - val_acc: 0.8318\n",
      "Epoch 5/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.5551 - acc: 0.8323 - val_loss: 0.5475 - val_acc: 0.8331\n",
      "Epoch 6/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.5419 - acc: 0.8331 - val_loss: 0.5351 - val_acc: 0.8334\n",
      "Epoch 7/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.5304 - acc: 0.8338 - val_loss: 0.5238 - val_acc: 0.8351\n",
      "Epoch 8/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.5170 - acc: 0.8347 - val_loss: 0.5085 - val_acc: 0.8354\n",
      "Epoch 9/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.5048 - acc: 0.8365 - val_loss: 0.4995 - val_acc: 0.8361\n",
      "Epoch 10/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4976 - acc: 0.8385 - val_loss: 0.4945 - val_acc: 0.8400\n",
      "Epoch 11/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4921 - acc: 0.8412 - val_loss: 0.4897 - val_acc: 0.8438\n",
      "Epoch 12/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4879 - acc: 0.8428 - val_loss: 0.4857 - val_acc: 0.8442\n",
      "Epoch 13/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4848 - acc: 0.8444 - val_loss: 0.4820 - val_acc: 0.8442\n",
      "Epoch 14/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4821 - acc: 0.8448 - val_loss: 0.4792 - val_acc: 0.8462\n",
      "Epoch 15/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4802 - acc: 0.8457 - val_loss: 0.4779 - val_acc: 0.8471\n",
      "Epoch 16/100\n",
      "81250/81250 [==============================] - 2s 22us/step - loss: 0.4784 - acc: 0.8457 - val_loss: 0.4784 - val_acc: 0.8456\n",
      "Epoch 17/100\n",
      "81250/81250 [==============================] - 2s 22us/step - loss: 0.4769 - acc: 0.8469 - val_loss: 0.4766 - val_acc: 0.8470\n",
      "Epoch 18/100\n",
      "81250/81250 [==============================] - 2s 24us/step - loss: 0.4760 - acc: 0.8464 - val_loss: 0.4751 - val_acc: 0.8474\n",
      "Epoch 19/100\n",
      "81250/81250 [==============================] - 2s 21us/step - loss: 0.4747 - acc: 0.8473 - val_loss: 0.4747 - val_acc: 0.8468\n",
      "Epoch 20/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4736 - acc: 0.8471 - val_loss: 0.4716 - val_acc: 0.8480\n",
      "Epoch 21/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4727 - acc: 0.8473 - val_loss: 0.4727 - val_acc: 0.8482\n",
      "Epoch 22/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4720 - acc: 0.8476 - val_loss: 0.4712 - val_acc: 0.8477\n",
      "Epoch 23/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4714 - acc: 0.8475 - val_loss: 0.4694 - val_acc: 0.8485\n",
      "Epoch 24/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4708 - acc: 0.8472 - val_loss: 0.4701 - val_acc: 0.8485\n",
      "Epoch 25/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4701 - acc: 0.8477 - val_loss: 0.4687 - val_acc: 0.8485\n",
      "Epoch 26/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4694 - acc: 0.8480 - val_loss: 0.4725 - val_acc: 0.8487\n",
      "Epoch 27/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4694 - acc: 0.8483 - val_loss: 0.4679 - val_acc: 0.8480\n",
      "Epoch 28/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4687 - acc: 0.8481 - val_loss: 0.4668 - val_acc: 0.8490\n",
      "Epoch 29/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4681 - acc: 0.8484 - val_loss: 0.4685 - val_acc: 0.8480\n",
      "Epoch 30/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4682 - acc: 0.8487 - val_loss: 0.4666 - val_acc: 0.8484\n",
      "Epoch 31/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4675 - acc: 0.8486 - val_loss: 0.4650 - val_acc: 0.8490\n",
      "Epoch 32/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4670 - acc: 0.8487 - val_loss: 0.4678 - val_acc: 0.8486\n",
      "Epoch 33/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4668 - acc: 0.8490 - val_loss: 0.4695 - val_acc: 0.8480\n",
      "Epoch 34/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4667 - acc: 0.8489 - val_loss: 0.4659 - val_acc: 0.8486\n",
      "Epoch 35/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4662 - acc: 0.8494 - val_loss: 0.4662 - val_acc: 0.8495\n",
      "Epoch 36/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4659 - acc: 0.8487 - val_loss: 0.4663 - val_acc: 0.8492\n",
      "Epoch 37/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4657 - acc: 0.8489 - val_loss: 0.4722 - val_acc: 0.8467\n",
      "Epoch 38/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4654 - acc: 0.8491 - val_loss: 0.4647 - val_acc: 0.8492\n",
      "Epoch 39/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4652 - acc: 0.8490 - val_loss: 0.4654 - val_acc: 0.8489\n",
      "Epoch 40/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4653 - acc: 0.8489 - val_loss: 0.4640 - val_acc: 0.8499\n",
      "Epoch 41/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4647 - acc: 0.8494 - val_loss: 0.4643 - val_acc: 0.8497\n",
      "Epoch 42/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4649 - acc: 0.8493 - val_loss: 0.4678 - val_acc: 0.8489\n",
      "Epoch 43/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4645 - acc: 0.8494 - val_loss: 0.4629 - val_acc: 0.8493\n",
      "Epoch 44/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4638 - acc: 0.8491 - val_loss: 0.4626 - val_acc: 0.8501\n",
      "Epoch 45/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4638 - acc: 0.8494 - val_loss: 0.4628 - val_acc: 0.8504\n",
      "Epoch 46/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4640 - acc: 0.8497 - val_loss: 0.4624 - val_acc: 0.8489\n",
      "Epoch 47/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4634 - acc: 0.8492 - val_loss: 0.4625 - val_acc: 0.8492\n",
      "Epoch 48/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4630 - acc: 0.8496 - val_loss: 0.4611 - val_acc: 0.8504\n",
      "Epoch 49/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4632 - acc: 0.8500 - val_loss: 0.4620 - val_acc: 0.8507\n",
      "Epoch 50/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4627 - acc: 0.8494 - val_loss: 0.4609 - val_acc: 0.8504\n",
      "Epoch 51/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4626 - acc: 0.8499 - val_loss: 0.4642 - val_acc: 0.8478\n",
      "Epoch 52/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4619 - acc: 0.8496 - val_loss: 0.4630 - val_acc: 0.8497\n",
      "Epoch 53/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4622 - acc: 0.8502 - val_loss: 0.4628 - val_acc: 0.8500\n",
      "Epoch 54/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4617 - acc: 0.8504 - val_loss: 0.4599 - val_acc: 0.8507\n",
      "Epoch 55/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4613 - acc: 0.8498 - val_loss: 0.4600 - val_acc: 0.8496\n",
      "Epoch 56/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4613 - acc: 0.8500 - val_loss: 0.4616 - val_acc: 0.8495\n",
      "Epoch 57/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4612 - acc: 0.8501 - val_loss: 0.4603 - val_acc: 0.8499\n",
      "Epoch 58/100\n",
      "81250/81250 [==============================] - 2s 20us/step - loss: 0.4615 - acc: 0.8499 - val_loss: 0.4587 - val_acc: 0.8509\n",
      "Epoch 59/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4611 - acc: 0.8500 - val_loss: 0.4593 - val_acc: 0.8508\n",
      "Epoch 60/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4607 - acc: 0.8506 - val_loss: 0.4587 - val_acc: 0.8505\n",
      "Epoch 61/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4606 - acc: 0.8502 - val_loss: 0.4591 - val_acc: 0.8507\n",
      "Epoch 62/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4604 - acc: 0.8504 - val_loss: 0.4638 - val_acc: 0.8486\n",
      "Epoch 63/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4603 - acc: 0.8503 - val_loss: 0.4592 - val_acc: 0.8511\n",
      "Epoch 64/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4598 - acc: 0.8502 - val_loss: 0.4586 - val_acc: 0.8502\n",
      "Epoch 65/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4600 - acc: 0.8504 - val_loss: 0.4585 - val_acc: 0.8513\n",
      "Epoch 66/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4596 - acc: 0.8507 - val_loss: 0.4607 - val_acc: 0.8489\n",
      "Epoch 67/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4597 - acc: 0.8500 - val_loss: 0.4592 - val_acc: 0.8502\n",
      "Epoch 68/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4595 - acc: 0.8504 - val_loss: 0.4601 - val_acc: 0.8493\n",
      "Epoch 69/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4590 - acc: 0.8507 - val_loss: 0.4646 - val_acc: 0.8480\n",
      "Epoch 70/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4590 - acc: 0.8501 - val_loss: 0.4580 - val_acc: 0.8510\n",
      "Epoch 71/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4589 - acc: 0.8505 - val_loss: 0.4574 - val_acc: 0.8508\n",
      "Epoch 72/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4587 - acc: 0.8508 - val_loss: 0.4606 - val_acc: 0.8504\n",
      "Epoch 73/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4587 - acc: 0.8506 - val_loss: 0.4564 - val_acc: 0.8509\n",
      "Epoch 74/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4590 - acc: 0.8502 - val_loss: 0.4566 - val_acc: 0.8520\n",
      "Epoch 75/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4581 - acc: 0.8501 - val_loss: 0.4578 - val_acc: 0.8516\n",
      "Epoch 76/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4584 - acc: 0.8505 - val_loss: 0.4610 - val_acc: 0.8501\n",
      "Epoch 77/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4582 - acc: 0.8510 - val_loss: 0.4604 - val_acc: 0.8498\n",
      "Epoch 78/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4582 - acc: 0.8508 - val_loss: 0.4576 - val_acc: 0.8516\n",
      "Epoch 79/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4577 - acc: 0.8506 - val_loss: 0.4574 - val_acc: 0.8509\n",
      "Epoch 80/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4578 - acc: 0.8507 - val_loss: 0.4574 - val_acc: 0.8513\n",
      "Epoch 81/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4576 - acc: 0.8508 - val_loss: 0.4565 - val_acc: 0.8512\n",
      "Epoch 82/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4575 - acc: 0.8509 - val_loss: 0.4563 - val_acc: 0.8517\n",
      "Epoch 83/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4575 - acc: 0.8513 - val_loss: 0.4561 - val_acc: 0.8512\n",
      "Epoch 84/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4572 - acc: 0.8509 - val_loss: 0.4577 - val_acc: 0.8512\n",
      "Epoch 85/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4572 - acc: 0.8507 - val_loss: 0.4562 - val_acc: 0.8510\n",
      "Epoch 86/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4572 - acc: 0.8503 - val_loss: 0.4550 - val_acc: 0.8510\n",
      "Epoch 87/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4570 - acc: 0.8508 - val_loss: 0.4564 - val_acc: 0.8506\n",
      "Epoch 88/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4571 - acc: 0.8504 - val_loss: 0.4550 - val_acc: 0.8513\n",
      "Epoch 89/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4571 - acc: 0.8511 - val_loss: 0.4594 - val_acc: 0.8501\n",
      "Epoch 90/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4567 - acc: 0.8512 - val_loss: 0.4551 - val_acc: 0.8517\n",
      "Epoch 91/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4565 - acc: 0.8505 - val_loss: 0.4576 - val_acc: 0.8506\n",
      "Epoch 92/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4567 - acc: 0.8513 - val_loss: 0.4551 - val_acc: 0.8509\n",
      "Epoch 93/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4562 - acc: 0.8508 - val_loss: 0.4576 - val_acc: 0.8510\n",
      "Epoch 94/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4562 - acc: 0.8514 - val_loss: 0.4556 - val_acc: 0.8499\n",
      "Epoch 95/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4563 - acc: 0.8508 - val_loss: 0.4581 - val_acc: 0.8505\n",
      "Epoch 96/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4563 - acc: 0.8510 - val_loss: 0.4556 - val_acc: 0.8513\n",
      "Epoch 97/100\n",
      "81250/81250 [==============================] - 1s 17us/step - loss: 0.4559 - acc: 0.8511 - val_loss: 0.4548 - val_acc: 0.8507\n",
      "Epoch 98/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4558 - acc: 0.8513 - val_loss: 0.4551 - val_acc: 0.8517\n",
      "Epoch 99/100\n",
      "81250/81250 [==============================] - 1s 18us/step - loss: 0.4559 - acc: 0.8506 - val_loss: 0.4547 - val_acc: 0.8513\n",
      "Epoch 100/100\n",
      "81250/81250 [==============================] - 2s 19us/step - loss: 0.4559 - acc: 0.8508 - val_loss: 0.4554 - val_acc: 0.8514\n",
      "81251/81251 [==============================] - 10s 118us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32500 samples, validate on 32500 samples\n",
      "Epoch 1/100\n",
      "32500/32500 [==============================] - 5s 144us/step - loss: 1.2899 - acc: 0.6969 - val_loss: 0.7973 - val_acc: 0.7929\n",
      "Epoch 2/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.7116 - acc: 0.8118 - val_loss: 0.6613 - val_acc: 0.8227\n",
      "Epoch 3/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.6410 - acc: 0.8245 - val_loss: 0.6248 - val_acc: 0.8270\n",
      "Epoch 4/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.6178 - acc: 0.8277 - val_loss: 0.6091 - val_acc: 0.8289\n",
      "Epoch 5/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.6045 - acc: 0.8288 - val_loss: 0.5989 - val_acc: 0.8315\n",
      "Epoch 6/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5945 - acc: 0.8298 - val_loss: 0.5891 - val_acc: 0.8315\n",
      "Epoch 7/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.5862 - acc: 0.8307 - val_loss: 0.5865 - val_acc: 0.8294\n",
      "Epoch 8/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5789 - acc: 0.8317 - val_loss: 0.5741 - val_acc: 0.8325\n",
      "Epoch 9/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5717 - acc: 0.8320 - val_loss: 0.5676 - val_acc: 0.8337\n",
      "Epoch 10/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.5652 - acc: 0.8325 - val_loss: 0.5613 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.5593 - acc: 0.8332 - val_loss: 0.5572 - val_acc: 0.8331\n",
      "Epoch 12/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5537 - acc: 0.8335 - val_loss: 0.5522 - val_acc: 0.8331\n",
      "Epoch 13/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5488 - acc: 0.8334 - val_loss: 0.5460 - val_acc: 0.8337\n",
      "Epoch 14/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5438 - acc: 0.8340 - val_loss: 0.5406 - val_acc: 0.8322\n",
      "Epoch 15/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5390 - acc: 0.8341 - val_loss: 0.5362 - val_acc: 0.8335\n",
      "Epoch 16/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5345 - acc: 0.8340 - val_loss: 0.5325 - val_acc: 0.8344\n",
      "Epoch 17/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5301 - acc: 0.8340 - val_loss: 0.5312 - val_acc: 0.8350\n",
      "Epoch 18/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5251 - acc: 0.8347 - val_loss: 0.5219 - val_acc: 0.8353\n",
      "Epoch 19/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5190 - acc: 0.8350 - val_loss: 0.5132 - val_acc: 0.8347\n",
      "Epoch 20/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5104 - acc: 0.8355 - val_loss: 0.5055 - val_acc: 0.8368\n",
      "Epoch 21/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5052 - acc: 0.8366 - val_loss: 0.5020 - val_acc: 0.8363\n",
      "Epoch 22/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5008 - acc: 0.8383 - val_loss: 0.4986 - val_acc: 0.8404\n",
      "Epoch 23/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4981 - acc: 0.8395 - val_loss: 0.4962 - val_acc: 0.8403\n",
      "Epoch 24/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4957 - acc: 0.8410 - val_loss: 0.4926 - val_acc: 0.8426\n",
      "Epoch 25/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4932 - acc: 0.8420 - val_loss: 0.4906 - val_acc: 0.8423\n",
      "Epoch 26/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4909 - acc: 0.8423 - val_loss: 0.4894 - val_acc: 0.8436\n",
      "Epoch 27/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4894 - acc: 0.8430 - val_loss: 0.4872 - val_acc: 0.8441\n",
      "Epoch 28/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4884 - acc: 0.8431 - val_loss: 0.4894 - val_acc: 0.8426\n",
      "Epoch 29/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4866 - acc: 0.8441 - val_loss: 0.4842 - val_acc: 0.8434\n",
      "Epoch 30/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4850 - acc: 0.8445 - val_loss: 0.4837 - val_acc: 0.8452\n",
      "Epoch 31/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4839 - acc: 0.8447 - val_loss: 0.4832 - val_acc: 0.8444\n",
      "Epoch 32/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4826 - acc: 0.8450 - val_loss: 0.4833 - val_acc: 0.8456\n",
      "Epoch 33/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4814 - acc: 0.8455 - val_loss: 0.4859 - val_acc: 0.8422\n",
      "Epoch 34/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4804 - acc: 0.8458 - val_loss: 0.4794 - val_acc: 0.8473\n",
      "Epoch 35/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4795 - acc: 0.8469 - val_loss: 0.4837 - val_acc: 0.8438\n",
      "Epoch 36/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4787 - acc: 0.8456 - val_loss: 0.4791 - val_acc: 0.8464\n",
      "Epoch 37/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4774 - acc: 0.8459 - val_loss: 0.4786 - val_acc: 0.8447\n",
      "Epoch 38/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4765 - acc: 0.8465 - val_loss: 0.4756 - val_acc: 0.8472\n",
      "Epoch 39/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4762 - acc: 0.8466 - val_loss: 0.4742 - val_acc: 0.8468\n",
      "Epoch 40/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4753 - acc: 0.8465 - val_loss: 0.4773 - val_acc: 0.8469\n",
      "Epoch 41/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4746 - acc: 0.8464 - val_loss: 0.4727 - val_acc: 0.8469\n",
      "Epoch 42/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4739 - acc: 0.8467 - val_loss: 0.4743 - val_acc: 0.8470\n",
      "Epoch 43/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4737 - acc: 0.8473 - val_loss: 0.4718 - val_acc: 0.8474\n",
      "Epoch 44/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4730 - acc: 0.8471 - val_loss: 0.4753 - val_acc: 0.8466\n",
      "Epoch 45/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4730 - acc: 0.8474 - val_loss: 0.4705 - val_acc: 0.8480\n",
      "Epoch 46/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4724 - acc: 0.8475 - val_loss: 0.4700 - val_acc: 0.8480\n",
      "Epoch 47/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4715 - acc: 0.8474 - val_loss: 0.4700 - val_acc: 0.8478\n",
      "Epoch 48/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4710 - acc: 0.8476 - val_loss: 0.4706 - val_acc: 0.8482\n",
      "Epoch 49/100\n",
      "32500/32500 [==============================] - 1s 17us/step - loss: 0.4707 - acc: 0.8476 - val_loss: 0.4709 - val_acc: 0.8472\n",
      "Epoch 50/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4699 - acc: 0.8487 - val_loss: 0.4721 - val_acc: 0.8473\n",
      "Epoch 51/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4701 - acc: 0.8478 - val_loss: 0.4708 - val_acc: 0.8486\n",
      "Epoch 52/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4701 - acc: 0.8480 - val_loss: 0.4688 - val_acc: 0.8492\n",
      "Epoch 53/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4691 - acc: 0.8486 - val_loss: 0.4692 - val_acc: 0.8486\n",
      "Epoch 54/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4693 - acc: 0.8484 - val_loss: 0.4681 - val_acc: 0.8480\n",
      "Epoch 55/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4690 - acc: 0.8488 - val_loss: 0.4681 - val_acc: 0.8481\n",
      "Epoch 56/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4687 - acc: 0.8479 - val_loss: 0.4672 - val_acc: 0.8494\n",
      "Epoch 57/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4687 - acc: 0.8484 - val_loss: 0.4684 - val_acc: 0.8488\n",
      "Epoch 58/100\n",
      "32500/32500 [==============================] - 1s 28us/step - loss: 0.4681 - acc: 0.8482 - val_loss: 0.4663 - val_acc: 0.8495\n",
      "Epoch 59/100\n",
      "32500/32500 [==============================] - 1s 32us/step - loss: 0.4677 - acc: 0.8488 - val_loss: 0.4666 - val_acc: 0.8487\n",
      "Epoch 60/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4675 - acc: 0.8485 - val_loss: 0.4660 - val_acc: 0.8497\n",
      "Epoch 61/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.4678 - acc: 0.8482 - val_loss: 0.4667 - val_acc: 0.8491\n",
      "Epoch 62/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4668 - acc: 0.8490 - val_loss: 0.4653 - val_acc: 0.8497\n",
      "Epoch 63/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4666 - acc: 0.8487 - val_loss: 0.4653 - val_acc: 0.8492\n",
      "Epoch 64/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4669 - acc: 0.8492 - val_loss: 0.4669 - val_acc: 0.8486\n",
      "Epoch 65/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4662 - acc: 0.8491 - val_loss: 0.4677 - val_acc: 0.8490\n",
      "Epoch 66/100\n",
      "32500/32500 [==============================] - 1s 29us/step - loss: 0.4660 - acc: 0.8499 - val_loss: 0.4645 - val_acc: 0.8501\n",
      "Epoch 67/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.4658 - acc: 0.8493 - val_loss: 0.4637 - val_acc: 0.8503\n",
      "Epoch 68/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4653 - acc: 0.8498 - val_loss: 0.4642 - val_acc: 0.8493\n",
      "Epoch 69/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4656 - acc: 0.8497 - val_loss: 0.4635 - val_acc: 0.8491\n",
      "Epoch 70/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4649 - acc: 0.8489 - val_loss: 0.4656 - val_acc: 0.8492\n",
      "Epoch 71/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4651 - acc: 0.8497 - val_loss: 0.4712 - val_acc: 0.8484\n",
      "Epoch 72/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4646 - acc: 0.8499 - val_loss: 0.4625 - val_acc: 0.8507\n",
      "Epoch 73/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4643 - acc: 0.8497 - val_loss: 0.4648 - val_acc: 0.8503\n",
      "Epoch 74/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4642 - acc: 0.8493 - val_loss: 0.4631 - val_acc: 0.8503\n",
      "Epoch 75/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4644 - acc: 0.8490 - val_loss: 0.4615 - val_acc: 0.8499\n",
      "Epoch 76/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4635 - acc: 0.8500 - val_loss: 0.4631 - val_acc: 0.8494\n",
      "Epoch 77/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4630 - acc: 0.8499 - val_loss: 0.4631 - val_acc: 0.8489\n",
      "Epoch 78/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4630 - acc: 0.8501 - val_loss: 0.4634 - val_acc: 0.8491\n",
      "Epoch 79/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4637 - acc: 0.8491 - val_loss: 0.4647 - val_acc: 0.8487\n",
      "Epoch 80/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4630 - acc: 0.8504 - val_loss: 0.4671 - val_acc: 0.8476\n",
      "Epoch 81/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4638 - acc: 0.8496 - val_loss: 0.4643 - val_acc: 0.8490\n",
      "Epoch 82/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4626 - acc: 0.8497 - val_loss: 0.4629 - val_acc: 0.8499\n",
      "Epoch 83/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4629 - acc: 0.8493 - val_loss: 0.4612 - val_acc: 0.8509\n",
      "Epoch 84/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4619 - acc: 0.8497 - val_loss: 0.4647 - val_acc: 0.8501\n",
      "Epoch 85/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4621 - acc: 0.8503 - val_loss: 0.4628 - val_acc: 0.8498\n",
      "Epoch 86/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4620 - acc: 0.8508 - val_loss: 0.4601 - val_acc: 0.8505\n",
      "Epoch 87/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4620 - acc: 0.8498 - val_loss: 0.4597 - val_acc: 0.8513\n",
      "Epoch 88/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4618 - acc: 0.8505 - val_loss: 0.4600 - val_acc: 0.8512\n",
      "Epoch 89/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4616 - acc: 0.8501 - val_loss: 0.4597 - val_acc: 0.8508\n",
      "Epoch 90/100\n",
      "32500/32500 [==============================] - 1s 17us/step - loss: 0.4615 - acc: 0.8504 - val_loss: 0.4596 - val_acc: 0.8502\n",
      "Epoch 91/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4611 - acc: 0.8498 - val_loss: 0.4593 - val_acc: 0.8512\n",
      "Epoch 92/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4613 - acc: 0.8500 - val_loss: 0.4590 - val_acc: 0.8511\n",
      "Epoch 93/100\n",
      "32500/32500 [==============================] - 1s 17us/step - loss: 0.4605 - acc: 0.8495 - val_loss: 0.4604 - val_acc: 0.8498\n",
      "Epoch 94/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4607 - acc: 0.8502 - val_loss: 0.4619 - val_acc: 0.8490\n",
      "Epoch 95/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4606 - acc: 0.8509 - val_loss: 0.4653 - val_acc: 0.8492\n",
      "Epoch 96/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.4610 - acc: 0.8509 - val_loss: 0.4583 - val_acc: 0.8510\n",
      "Epoch 97/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4604 - acc: 0.8510 - val_loss: 0.4600 - val_acc: 0.8509\n",
      "Epoch 98/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4597 - acc: 0.8498 - val_loss: 0.4610 - val_acc: 0.8503\n",
      "Epoch 99/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4600 - acc: 0.8502 - val_loss: 0.4597 - val_acc: 0.8505\n",
      "Epoch 100/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4599 - acc: 0.8505 - val_loss: 0.4617 - val_acc: 0.8498\n",
      "130001/130001 [==============================] - 17s 134us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32500 samples, validate on 32500 samples\n",
      "Epoch 1/100\n",
      "32500/32500 [==============================] - 5s 158us/step - loss: 1.2514 - acc: 0.7001 - val_loss: 0.8256 - val_acc: 0.7841\n",
      "Epoch 2/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.7469 - acc: 0.8038 - val_loss: 0.6919 - val_acc: 0.8173\n",
      "Epoch 3/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.6631 - acc: 0.8186 - val_loss: 0.6376 - val_acc: 0.8221\n",
      "Epoch 4/100\n",
      "32500/32500 [==============================] - 1s 17us/step - loss: 0.6225 - acc: 0.8236 - val_loss: 0.6101 - val_acc: 0.8264\n",
      "Epoch 5/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.5999 - acc: 0.8271 - val_loss: 0.5913 - val_acc: 0.8290\n",
      "Epoch 6/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5851 - acc: 0.8284 - val_loss: 0.5770 - val_acc: 0.8293\n",
      "Epoch 7/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.5723 - acc: 0.8296 - val_loss: 0.5652 - val_acc: 0.8303\n",
      "Epoch 8/100\n",
      "32500/32500 [==============================] - 1s 28us/step - loss: 0.5610 - acc: 0.8305 - val_loss: 0.5541 - val_acc: 0.8306\n",
      "Epoch 9/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.5507 - acc: 0.8310 - val_loss: 0.5455 - val_acc: 0.8319\n",
      "Epoch 10/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5414 - acc: 0.8320 - val_loss: 0.5373 - val_acc: 0.8325\n",
      "Epoch 11/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.5337 - acc: 0.8322 - val_loss: 0.5293 - val_acc: 0.8330\n",
      "Epoch 12/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5271 - acc: 0.8332 - val_loss: 0.5224 - val_acc: 0.8331\n",
      "Epoch 13/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5208 - acc: 0.8327 - val_loss: 0.5162 - val_acc: 0.8334\n",
      "Epoch 14/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5155 - acc: 0.8333 - val_loss: 0.5116 - val_acc: 0.8335\n",
      "Epoch 15/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5112 - acc: 0.8338 - val_loss: 0.5100 - val_acc: 0.8330\n",
      "Epoch 16/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5073 - acc: 0.8335 - val_loss: 0.5038 - val_acc: 0.8345\n",
      "Epoch 17/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.5036 - acc: 0.8348 - val_loss: 0.5002 - val_acc: 0.8347\n",
      "Epoch 18/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.5004 - acc: 0.8358 - val_loss: 0.4999 - val_acc: 0.8369\n",
      "Epoch 19/100\n",
      "32500/32500 [==============================] - 1s 30us/step - loss: 0.4978 - acc: 0.8367 - val_loss: 0.4967 - val_acc: 0.8388\n",
      "Epoch 20/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4956 - acc: 0.8376 - val_loss: 0.4946 - val_acc: 0.8403\n",
      "Epoch 21/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4939 - acc: 0.8393 - val_loss: 0.4911 - val_acc: 0.8404\n",
      "Epoch 22/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4917 - acc: 0.8403 - val_loss: 0.4897 - val_acc: 0.8419\n",
      "Epoch 23/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4906 - acc: 0.8412 - val_loss: 0.4891 - val_acc: 0.8425\n",
      "Epoch 24/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4890 - acc: 0.8408 - val_loss: 0.4878 - val_acc: 0.8417\n",
      "Epoch 25/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4879 - acc: 0.8418 - val_loss: 0.4867 - val_acc: 0.8444\n",
      "Epoch 26/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4860 - acc: 0.8419 - val_loss: 0.4914 - val_acc: 0.8416\n",
      "Epoch 27/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4854 - acc: 0.8426 - val_loss: 0.4854 - val_acc: 0.8434\n",
      "Epoch 28/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4841 - acc: 0.8428 - val_loss: 0.4822 - val_acc: 0.8438\n",
      "Epoch 29/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4836 - acc: 0.8438 - val_loss: 0.4819 - val_acc: 0.8451\n",
      "Epoch 30/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4823 - acc: 0.8446 - val_loss: 0.4805 - val_acc: 0.8446\n",
      "Epoch 31/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4813 - acc: 0.8442 - val_loss: 0.4798 - val_acc: 0.8440\n",
      "Epoch 32/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4806 - acc: 0.8446 - val_loss: 0.4807 - val_acc: 0.8450\n",
      "Epoch 33/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4797 - acc: 0.8451 - val_loss: 0.4794 - val_acc: 0.8451\n",
      "Epoch 34/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4792 - acc: 0.8451 - val_loss: 0.4780 - val_acc: 0.8453\n",
      "Epoch 35/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4781 - acc: 0.8443 - val_loss: 0.4775 - val_acc: 0.8459\n",
      "Epoch 36/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4777 - acc: 0.8448 - val_loss: 0.4806 - val_acc: 0.8451\n",
      "Epoch 37/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4765 - acc: 0.8456 - val_loss: 0.4762 - val_acc: 0.8451\n",
      "Epoch 38/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4762 - acc: 0.8458 - val_loss: 0.4780 - val_acc: 0.8458\n",
      "Epoch 39/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4759 - acc: 0.8454 - val_loss: 0.4750 - val_acc: 0.8460\n",
      "Epoch 40/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4749 - acc: 0.8466 - val_loss: 0.4766 - val_acc: 0.8465\n",
      "Epoch 41/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4749 - acc: 0.8458 - val_loss: 0.4726 - val_acc: 0.8466\n",
      "Epoch 42/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4740 - acc: 0.8462 - val_loss: 0.4744 - val_acc: 0.8466\n",
      "Epoch 43/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4731 - acc: 0.8462 - val_loss: 0.4718 - val_acc: 0.8468\n",
      "Epoch 44/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4730 - acc: 0.8465 - val_loss: 0.4730 - val_acc: 0.8466\n",
      "Epoch 45/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4725 - acc: 0.8466 - val_loss: 0.4714 - val_acc: 0.8476\n",
      "Epoch 46/100\n",
      "32500/32500 [==============================] - 1s 29us/step - loss: 0.4723 - acc: 0.8475 - val_loss: 0.4720 - val_acc: 0.8479\n",
      "Epoch 47/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4720 - acc: 0.8471 - val_loss: 0.4698 - val_acc: 0.8477\n",
      "Epoch 48/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4707 - acc: 0.8474 - val_loss: 0.4725 - val_acc: 0.8467\n",
      "Epoch 49/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4711 - acc: 0.8469 - val_loss: 0.4702 - val_acc: 0.8474\n",
      "Epoch 50/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4708 - acc: 0.8478 - val_loss: 0.4721 - val_acc: 0.8468\n",
      "Epoch 51/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4704 - acc: 0.8468 - val_loss: 0.4684 - val_acc: 0.8475\n",
      "Epoch 52/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4696 - acc: 0.8482 - val_loss: 0.4692 - val_acc: 0.8463\n",
      "Epoch 53/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4693 - acc: 0.8475 - val_loss: 0.4688 - val_acc: 0.8477\n",
      "Epoch 54/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4693 - acc: 0.8480 - val_loss: 0.4681 - val_acc: 0.8480\n",
      "Epoch 55/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4686 - acc: 0.8477 - val_loss: 0.4681 - val_acc: 0.8476\n",
      "Epoch 56/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4682 - acc: 0.8478 - val_loss: 0.4693 - val_acc: 0.8485\n",
      "Epoch 57/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4678 - acc: 0.8480 - val_loss: 0.4669 - val_acc: 0.8482\n",
      "Epoch 58/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4680 - acc: 0.8478 - val_loss: 0.4668 - val_acc: 0.8486\n",
      "Epoch 59/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4674 - acc: 0.8479 - val_loss: 0.4679 - val_acc: 0.8482\n",
      "Epoch 60/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4672 - acc: 0.8489 - val_loss: 0.4660 - val_acc: 0.8493\n",
      "Epoch 61/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4669 - acc: 0.8478 - val_loss: 0.4651 - val_acc: 0.8489\n",
      "Epoch 62/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4667 - acc: 0.8484 - val_loss: 0.4651 - val_acc: 0.8488\n",
      "Epoch 63/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4662 - acc: 0.8492 - val_loss: 0.4655 - val_acc: 0.8486\n",
      "Epoch 64/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4662 - acc: 0.8481 - val_loss: 0.4649 - val_acc: 0.8486\n",
      "Epoch 65/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4659 - acc: 0.8483 - val_loss: 0.4674 - val_acc: 0.8474\n",
      "Epoch 66/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4660 - acc: 0.8482 - val_loss: 0.4685 - val_acc: 0.8483\n",
      "Epoch 67/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4656 - acc: 0.8489 - val_loss: 0.4685 - val_acc: 0.8480\n",
      "Epoch 68/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4652 - acc: 0.8488 - val_loss: 0.4653 - val_acc: 0.8476\n",
      "Epoch 69/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4651 - acc: 0.8480 - val_loss: 0.4647 - val_acc: 0.8492\n",
      "Epoch 70/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4648 - acc: 0.8492 - val_loss: 0.4638 - val_acc: 0.8490\n",
      "Epoch 71/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4646 - acc: 0.8496 - val_loss: 0.4631 - val_acc: 0.8493\n",
      "Epoch 72/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4646 - acc: 0.8494 - val_loss: 0.4643 - val_acc: 0.8493\n",
      "Epoch 73/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4642 - acc: 0.8486 - val_loss: 0.4619 - val_acc: 0.8498\n",
      "Epoch 74/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4637 - acc: 0.8488 - val_loss: 0.4646 - val_acc: 0.8488\n",
      "Epoch 75/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4638 - acc: 0.8489 - val_loss: 0.4617 - val_acc: 0.8499\n",
      "Epoch 76/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4636 - acc: 0.8492 - val_loss: 0.4621 - val_acc: 0.8498\n",
      "Epoch 77/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4637 - acc: 0.8492 - val_loss: 0.4629 - val_acc: 0.8499\n",
      "Epoch 78/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4635 - acc: 0.8495 - val_loss: 0.4638 - val_acc: 0.8492\n",
      "Epoch 79/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4633 - acc: 0.8498 - val_loss: 0.4621 - val_acc: 0.8496\n",
      "Epoch 80/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4625 - acc: 0.8497 - val_loss: 0.4623 - val_acc: 0.8494\n",
      "Epoch 81/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4629 - acc: 0.8504 - val_loss: 0.4621 - val_acc: 0.8498\n",
      "Epoch 82/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4627 - acc: 0.8497 - val_loss: 0.4610 - val_acc: 0.8500\n",
      "Epoch 83/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4623 - acc: 0.8488 - val_loss: 0.4619 - val_acc: 0.8493\n",
      "Epoch 84/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4618 - acc: 0.8498 - val_loss: 0.4629 - val_acc: 0.8496\n",
      "Epoch 85/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4618 - acc: 0.8497 - val_loss: 0.4618 - val_acc: 0.8492\n",
      "Epoch 86/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4618 - acc: 0.8497 - val_loss: 0.4609 - val_acc: 0.8497\n",
      "Epoch 87/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4614 - acc: 0.8498 - val_loss: 0.4597 - val_acc: 0.8503\n",
      "Epoch 88/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4613 - acc: 0.8491 - val_loss: 0.4607 - val_acc: 0.8489\n",
      "Epoch 89/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4620 - acc: 0.8495 - val_loss: 0.4608 - val_acc: 0.8501\n",
      "Epoch 90/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4612 - acc: 0.8494 - val_loss: 0.4600 - val_acc: 0.8500\n",
      "Epoch 91/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4611 - acc: 0.8500 - val_loss: 0.4606 - val_acc: 0.8500\n",
      "Epoch 92/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4610 - acc: 0.8499 - val_loss: 0.4598 - val_acc: 0.8496\n",
      "Epoch 93/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4609 - acc: 0.8499 - val_loss: 0.4587 - val_acc: 0.8505\n",
      "Epoch 94/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4606 - acc: 0.8501 - val_loss: 0.4594 - val_acc: 0.8505\n",
      "Epoch 95/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4603 - acc: 0.8493 - val_loss: 0.4598 - val_acc: 0.8513\n",
      "Epoch 96/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4602 - acc: 0.8503 - val_loss: 0.4604 - val_acc: 0.8501\n",
      "Epoch 97/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4602 - acc: 0.8504 - val_loss: 0.4585 - val_acc: 0.8507\n",
      "Epoch 98/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4605 - acc: 0.8499 - val_loss: 0.4581 - val_acc: 0.8500\n",
      "Epoch 99/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4600 - acc: 0.8504 - val_loss: 0.4581 - val_acc: 0.8504\n",
      "Epoch 100/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4597 - acc: 0.8494 - val_loss: 0.4586 - val_acc: 0.8507\n",
      "130001/130001 [==============================] - 24s 185us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32500 samples, validate on 32500 samples\n",
      "Epoch 1/100\n",
      "32500/32500 [==============================] - 10s 321us/step - loss: 1.2495 - acc: 0.6997 - val_loss: 0.7904 - val_acc: 0.7957\n",
      "Epoch 2/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.7102 - acc: 0.8138 - val_loss: 0.6584 - val_acc: 0.8210\n",
      "Epoch 3/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.6360 - acc: 0.8262 - val_loss: 0.6201 - val_acc: 0.8284\n",
      "Epoch 4/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.6091 - acc: 0.8296 - val_loss: 0.6013 - val_acc: 0.8305\n",
      "Epoch 5/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.5940 - acc: 0.8308 - val_loss: 0.5879 - val_acc: 0.8303\n",
      "Epoch 6/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.5823 - acc: 0.8318 - val_loss: 0.5768 - val_acc: 0.8315\n",
      "Epoch 7/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5728 - acc: 0.8324 - val_loss: 0.5676 - val_acc: 0.8319\n",
      "Epoch 8/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.5643 - acc: 0.8331 - val_loss: 0.5606 - val_acc: 0.8344\n",
      "Epoch 9/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5571 - acc: 0.8341 - val_loss: 0.5543 - val_acc: 0.8335\n",
      "Epoch 10/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5508 - acc: 0.8332 - val_loss: 0.5485 - val_acc: 0.8349\n",
      "Epoch 11/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.5451 - acc: 0.8345 - val_loss: 0.5415 - val_acc: 0.8349\n",
      "Epoch 12/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.5401 - acc: 0.8347 - val_loss: 0.5359 - val_acc: 0.8358\n",
      "Epoch 13/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5354 - acc: 0.8355 - val_loss: 0.5334 - val_acc: 0.8363\n",
      "Epoch 14/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5316 - acc: 0.8358 - val_loss: 0.5291 - val_acc: 0.8356\n",
      "Epoch 15/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5277 - acc: 0.8364 - val_loss: 0.5259 - val_acc: 0.8360\n",
      "Epoch 16/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.5243 - acc: 0.8366 - val_loss: 0.5227 - val_acc: 0.8367\n",
      "Epoch 17/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.5212 - acc: 0.8371 - val_loss: 0.5187 - val_acc: 0.8376\n",
      "Epoch 18/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.5183 - acc: 0.8370 - val_loss: 0.5189 - val_acc: 0.8379\n",
      "Epoch 19/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.5156 - acc: 0.8383 - val_loss: 0.5174 - val_acc: 0.8394\n",
      "Epoch 20/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5133 - acc: 0.8393 - val_loss: 0.5101 - val_acc: 0.8396\n",
      "Epoch 21/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.5104 - acc: 0.8394 - val_loss: 0.5101 - val_acc: 0.8397\n",
      "Epoch 22/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.5084 - acc: 0.8417 - val_loss: 0.5060 - val_acc: 0.8425\n",
      "Epoch 23/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.5058 - acc: 0.8428 - val_loss: 0.5043 - val_acc: 0.8424\n",
      "Epoch 24/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.5041 - acc: 0.8433 - val_loss: 0.5044 - val_acc: 0.8438\n",
      "Epoch 25/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.5024 - acc: 0.8440 - val_loss: 0.5000 - val_acc: 0.8453\n",
      "Epoch 26/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.5007 - acc: 0.8451 - val_loss: 0.4997 - val_acc: 0.8444\n",
      "Epoch 27/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4990 - acc: 0.8443 - val_loss: 0.4993 - val_acc: 0.8454\n",
      "Epoch 28/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4980 - acc: 0.8452 - val_loss: 0.4969 - val_acc: 0.8443\n",
      "Epoch 29/100\n",
      "32500/32500 [==============================] - 1s 36us/step - loss: 0.4964 - acc: 0.8450 - val_loss: 0.4958 - val_acc: 0.8454\n",
      "Epoch 30/100\n",
      "32500/32500 [==============================] - 1s 41us/step - loss: 0.4955 - acc: 0.8455 - val_loss: 0.4936 - val_acc: 0.8455\n",
      "Epoch 31/100\n",
      "32500/32500 [==============================] - 1s 30us/step - loss: 0.4943 - acc: 0.8454 - val_loss: 0.4925 - val_acc: 0.8458\n",
      "Epoch 32/100\n",
      "32500/32500 [==============================] - 1s 30us/step - loss: 0.4931 - acc: 0.8458 - val_loss: 0.4939 - val_acc: 0.8466\n",
      "Epoch 33/100\n",
      "32500/32500 [==============================] - 1s 29us/step - loss: 0.4920 - acc: 0.8462 - val_loss: 0.4902 - val_acc: 0.8463\n",
      "Epoch 34/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4893 - acc: 0.8468 - val_loss: 0.4853 - val_acc: 0.8466\n",
      "Epoch 35/100\n",
      "32500/32500 [==============================] - 1s 31us/step - loss: 0.4831 - acc: 0.8471 - val_loss: 0.4785 - val_acc: 0.8471\n",
      "Epoch 36/100\n",
      "32500/32500 [==============================] - 1s 33us/step - loss: 0.4789 - acc: 0.8468 - val_loss: 0.4771 - val_acc: 0.8468\n",
      "Epoch 37/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4777 - acc: 0.8470 - val_loss: 0.4784 - val_acc: 0.8465\n",
      "Epoch 38/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4764 - acc: 0.8473 - val_loss: 0.4756 - val_acc: 0.8467\n",
      "Epoch 39/100\n",
      "32500/32500 [==============================] - 1s 31us/step - loss: 0.4758 - acc: 0.8477 - val_loss: 0.4794 - val_acc: 0.8468\n",
      "Epoch 40/100\n",
      "32500/32500 [==============================] - 1s 35us/step - loss: 0.4750 - acc: 0.8479 - val_loss: 0.4733 - val_acc: 0.8484\n",
      "Epoch 41/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4741 - acc: 0.8476 - val_loss: 0.4771 - val_acc: 0.8473\n",
      "Epoch 42/100\n",
      "32500/32500 [==============================] - 1s 31us/step - loss: 0.4740 - acc: 0.8470 - val_loss: 0.4738 - val_acc: 0.8471\n",
      "Epoch 43/100\n",
      "32500/32500 [==============================] - 1s 43us/step - loss: 0.4726 - acc: 0.8489 - val_loss: 0.4734 - val_acc: 0.8459\n",
      "Epoch 44/100\n",
      "32500/32500 [==============================] - 2s 48us/step - loss: 0.4727 - acc: 0.8479 - val_loss: 0.4707 - val_acc: 0.8475\n",
      "Epoch 45/100\n",
      "32500/32500 [==============================] - 1s 41us/step - loss: 0.4720 - acc: 0.8480 - val_loss: 0.4721 - val_acc: 0.8483\n",
      "Epoch 46/100\n",
      "32500/32500 [==============================] - 1s 35us/step - loss: 0.4716 - acc: 0.8486 - val_loss: 0.4722 - val_acc: 0.8480\n",
      "Epoch 47/100\n",
      "32500/32500 [==============================] - 1s 36us/step - loss: 0.4714 - acc: 0.8484 - val_loss: 0.4723 - val_acc: 0.8482\n",
      "Epoch 48/100\n",
      "32500/32500 [==============================] - 1s 33us/step - loss: 0.4703 - acc: 0.8474 - val_loss: 0.4706 - val_acc: 0.8490\n",
      "Epoch 49/100\n",
      "32500/32500 [==============================] - 2s 47us/step - loss: 0.4701 - acc: 0.8488 - val_loss: 0.4683 - val_acc: 0.8484\n",
      "Epoch 50/100\n",
      "32500/32500 [==============================] - 1s 37us/step - loss: 0.4698 - acc: 0.8489 - val_loss: 0.4683 - val_acc: 0.8479\n",
      "Epoch 51/100\n",
      "32500/32500 [==============================] - 1s 40us/step - loss: 0.4695 - acc: 0.8480 - val_loss: 0.4693 - val_acc: 0.8484\n",
      "Epoch 52/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4694 - acc: 0.8480 - val_loss: 0.4673 - val_acc: 0.8489\n",
      "Epoch 53/100\n",
      "32500/32500 [==============================] - 1s 31us/step - loss: 0.4685 - acc: 0.8482 - val_loss: 0.4666 - val_acc: 0.8484\n",
      "Epoch 54/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4686 - acc: 0.8489 - val_loss: 0.4670 - val_acc: 0.8485\n",
      "Epoch 55/100\n",
      "32500/32500 [==============================] - 1s 32us/step - loss: 0.4682 - acc: 0.8482 - val_loss: 0.4668 - val_acc: 0.8474\n",
      "Epoch 56/100\n",
      "32500/32500 [==============================] - 1s 35us/step - loss: 0.4677 - acc: 0.8485 - val_loss: 0.4669 - val_acc: 0.8486\n",
      "Epoch 57/100\n",
      "32500/32500 [==============================] - 1s 33us/step - loss: 0.4675 - acc: 0.8476 - val_loss: 0.4655 - val_acc: 0.8485\n",
      "Epoch 58/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4671 - acc: 0.8481 - val_loss: 0.4659 - val_acc: 0.8494\n",
      "Epoch 59/100\n",
      "32500/32500 [==============================] - 1s 29us/step - loss: 0.4668 - acc: 0.8491 - val_loss: 0.4716 - val_acc: 0.8457\n",
      "Epoch 60/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4668 - acc: 0.8488 - val_loss: 0.4658 - val_acc: 0.8484\n",
      "Epoch 61/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4667 - acc: 0.8474 - val_loss: 0.4658 - val_acc: 0.8486\n",
      "Epoch 62/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4664 - acc: 0.8481 - val_loss: 0.4664 - val_acc: 0.8494\n",
      "Epoch 63/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4656 - acc: 0.8485 - val_loss: 0.4640 - val_acc: 0.8487\n",
      "Epoch 64/100\n",
      "32500/32500 [==============================] - 1s 18us/step - loss: 0.4655 - acc: 0.8485 - val_loss: 0.4645 - val_acc: 0.8484\n",
      "Epoch 65/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4654 - acc: 0.8486 - val_loss: 0.4641 - val_acc: 0.8489\n",
      "Epoch 66/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4652 - acc: 0.8489 - val_loss: 0.4635 - val_acc: 0.8492\n",
      "Epoch 67/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4647 - acc: 0.8487 - val_loss: 0.4632 - val_acc: 0.8490\n",
      "Epoch 68/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4645 - acc: 0.8490 - val_loss: 0.4648 - val_acc: 0.8483\n",
      "Epoch 69/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4648 - acc: 0.8495 - val_loss: 0.4644 - val_acc: 0.8478\n",
      "Epoch 70/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4640 - acc: 0.8495 - val_loss: 0.4617 - val_acc: 0.8498\n",
      "Epoch 71/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4634 - acc: 0.8491 - val_loss: 0.4625 - val_acc: 0.8504\n",
      "Epoch 72/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4641 - acc: 0.8491 - val_loss: 0.4615 - val_acc: 0.8494\n",
      "Epoch 73/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4633 - acc: 0.8492 - val_loss: 0.4617 - val_acc: 0.8497\n",
      "Epoch 74/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4631 - acc: 0.8497 - val_loss: 0.4629 - val_acc: 0.8503\n",
      "Epoch 75/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4631 - acc: 0.8486 - val_loss: 0.4618 - val_acc: 0.8487\n",
      "Epoch 76/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4629 - acc: 0.8493 - val_loss: 0.4641 - val_acc: 0.8496\n",
      "Epoch 77/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4627 - acc: 0.8493 - val_loss: 0.4608 - val_acc: 0.8502\n",
      "Epoch 78/100\n",
      "32500/32500 [==============================] - 1s 23us/step - loss: 0.4625 - acc: 0.8493 - val_loss: 0.4599 - val_acc: 0.8496\n",
      "Epoch 79/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4618 - acc: 0.8493 - val_loss: 0.4611 - val_acc: 0.8496\n",
      "Epoch 80/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4622 - acc: 0.8494 - val_loss: 0.4621 - val_acc: 0.8507\n",
      "Epoch 81/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4617 - acc: 0.8490 - val_loss: 0.4613 - val_acc: 0.8496\n",
      "Epoch 82/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4618 - acc: 0.8498 - val_loss: 0.4657 - val_acc: 0.8498\n",
      "Epoch 83/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4618 - acc: 0.8499 - val_loss: 0.4605 - val_acc: 0.8497\n",
      "Epoch 84/100\n",
      "32500/32500 [==============================] - 1s 19us/step - loss: 0.4615 - acc: 0.8494 - val_loss: 0.4592 - val_acc: 0.8495\n",
      "Epoch 85/100\n",
      "32500/32500 [==============================] - 1s 24us/step - loss: 0.4610 - acc: 0.8506 - val_loss: 0.4629 - val_acc: 0.8494\n",
      "Epoch 86/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4610 - acc: 0.8500 - val_loss: 0.4594 - val_acc: 0.8510\n",
      "Epoch 87/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4607 - acc: 0.8501 - val_loss: 0.4647 - val_acc: 0.8472\n",
      "Epoch 88/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4608 - acc: 0.8508 - val_loss: 0.4607 - val_acc: 0.8510\n",
      "Epoch 89/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4605 - acc: 0.8500 - val_loss: 0.4594 - val_acc: 0.8496\n",
      "Epoch 90/100\n",
      "32500/32500 [==============================] - 1s 22us/step - loss: 0.4606 - acc: 0.8493 - val_loss: 0.4590 - val_acc: 0.8496\n",
      "Epoch 91/100\n",
      "32500/32500 [==============================] - 1s 20us/step - loss: 0.4601 - acc: 0.8502 - val_loss: 0.4583 - val_acc: 0.8499\n",
      "Epoch 92/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.4602 - acc: 0.8490 - val_loss: 0.4579 - val_acc: 0.8509\n",
      "Epoch 93/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4600 - acc: 0.8507 - val_loss: 0.4585 - val_acc: 0.8501\n",
      "Epoch 94/100\n",
      "32500/32500 [==============================] - 1s 28us/step - loss: 0.4600 - acc: 0.8495 - val_loss: 0.4590 - val_acc: 0.8510\n",
      "Epoch 95/100\n",
      "32500/32500 [==============================] - 1s 27us/step - loss: 0.4595 - acc: 0.8507 - val_loss: 0.4584 - val_acc: 0.8501\n",
      "Epoch 96/100\n",
      "32500/32500 [==============================] - 1s 30us/step - loss: 0.4605 - acc: 0.8501 - val_loss: 0.4582 - val_acc: 0.8513\n",
      "Epoch 97/100\n",
      "32500/32500 [==============================] - 1s 34us/step - loss: 0.4591 - acc: 0.8509 - val_loss: 0.4585 - val_acc: 0.8502\n",
      "Epoch 98/100\n",
      "32500/32500 [==============================] - 1s 26us/step - loss: 0.4587 - acc: 0.8495 - val_loss: 0.4588 - val_acc: 0.8500\n",
      "Epoch 99/100\n",
      "32500/32500 [==============================] - 1s 25us/step - loss: 0.4593 - acc: 0.8492 - val_loss: 0.4587 - val_acc: 0.8500\n",
      "Epoch 100/100\n",
      "32500/32500 [==============================] - 1s 21us/step - loss: 0.4593 - acc: 0.8500 - val_loss: 0.4599 - val_acc: 0.8503\n",
      "130001/130001 [==============================] - 18s 139us/step\n",
      "[0.8520968585649227, 0.8510418333317711, 0.8486319336017334]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ave_acc_act_ann=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(Act_x, Act_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        model = create_model(X_train1.shape[1],y_train1.shape[1])\n",
    "        history = model.fit(X_train1, y_train1,validation_data=(X_train1,y_train1),batch_size=512,epochs=100)\n",
    "        score += model.evaluate(X_test1,y_test1,verbose=1)[1]\n",
    "    ave_acc_act_ann.append(score/3.0)\n",
    "    \n",
    "print ave_acc_act_ann  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmYFfWd7/H392y9A00viDQIKgq4obZbzCQYR4OaaFZ3Z+JNgrlP9ps40ZmszmTiJN4k48SYGMM1q45xScjERGIiMYmiIBoFBEFEadamm6X3Psv3/nGK9tD0aZqlOHSfz+t5+qFrOVXfsvB8qN+v6lfm7oiIiABECl2AiIgcPhQKIiLSR6EgIiJ9FAoiItJHoSAiIn0UCiIi0kehIDJEZnaPmf3bENdda2Z/f6DbETnUFAoiItJHoSAiIn0UCjKiBM02N5rZC2bWYWY/NLNxZvZbM2szs8fMrDpn/UvNbJmZbTezBWY2PWfZqWa2JPjcfwOl/fb1DjN7Pvjsk2Z28n7W/GEzW21mrWY2z8yODOabmX3LzLaY2U4ze9HMTgyWXWxmy4Pa1pvZZ/frP5hIPwoFGYneC1wAHAe8E/gt8M9AHdm/858AMLPjgHuBTwXLHgF+bWYJM0sAvwR+AowFfhFsl+CzpwJzgRuAGuD7wDwzK9mXQs3sbcDXgMuB8cBrwH3B4guBtwTHMTpYpyVY9kPgBnevAk4E/rgv+xXJR6EgI9F/uftmd18P/Bl42t2fc/du4GHg1GC9K4DfuPvv3T0J3AaUAW8CzgbiwLfdPenuDwCLcvYxB/i+uz/t7ml3/xHQE3xuX1wDzHX3Je7eA9wMnGNmk4EkUAVMA8zdX3L3jcHnksAMMxvl7tvcfck+7ldkQAoFGYk25/zeNcB0ZfD7kWT/ZQ6Au2eAdcCEYNl6333EyNdyfj8K+EzQdLTdzLYDE4PP7Yv+NbSTvRqY4O5/BL4D3AFsMbO7zGxUsOp7gYuB18zsT2Z2zj7uV2RACgUpZhvIfrkD2TZ8sl/s64GNwIRg3i6Tcn5fB3zV3cfk/JS7+70HWEMF2eao9QDufru7nw7MINuMdGMwf5G7XwbUk23mun8f9ysyIIWCFLP7gUvM7HwziwOfIdsE9CTwFJACPmFmcTN7D3Bmzmd/AHzEzM4KOoQrzOwSM6vaxxruBa43s5lBf8S/k23uWmtmZwTbjwMdQDeQCfo8rjGz0UGz104gcwD/HUT6KBSkaLn7SuBa4L+ArWQ7pd/p7r3u3gu8B/gA0Eq2/+GhnM8uBj5MtnlnG7A6WHdfa3gM+ALwINmrk2OAK4PFo8iGzzayTUwtwDeCZdcBa81sJ/ARsn0TIgfM9JIdERHZRVcKIiLSR6EgIiJ9FAoiItJHoSAiIn1ihS5gX9XW1vrkyZMLXYaIyLDy7LPPbnX3ur2tN+xCYfLkySxevLjQZYiIDCtm9tre11LzkYiI5FAoiIhIH4WCiIj0GXZ9CgNJJpM0NTXR3d1d6FJCV1paSkNDA/F4vNCliMgINCJCoampiaqqKiZPnszug1qOLO5OS0sLTU1NTJkypdDliMgINCKaj7q7u6mpqRnRgQBgZtTU1BTFFZGIFMaICAVgxAfCLsVynCJSGCMmFPamO5lm045ukmkNOy8ikk/RhEJPMs2Wtm5SmYM/VPj27dv57ne/u8+fu/jii9m+fftBr0dEZH8VTSj0NbuE8P6IfKGQSqUG/dwjjzzCmDFjDno9IiL7a0TcfTQUuzIhhAsFbrrpJl555RVmzpxJPB6ntLSU6upqVqxYwcsvv8y73vUu1q1bR3d3N5/85CeZM2cO8MaQHe3t7Vx00UW8+c1v5sknn2TChAn86le/oqys7OAXKyIyiBEXCl/59TKWb9i5x/x0xulOpilNRInuY2ftjCNH8aV3npB3+a233srSpUt5/vnnWbBgAZdccglLly7tu2107ty5jB07lq6uLs444wze+973UlNTs9s2Vq1axb333ssPfvADLr/8ch588EGuvfbafapTRORAjbhQyKcvBxwI+QaeM888c7fnCG6//XYefvhhANatW8eqVav2CIUpU6Ywc+ZMAE4//XTWrl0bbpEiIgMYcaGQ71/0nT0pVje3M7mmglFl4T4NXFFR0ff7ggULeOyxx3jqqacoLy9n1qxZAz5nUFJS0vd7NBqlq6sr1BpFRAZSRB3N2T9D6FKgqqqKtra2AZft2LGD6upqysvLWbFiBQsXLgyhAhGRg2PEXSnks+vuIw/h7qOamhrOPfdcTjzxRMrKyhg3blzfstmzZ/O9732P6dOnc/zxx3P22Wcf9P2LiBwsFsaXZJgaGxu9/0t2XnrpJaZPnz7o53qSaVZubmPi2HKqyxNhlhi6oRyviEguM3vW3Rv3tl5ozUdmNtfMtpjZ0jzLrzGzF8zsRTN70sxOCauW7P6yfw6zDBQROaTC7FO4B5g9yPJXgbe6+0nAvwJ3hVhLqM1HIiIjRWh9Cu7+hJlNHmT5kzmTC4GGsGqBN+5CVSSIiOR3uNx99EHgt2HuQM1HIiJ7V/C7j8zsPLKh8OZB1pkDzAGYNGnS/u0HNR+JiOxNQa8UzOxk4G7gMndvybeeu9/l7o3u3lhXV7ef+wq2tV+fFhEpDgULBTObBDwEXOfuLx+C/WFYKM1H+zt0NsC3v/1tOjs7D3JFIiL7J8xbUu8FngKON7MmM/ugmX3EzD4SrPJFoAb4rpk9b2aL827soNUEHsK1gkJBREaKMO8+umovyz8EfCis/Q/ELJyO5tyhsy+44ALq6+u5//776enp4d3vfjdf+cpX6Ojo4PLLL6epqYl0Os0XvvAFNm/ezIYNGzjvvPOora3l8ccfP/jFiYjsg4J3NB90v70JNr044KLJvSliEYNYdN+2ecRJcNGteRfnDp09f/58HnjgAZ555hncnUsvvZQnnniC5uZmjjzySH7zm98A2TGRRo8ezTe/+U0ef/xxamtr960mEZEQHC63pI4Y8+fPZ/78+Zx66qmcdtpprFixglWrVnHSSSfx+9//ns997nP8+c9/ZvTo0YUuVURkDyPvSmGQf9Gv27iTipIYE8eWh7Z7d+fmm2/mhhtu2GPZkiVLeOSRR/j85z/P+eefzxe/+MXQ6hAR2R9FdaVgFs7dR7lDZ7/97W9n7ty5tLe3A7B+/Xq2bNnChg0bKC8v59prr+XGG29kyZIle3xWRKTQRt6VwiDCuvsod+jsiy66iKuvvppzzjkHgMrKSn7605+yevVqbrzxRiKRCPF4nDvvvBOAOXPmMHv2bI488kh1NItIwRXN0NkAqza3EY9GmFxbsdd1D2caOltE9lXBh84+HJkZmWEWgiIih1JxhQIa5kJEZDAjJhSG0gwW1sNrh9Jwa+4TkeFlRIRCaWkpLS0te/3CzL5oZ/h+qbo7LS0tlJaWFroUERmhRsTdRw0NDTQ1NdHc3Dzoei3tPaQzTrJl+H6plpaW0tAQ6vuIRKSIjYhQiMfjTJkyZa/rffRnS1i5uY3H/s+ph6AqEZHhZ0Q0Hw1VLGok05lClyEictgqqlCIRyMkUwoFEZF8ii4UetPDt6NZRCRsRRUKiaiRyuhKQUQknzDfvDbXzLaY2dI8y6eZ2VNm1mNmnw2rjlxqPhIRGVyYVwr3ALMHWd4KfAK4LcQadhOPRUiq+UhEJK/QQsHdnyD7xZ9v+RZ3XwQkw6qhv2yfQkZPBYuI5FFUfQrxiAGQyigUREQGMixCwczmmNliM1u8t6eWBxOPZQ9XzyqIiAxsWISCu9/l7o3u3lhXV7ff24lHd4WCrhRERAYyLELhYElEs81HulIQERlYaGMfmdm9wCyg1syagC8BcQB3/56ZHQEsBkYBGTP7FDDD3XeGVdMbVwoKBRGRgYQWCu5+1V6WbwIO6XCffaGQUvORiMhAiqr5KBY0H/XqSkFEZEBFFQoJNR+JiAyqqEJBfQoiIoMrrlCI6ZZUEZHBFFco6JZUEZFBFVUoqE9BRGRwRRUKMYWCiMigiioUdjUf9eo5BRGRARVVKKj5SERkcEUVCrtuSdUrOUVEBlZcoRDTMBciIoMprlDQMBciIoMqrlCIqE9BRGQwxRUKevOaiMigiisU+p5oVp+CiMhAiisUguaj3pSuFEREBhJaKJjZXDPbYmZL8yw3M7vdzFab2QtmdlpYtewSiRixiOmWVBGRPMK8UrgHmD3I8ouAqcHPHODOEGvpE49G1HwkIpJHaKHg7k8ArYOschnwY89aCIwxs/Fh1bNLPGpqPhIRyaOQfQoTgHU5003BvFBlrxQUCiIiAxkWHc1mNsfMFpvZ4ubm5gPalkJBRCS/QobCemBiznRDMG8P7n6Xuze6e2NdXd0B7TQeM/UpiIjkUchQmAf8Q3AX0tnADnffGPZOdaUgIpJfLKwNm9m9wCyg1syagC8BcQB3/x7wCHAxsBroBK4Pq5ZcCYWCiEheoYWCu1+1l+UOfDSs/eejW1JFRPIbFh3NB1MsarpSEBHJo+hCIR6N6DkFEZE8ii4U1KcgIpJf0YVCPGqkMupTEBEZSBGGgpqPRETyKb5QiKn5SEQkn+ILhYieaBYRyaf4QkEdzSIieRVfKKj5SEQkr6ILhYQ6mkVE8iq6UNAtqSIi+RVhKKj5SEQknyINBSc7Hp+IiOQqwlAwAN2WKiIygCIMhewhqwlJRGRPCgUREekTaiiY2WwzW2lmq83spgGWH2VmfzCzF8xsgZk1hFkPZJ9TADUfiYgMJLRQMLMocAdwETADuMrMZvRb7Tbgx+5+MnAL8LWw6tkl0denoCsFEZH+wrxSOBNY7e5r3L0XuA+4rN86M4A/Br8/PsDyg07NRyIi+YUZChOAdTnTTcG8XH8D3hP8/m6gysxqQqyJmEJBRCSvQnc0fxZ4q5k9B7wVWA+k+69kZnPMbLGZLW5ubj6gHe5qPupNqU9BRKS/MENhPTAxZ7ohmNfH3Te4+3vc/VTgX4J52/tvyN3vcvdGd2+sq6s7oKLUfCQikl+YobAImGpmU8wsAVwJzMtdwcxqzWxXDTcDc0OsB1AoiIgMJrRQcPcU8DHgUeAl4H53X2Zmt5jZpcFqs4CVZvYyMA74alj17PJGKKj5SESkv1iYG3f3R4BH+s37Ys7vDwAPhFlDf4mYbkkVEclnSFcKZvZJMxtlWT80syVmdmHYxYUhFlHzkYhIPkNtPvpf7r4TuBCoBq4Dbg2tqhCpT0FEJL+hhoIFf14M/MTdl+XMG1Z2NR/1qk9BRGQPQw2FZ81sPtlQeNTMqoBh+U/tvisFvZJTRGQPQ+1o/iAwE1jj7p1mNha4PryywrMrFFIZhYKISH9DvVI4B1jp7tvN7Frg88CO8MoKz65QUPORiMiehhoKdwKdZnYK8BngFeDHoVUVooSaj0RE8hpqKKQ8+1Ljy4DvuPsdQFV4ZYUnpqGzRUTyGmqfQpuZ3Uz2VtS/C4amiIdXVnh0S6qISH5DvVK4Augh+7zCJrKD230jtKpCFI/qllQRkXyGFApBEPwMGG1m7wC63X1Y9imYGfGokdKVgojIHoY6zMXlwDPA+4HLgafN7H1hFhameDSi5iMRkQEMtU/hX4Az3H0LgJnVAY9xiAezO1iyoaDmIxGR/obapxDZFQiBln347GEnHjV6daUgIrKHoV4p/M7MHgXuDaavoN+Q2MNJPBrRcwoiIgMYUii4+41m9l7g3GDWXe7+cHhlhUt9CiIiAxvyS3bc/UHgwX3ZuJnNBv4TiAJ3u/ut/ZZPAn4EjAnWuSl4MU+o4lFTn4KIyAAGDQUzawMG+vY0wN191CCfjQJ3ABcATcAiM5vn7stzVvs82dd03mlmM8g2SU3et0PYd7pSEBEZ2KCh4O4HMpTFmcBqd18DYGb3kR0mIzcUHNgVLKOBDQewvyFLxBQKIiIDCfMOognAupzppmBeri8D15pZE9mrhI8PtCEzm2Nmi81scXNz8wEXFouo+UhEZCCFvq30KuAed28geKtbMK7Sbtz9LndvdPfGurq6A95pPBrRLakiIgMIMxTWAxNzphuCebk+CNwP4O5PAaVAbSjVrHgEbjseWl9V85GISB5hhsIiYKqZTTGzBHAlMK/fOq8D5wOY2XSyoXDg7UMDiSWgfRO0b1FHs4hIHqGFgrungI8BjwIvkb3LaJmZ3WJmlwarfQb4sJn9jeyDcR8I3ttw8FWOy/7ZvjkYEE99CiIi/Q35OYX9ETxz8Ei/eV/M+X05bzwQF67dQqFBfQoiIgModEfzoVNeAxaB9i0k1HwkIjKg4gmFSBTKa6F9M7GokUyp+UhEpL/iCQXINiGpo1lEJK8iC4X6oE9BzymIiAykyEJhHHQ06zkFEZE8iiwUgiuFCLolVURkAEUWCuMg3UslHaQyTiajYBARyVVkoVAPwOjUNgCSGTUhiYjkKspQqEq3AGikVBGRfoosFLJPNVclWwH0nmYRkX6KLBSyVwqVqSAUdAeSiMhuiisUSsdANEFlb9B8pI5mEZHdFFcomEHlOMqSQSio+UhEZDfFFQoAlfWU9ezqaFYoiIjkKsJQGEdpz1YADXUhItJPEYZCPSVBKOiWVBGR3YUaCmY228xWmtlqM7tpgOXfMrPng5+XzWx7mPUAUFFPoqeVCBl61acgIrKb0N68ZmZR4A7gAqAJWGRm84K3rQHg7p/OWf/jwKlh1dOnsh7zDDXsZOOOrtB3JyIynIR5pXAmsNrd17h7L3AfcNkg619F9j3N4QoeYDsiuoOXN7eFvjsRkeEkzFCYAKzLmW4K5u3BzI4CpgB/zLN8jpktNrPFzc3NB1ZVEAonju5m1eb2A9uWiMgIc7h0NF8JPODu6YEWuvtd7t7o7o11dXUHtqfgqeZpld2s2qJQEBHJFWYorAcm5kw3BPMGciWHoukI+kLh6LIOXmvpoDs5YA6JiBSlMENhETDVzKaYWYLsF/+8/iuZ2TSgGngqxFrekKiARBUTYjvJOLy6teOQ7FZEZDgILRTcPQV8DHgUeAm4392XmdktZnZpzqpXAve5+6F7aKCyntrg7ld1NouIvCG0W1IB3P0R4JF+877Yb/rLYdYwoMpxVCZbiUaM1epXEBHpc7h0NB9alXVEOrZwVE25rhRERHIUaSiMg/bNHFdfpTuQRERyFGko1EP3DqbVxXmtpZOelO5AEhGBog2F7ANsJ4zuJZ1x3YEkIhIo6lCYWroTgJf1ZLOICFCsoTB+JmA0bHuaiMFqdTaLiADFGgpV42DS2cRW/obJNRXqbBYRCRRnKABMfydsfpFzxu7UbakiIoHiDYVp7wDgQp5hbUunXrgjIkIxh0L1UTD+FE5qe0J3IImIBIo3FACmX8rYbX9jHK08+crWQlcjIlJwRR8KAB+qXcbcv75KKq0mJBEpbsUdCnXHQe3xvKdsCetau/jdsk2FrkhEpKCKOxQApr+TsVsXcUpNmrueWMOhHMFbRORwo1A44V2YZ/j3cQt4oWkHC9e0FroiEZGCUSgccRKceh0zXr2Hc8vXcdcTrxS6IhGRggk1FMxstpmtNLPVZnZTnnUuN7PlZrbMzH4eZj15XfhvWGU93y67m7+s3MhLG3cWpAwRkUILLRTMLArcAVwEzACuMrMZ/daZCtwMnOvuJwCfCqueQZWNgUu+SV3HKj5d9hs+fu9ztHUnC1KKiEghhXmlcCaw2t3XuHsvcB9wWb91Pgzc4e7bANx9S4j1DG7axXDi+/iIPUR5y1I+ed/zpDPqdBaR4hJmKEwA1uVMNwXzch0HHGdmfzWzhWY2e6ANmdkcM1tsZoubm5tDKhe46OtEKuq4r/JbrFr5IrfNXxnevkREDkOF7miOAVOBWcBVwA/MbEz/ldz9LndvdPfGurq68KqpqIHrHqY8kuKXVbfxwIJnufeZ18Pbn4jIYSbMUFgPTMyZbgjm5WoC5rl70t1fBV4mGxKFUz8NrnmAsb6dB6tu42sPLeT//fXVgpYkInKohBkKi4CpZjbFzBLAlcC8fuv8kuxVAmZWS7Y5aU2INQ1NQyN2xU+YmF7Hb0bdynd//VfueHx1oasSEQldaKHg7ingY8CjwEvA/e6+zMxuMbNLg9UeBVrMbDnwOHCju7eEVdM+OfZ87Or7aPBN/K7qX3lg/gK+PG8ZSY2PJCIjmA23YR0aGxt98eLFh26H65/Ff3Y5nb0pruv4FDbpLO64+jSOGF166GoQETlAZvasuzfubb1CdzQf/iacjn1wPhVV1TxQ+m+ctfFnvOP2J/jzqhDvghIRKRCFwlDUHANzFhA5fjb/FPkp/2Vf5+M//ANfnreMrt50oasTETloFApDVTYGrvgpzP4Pzs48z5MVN5J5+vtcdvvjPPf6tkJXJyJyUKhPYX9sXga/uwlefYK1TOBfe6+keual/NPsadSPUl+DiBx+htqnoFDYX+6w8rdkHv1nItte5fnMsdzB5cyc9R6uf/MUyhOxQlcoItJHoXCopJPw/M9JLfgPYm3rWZw5jp/G3sNJ513BNWcfRWk8WugKRUQUCodcqgeW/JieJ75FSft6VmQm8pP4+5jw5mu45uzJjC6LF7pCESliCoVCSSdh6UN0/vEblO9YxZLMsXydDzDjjPO59uxJHF1XWegKRaQIKRQKLZOBv/2c5PwvE+9q5tfpc7gz9U7GHH06V581iQtmjKMkpqYlETk0FAqHi542+Mu3yCy8k0iyk0WRk7m7+3xWlZzArNNO4P2NDUw7ogozK3SlIjKCKRQON13b4Nl78Ke/j7VtBGCD17AkM5Vnyt9C2QkXcd6JR9F4VDWxqB4fEZGDS6FwuEr1QtMzsOE5el5/lsyaJyjrbaHdy5ifOZ1nY6dSMvU8zjrlBM49tpbKEt3aKiIHTqEwXGTS8OoTJP/2C3zF/5Do3QHA6syRLPQT2Fx7NmNmnE/jtCmcOGE00YiamURk3ykUhqNMBja/SPqVP7Fz+WNUbHqGRKaLtBvPZKazIHIWrRP/nmlTJnFaQxXTj6igtHIsRHU1ISKDUyiMBKleWP8sHct/R3r5/zCqbc8X/aSI0paoJz16EnbS5Yx98/VYRHc1icjuFAoj0dZVsPoPdHR1sm5Hktdbu2lvWU+srYnjeJ1pkXW84Mfy89qPUXX0WZw5pYYzJlczpjxR6MpFpMAOi1Aws9nAfwJR4G53v7Xf8g8A3+CNdzd/x93vHmybRR0KefSmMry0YQcdi3/OScv/LxWpVjZ7dd/ypuhElle/jW2T3s5RkyZycsMYptRUEFH/hEjRKHgomFkUeBm4AGgi+87mq9x9ec46HwAa3f1jQ92uQmEvunfCwu+San2NbZ0ptrZ3M7ZlCeOSTaQ8wjKfzOtez6bIEcRH1VNTHmdsRZyymgmUTL+EhiPqGV2uITlERpqhhkKYPZRnAqvdfU1Q0H3AZcDyQT8lB6Z0FMy6iRhQF/zgDpteJLL0IY59bTFHt66lvHMR0bY0tAWfWwNdz/wL8zONPB09jaqqUVRXVVIzupLaMVWMqx5NXW0toxumk0ioOUpkpAozFCYA63Kmm4CzBljvvWb2FrJXFZ9293X9VzCzOcAcgEmTJoVQ6ghnBuNPJjL+ZCp2zUunoLcdzOjoTbNl9XNEl97P219/hMtST2bDog3YsPumejzOi0xibexoesvriVSOIzFmPJm6acTrjqa6oozaqhLqq0qoLInpSW2RYSbM5qP3AbPd/UPB9HXAWblNRWZWA7S7e4+Z3QBc4e5vG2y7aj4KWaoXtq2FdC+ke0klu9m6vY0t23bStm0LZS3Lqd75ErWdr1CR3kGEN/7+dHoJL3sDm3wsrV5Fe6SK8YlOGiLbqLNttJYfw4YjzqOtYRZTJhzBiRNGa2hxkUPkcGg+Wg9MzJlu4I0OZQDcvSVn8m7g6yHWI0MRS0DdcW9MAkcEP3tIp6CzhZ7WdXStf4H0hheZvHUFR3dsIdb9CqXJHXR4Fc2ZWpp8NMe1PsXJ2+bTuzzK45lT+VjmPLYe8XdceNIE/vGcyVTo6W2RggvzSiFGtknofLJhsAi42t2X5awz3t03Br+/G/icu5892HZ1pTCMuGebrnbJpOlZu5DepfNILPsFJT0ttERq+H7Phfwq8Q4+8NbpXHfOURraQyQEBb/7KCjiYuDbZG9JnevuXzWzW4DF7j7PzL4GXAqkgFbgf7v7isG2qVAYIVK98PLvYPEPYc0CtkbHcUvX+3ksei5vm34El55yJOceWzv41UNPO/TshFFHHrq6RYapwyIUwqBQGIHWLID5n4dNL9IVqaQpM5b16WrSRBgf2c64yHbcYqyrOIGW6lMoLS1nyrY/c0TLM0QzvbROmk3bWZ+mbOJMRqdbKHllPmxeChMa4ZjzoGrAxi+RoqJQkOElk4FlD8G6p8lsb6Kj+TW6kym2RcbSzFi8t51jepYz3psBeC1Tz+8zp9NNgn+I/p5R1skrmfEcE8kOS95NglJ6AXjdJtAdq8LipRCvYGv5MayvPIENZcdTFU1STys1mRZKLUncnHgkQxQnGoGoGfGqOsqPmErVkVOx8prdm8QONXdoXpm9OiodVbg6ZNhRKMiIlNmxkY72HXRUHEVnMk17T4r2HS3ULJ3LqC2LeK3qdF6oeBNrmEh91yqmti9iYudyrLcDT3VTmm7nGNYTt/R+7b+VUbzMZFZFJrMtUkMsYkQjToX1Us1Oqn0nZsaG+CQ2xCexrWQC0fJqSirHUFpWSdR7iKR6iHqSeCxGLJ4gnkhQEjVKY0ZJLIqXjyURjxOPRiiJRSiJRyiJGlVrH2XUs3eQ2LSETMkoemZeT8/pN0BlHWZGxCAejZCIRvS0uuxBoSCST7ILNr4Am18kk6iis7Se9ngt3VZKT9roSUMqY6TcSaWdnu0bSW1dQ2T7q1S3v8K4zpep736VuPfuttkOq2BnZDQRT1OX2bLb7br7VJ5H2Uw1W3wMACUkGWttjLdWXs/U8eP0hZwaWcVFkUX0EmOtZ5vHDCdBkgrroZxudlgVq6PH8FpiKu3xGkq9mzLvJkqaZKSEZKSU3kgZqWgZqWgp6WgZkXgJsXgJpVGnJrWJmt6NVCa3kiJKj5XQbeVsHz2N9tpTKKuqpqJwz0nzAAAJyUlEQVQkRkVJjPJ4lJ5UhvaeFJ29aUpiEapKY1SVximJRYhFIJ7uwKJxPFaGAVGDRHsTpa0vZa/iJp5NrKwqCFojFrGhP+eydTW8eD80LYLjL4ZTroSSqvznP1a6+xVfe3P282XV2c+Xjdmvc3c4UyiIhCmdgmRHMGHZL5lYzpPevZ3Qsgq2v54deqR7B5lkFxYvxeJleCROKp0inewlmUyRykDSIZVKEWnfTKx9A7HOzaSJkLI4yUgp6+rPY239BfRksl9mozrWMn3dfZT1NLPr/+OkxemxMrqtlNLuZsZ1rKQ+2XRAh7rTy4iRoZReIpbdT8aN17yeKBlGWSeVdNFGOS0+ilaqcIwYaWKkqaaNOttBmWVDtN1LafUqxlg7o6yrbz89HmNJ5jhe8Cns8Eq2U0nEIoyPtzE+upNatjMqs4Mxvp0y76bDymmPVFFOD8emXyGDsTlSz/jMZjop4/HELF6LH822WC290UpOTC/j9O6nmdyzgu3xelZWnMEr5TM5vvNZTt3xGDFPZk+txVhTdQZNlSfTUVpHZ0k9PbHRJC1OyuKkY+VEy0eTKKkgHo+SyTjpTPa/SzwWIR6NECdFJNVNNN1NNJMkVlZJvGwU0UQJ2zuTtHb00t6TorYywfjRZYwfVUJ5wkiQJGFpelNpupJOVzIDna3EurYQ72ymfNyx1E7d6/f6gBQKIpLVvRO6t0OiMvtjEUh1Zf/F3NsByc43/kynsg8uWgTGTIQxR5GJB4MnukPPTjJNz5F8/WnSG18kZXG6o1X0RMopSbdT2ruNRE8rGc+QIkbSo/TER9NdUktXSQ2WSVLS00qip4VkrJLWquNpqTiOaLKN+q0LObJlIWM6XiXW/yosMoqd0TG0x6rpjI8lGS0lkWqnJNUGmQzPV7yJRZXnsTNaw+SeFZy381c0ti8gTnK37ayITmWxnURDZgONmReopJMuSvi1zeIn6Qspp5u329NcwNNMtM2D/mft9SgdlNFNgh7PjhdWYV1U0UWpJfN+JkWMFFFSRIiRJkGKOCmitvfv4qfHX8NZN3x3r+sNRKEgIsNXsgu6tkMmBZX1ECvZ922kU9CxBXZuhM4WGH8KVI3LWZ6ETS/C2CnZZqM9auiGtg2wc0M2WNM9kE7iPe2kunaQ6tiGd7dh6R4i6R4gQzpeRTpeSSpWgcfLIV5GxqKkezpId7VBbwel0Qxl0Qxxy9CVjtCWitCehF6P00OMXo8Ri0ZIRCARhXTJGHrK6ugtraN+4rEcM2ninrUOweHwRLOIyP6Jl2V/DkQ0lr1LK99zLNE4TDhtkBpKYezR2Z8cBsSDnwNVFfwcTiKFLkBERA4fCgUREemjUBARkT4KBRER6aNQEBGRPgoFERHpo1AQEZE+CgUREekz7J5oNrNm4LX9/HgtsPUgljNcFONxF+MxQ3EedzEeM+z7cR/l7nV7W2nYhcKBMLPFQ3nMe6QpxuMuxmOG4jzuYjxmCO+41XwkIiJ9FAoiItKn2ELhrkIXUCDFeNzFeMxQnMddjMcMIR13UfUpiIjI4IrtSkFERAahUBARkT5FEwpmNtvMVprZajO7qdD1hMHMJprZ42a23MyWmdkng/ljzez3ZrYq+HOA10wNf2YWNbPnzOx/gukpZvZ0cM7/28wSe9vGcGJmY8zsATNbYWYvmdk5xXCuzezTwd/vpWZ2r5mVjsRzbWZzzWyLmS3NmTfg+bWs24Pjf8HMBnl70OCKIhTMLArcAVwEzACuMrMZha0qFCngM+4+Azgb+GhwnDcBf3D3qcAfgumR6JPASznT/wF8y92PBbYBHyxIVeH5T+B37j4NOIXssY/oc21mE4BPAI3ufiIQBa5kZJ7re4DZ/eblO78XAVODnznAnfu706IIBeBMYLW7r3H3XuA+4LIC13TQuftGd18S/N5G9ktiAtlj/VGw2o+AdxWmwvCYWQNwCXB3MG3A24AHglVG1HGb2WjgLcAPAdy91923UwTnmuxrhMvMLAaUAxsZgefa3Z8AWvvNznd+LwN+7FkLgTFmNn5/9lssoTABWJcz3RTMG7HMbDJwKvA0MM7dNwaLNgHj8nxsOPs28E9AJpiuAba7eyqYHmnnfArQDPy/oMnsbjOrYISfa3dfD9wGvE42DHYAzzKyz3WufOf3oH3HFUsoFBUzqwQeBD7l7jtzl3n2HuQRdR+ymb0D2OLuzxa6lkMoBpwG3OnupwId9GsqGqHnuprsv4qnAEcCFezZxFIUwjq/xRIK64GJOdMNwbwRx8ziZAPhZ+7+UDB7865LyeDPLYWqLyTnApea2VqyTYNvI9vePiZoYoCRd86bgCZ3fzqYfoBsSIz0c/33wKvu3uzuSeAhsud/JJ/rXPnO70H7jiuWUFgETA3uUEiQ7ZiaV+CaDrqgHf2HwEvu/s2cRfOAfwx+/0fgV4e6tjC5+83u3uDuk8me2z+6+zXA48D7gtVG1HG7+yZgnZkdH8w6H1jOCD/XZJuNzjaz8uDv+67jHrHnup9853ce8A/BXUhnAztympn2SdE80WxmF5Ntd44Cc939qwUu6aAzszcDfwZe5I229X8m269wPzCJ7LDjl7t7/w6sEcHMZgGfdfd3mNnRZK8cxgLPAde6e08h6zuYzGwm2Y71BLAGuJ7sP/RG9Lk2s68AV5C92+454ENk289H1Lk2s3uBWWSHyN4MfAn4JQOc3yAgv0O2Ka0TuN7dF+/XfoslFEREZO+KpflIRESGQKEgIiJ9FAoiItJHoSAiIn0UCiIi0kehIHIImdmsXaO4ihyOFAoiItJHoSAyADO71syeMbPnzez7wbsa2s3sW8FY/n8ws7pg3ZlmtjAYx/7hnDHujzWzx8zsb2a2xMyOCTZfmfMehJ8FDx6JHBYUCiL9mNl0sk/MnuvuM4E0cA3ZwdcWu/sJwJ/IPmEK8GPgc+5+MtmnyXfN/xlwh7ufAryJ7KiekB299lNk3+1xNNmxe0QOC7G9ryJSdM4HTgcWBf+ILyM78FgG+O9gnZ8CDwXvNRjj7n8K5v8I+IWZVQET3P1hAHfvBgi294y7NwXTzwOTgb+Ef1gie6dQENmTAT9y95t3m2n2hX7r7e8YMblj8qTR/4dyGFHzkcie/gC8z8zqoe+9uEeR/f9l10icVwN/cfcdwDYz+7tg/nXAn4I33zWZ2buCbZSYWfkhPQqR/aB/oYj04+7LzezzwHwziwBJ4KNkX2RzZrBsC9l+B8gOYfy94Et/12ilkA2I75vZLcE23n8ID0Nkv2iUVJEhMrN2d68sdB0iYVLzkYiI9NGVgoiI9NGVgoiI9FEoiIhIH4WCiIj0USiIiEgfhYKIiPT5/yg1f1hSf5QgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupance = pd.read_table('occupancy_data/datatraining.txt',delimiter=',')\n",
    "Stu_Eva = pd.read_csv('turkiye-student-evaluation_generic.csv',delimiter=',',header=0)\n",
    "Activity = pd.read_csv('Activity_Recognition/1.csv',delimiter=',',header=None)\n",
    "Parking = pd.read_csv('Parking_Birminghan_Data.csv',delimiter=',',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Activity.columns=['sequential','x','y','z','activity_type']\n",
    "Activity.drop(columns=['sequential'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually divide 7 kinds of activity into 2 categories\n",
    "# first one is going up\n",
    "# second one is the remaining\n",
    "Activity['activity_type'].replace([0,1,2,3,4,5,6,7],[0,0,1,0,0,1,0,0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Act_x = Activity.iloc[:,:3]\n",
    "Act_y = Activity.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupance.drop(columns=['date'],inplace=True)\n",
    "occ_x = occupance.iloc[:,:-1]\n",
    "occ_y = occupance.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stu_Eva = pd.concat([Stu_Eva,pd.get_dummies(Stu_Eva['instr'])],axis=1)\n",
    "Instr = Stu_Eva.iloc[:,0]\n",
    "Stu_Eva.drop(columns=['instr'],inplace=True)\n",
    "eva_x = Stu_Eva.iloc[:,0:-3]\n",
    "eva_y = Stu_Eva.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "Parking.drop(columns=['LastUpdated'],inplace=True)\n",
    "Parking['parking_label'] = le.fit_transform(Parking['SystemCodeNumber'])\n",
    "parking_label = Parking['parking_label']\n",
    "Parking.drop(columns=['parking_label'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parking = pd.concat([Parking,pd.get_dummies(Parking['SystemCodeNumber'])],axis=1)\n",
    "Parking.drop(columns=['SystemCodeNumber'],inplace=True)\n",
    "parking_x = Parking.iloc[:,:2]\n",
    "parking_y = Parking.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Second Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='scale')\n",
    "C_list = [{'kernel':['rbf','linear']}]\n",
    "#C_list = [{'kernel':['linear','poly','rbf'],'C':[0.0001,0.001,0.01,0.1]}] # Different C to try.\n",
    "#search = GridSearchCV(clf, C_list, cv=5)\n",
    "search = GridSearchCV(clf, C_list,cv=3)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "#X = preprocessing.scale(Act_x)\n",
    "\n",
    "search.fit(Act_x,Activity_type)\n",
    "\n",
    "clf=SVC(kernel=search.best_params_['kernel'],C=0.1)\n",
    "print search.best_params_['kernel']\n",
    "#clf=SVC(kernel='linear',C=0.1,gamma='scale')\n",
    "\n",
    "ave_acc_activity_svm = []\n",
    "for j in test_size:\n",
    "    print j\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        print i\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(Act_x, Act_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1,y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_activity_svm.append(score/3.0)\n",
    "\n",
    "print ave_acc_activity_svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "#parameter = [{'learning_rate':[0.01,0.1,0.3],'n_estimators':[100,200,500]}]\n",
    "parameter = [{'learning_rate':[0.1],'n_estimators':[100,200]}]\n",
    "search = GridSearchCV(clf,parameter,cv=3)\n",
    "search.fit(Act_x,Act_y)\n",
    "print search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=search.best_params_['learning_rate'],n_estimators=search.best_params_['n_estimators'],)\n",
    "ave_acc_activity_boo = []\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(Act_x, Act_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1,y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_activity_boo.append(score/3.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_acc_activity_boo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "search.fit(occ_x,occ_y)\n",
    "print search.best_params_\n",
    "clf = GradientBoostingClassifier(learning_rate=search.best_params_['learning_rate'],n_estimators=search.best_params_['n_estimators'])\n",
    "ave_acc_occ_boo = []\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(occ_x, occ_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_occ_boo.append(score/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.992633517495396, 0.9918958742632613, 0.9912509593246356]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_acc_occ_boo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "search.fit(eva_x,Instr)\n",
    "print search.best_params_\n",
    "clf = GradientBoostingClassifier(learning_rate=search.best_params_['learning_rate'],n_estimators=search.best_params_['n_estimators'])\n",
    "ave_acc_eva_boo=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(eva_x, Instr, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_eva_boo.append(score/3.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9816723940435281, 0.983848797250859, 0.979524627720504]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_acc_eva_boo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "search.fit(parking_x,parking_label)\n",
    "print search.best_params_\n",
    "clf = GradientBoostingClassifier(learning_rate=search.best_params_['learning_rate'],n_estimators=search.best_params_['n_estimators'])\n",
    "ave_acc_parking_boo=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(parking_x, parking_label, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_parking_boo.append(score/3.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_acc_parking_boo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8b6308ed15f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_neighbors'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ball_tree'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brute'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAct_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAct_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             train_scores = _score(estimator, X_train, y_train, scorer,\n\u001b[0;32m--> 572\u001b[0;31m                                   is_multimetric)\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \"\"\"\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36m_kneighbors_reduce_func\u001b[0;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[1;32m    308\u001b[0m         \u001b[0msample_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# argpartition doesn't guarantee sorted order, so we sort again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \"\"\"\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argpartition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "parameter = [{'n_neighbors':[5,100],'algorithm':['auto','ball_tree','brute']}]\n",
    "search = GridSearchCV(clf,parameter,cv=3)\n",
    "search.fit(Act_x,Act_y)\n",
    "print search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=search.best_params_['n_neighbors'],algorithm=search.best_params_['algorithm'])\n",
    "ave_acc_activity_knn = []\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(Act_x, Act_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1,y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_activity_knn.append(score/3.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_acc_activity_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 100, 'algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "search.fit(occ_x,occ_y)\n",
    "print search.best_params_\n",
    "clf = KNeighborsClassifier(n_neighbors=search.best_params_['n_neighbors'],algorithm=search.best_params_['algorithm'])\n",
    "ave_acc_occ_knn = []\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(occ_x, occ_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_occ_knn.append(score/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9883364027010435, 0.9852652259332023, 0.9793297518546943]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_acc_occ_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 5, 'algorithm': 'auto'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "search.fit(eva_x,eva_y)\n",
    "print search.best_params_\n",
    "clf = KNeighborsClassifier(n_neighbors=search.best_params_['n_neighbors'],algorithm=search.best_params_['algorithm'])\n",
    "ave_acc_eva_knn=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(eva_x, eva_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_eva_knn.append(score/3.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7107674684994273, 0.6693012600229095, 0.5894186712485682]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_acc_eva_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 5, 'algorithm': 'auto'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "search.fit(parking_x,parking_y)\n",
    "print search.best_params_\n",
    "clf = KNeighborsClassifier(n_neighbors=search.best_params_['n_neighbors'],algorithm=search.best_params_['algorithm'])\n",
    "ave_acc_parking_knn=[]\n",
    "for j in test_size:\n",
    "    score=0\n",
    "    for i in range(3):\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(parking_x, parking_y, test_size=j) \n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        score += clf.score(X_test1,y_test1)\n",
    "    ave_acc_parking_knn.append(score/3.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9895483389324374, 0.9767250872575919, 0.9399104080632744]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_acc_parking_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
